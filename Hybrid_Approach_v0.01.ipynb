{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Approach V0.01\n",
    "\n",
    "This document records the process to implement the first simple version of hybrid approach based on G.B.'s approach.\n",
    "\n",
    "## Auto encoder decoder\n",
    "This subsetion describes how to design an auto encoder decoder to extract features of data.\n",
    "\n",
    "### What is auto encoder decoder ?\n",
    "Auto encoder decoder is a special artificial neural network that tranlate input data and ouput the same/similar data. Because the there is no information lost in the translating process, we can use the data inthe middle layer as the feathures of the input data.\n",
    "\n",
    "When middle layer's hidden variables are less than input variables, auto encoder decoder compresses the data.\n",
    "When middle layer's hidden variables are more than input variables, auto encoder decoder represents data sparsely.\n",
    "\n",
    "#### Auto encoder decoder test\n",
    "##### Test data\n",
    "   Test data comes from a simple function. \n",
    "+ Generate feature vector randomly\n",
    "+ Generate observation data\n",
    "[v1,v2,...,vn]   ==>   [v1,((v1+v2)/2),v2,((v2+v3)/2),...,vn] and then max min normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19475819  0.77058232  0.87010047]\n",
      " [ 0.9065282   0.89872616  0.01308841]\n",
      " [ 0.34570973  0.47027361  0.4906416 ]\n",
      " ..., \n",
      " [ 0.54165801  0.85655742  0.73898851]\n",
      " [ 0.64968321  0.50514054  0.20407222]\n",
      " [ 0.04361496  0.73136389  0.14377273]]\n",
      "Variable containing:\n",
      " 1.9476e-01  4.8267e-01  7.7058e-01  8.2034e-01  8.7010e-01\n",
      " 9.0653e-01  9.0263e-01  8.9873e-01  4.5591e-01  1.3088e-02\n",
      " 3.4571e-01  4.0799e-01  4.7027e-01  4.8046e-01  4.9064e-01\n",
      "                             â‹®                              \n",
      " 5.4166e-01  6.9911e-01  8.5656e-01  7.9777e-01  7.3899e-01\n",
      " 6.4968e-01  5.7741e-01  5.0514e-01  3.5461e-01  2.0407e-01\n",
      " 4.3615e-02  3.8749e-01  7.3136e-01  4.3757e-01  1.4377e-01\n",
      "[torch.FloatTensor of size 100000x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import torch\n",
    "\n",
    "#input dimension\n",
    "D = 3\n",
    "#input number\n",
    "N = 100000\n",
    "\n",
    "\n",
    "feature = random.rand(N,D)\n",
    "\n",
    "print(feature)\n",
    "\n",
    "\n",
    "def transfer(input):\n",
    "    n,dim   = input.shape\n",
    "    new_dim = dim*2-1\n",
    "    output  = np.zeros((n,new_dim))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(new_dim):\n",
    "            if j%2 == 0:\n",
    "                output[i][j] = input[i][int(j/2)]\n",
    "            else:\n",
    "                output[i][j] = ((input[i][int(j/2)]+input[i][int(j/2)+1])/2)\n",
    "    return output\n",
    "\n",
    "np_data   = transfer(feature)\n",
    "data      = torch.FloatTensor(np_data)\n",
    "data      = torch.autograd.Variable(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Air Unit Data\n",
    "Read and pre-process data by panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "file_name = './OAU1201.csv'\n",
    "raw_data  = pandas.read_csv(file_name)\n",
    "\n",
    "headings = np.array(raw_data.columns[1:])\n",
    "data     = np.array(raw_data[headings])\n",
    "\n",
    "#delete nan\n",
    "mask     = [[1 if type(j)==float and np.isnan(j) else 0 for j in i] for i in data]\n",
    "post_data= np.array([i for i,j in zip(data,mask) if sum(j)==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### auto encoder decoder structure design\n",
    "Now, we just use some simple structure.\n",
    "      \n",
    "The structure works!\n",
    "The \"works\" mean just works! But the result is bad! The good thing is that we can start now. The bad thing is that the structure need to be re-designed!\n",
    "\n",
    "TODO: use practical data and design new auto encoder decoder structrue. Quite importance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_endecoder (\n",
      "  (in1): Linear (5 -> 10)\n",
      "  (in2): Linear (10 -> 3)\n",
      "  (out1): Linear (3 -> 10)\n",
      "  (out2): Linear (10 -> 5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class simple_endecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_endecoder,self).__init__()\n",
    "        #linear\n",
    "        self.in1  = nn.Linear(5,10)\n",
    "        self.in2  = nn.Linear(10,3)\n",
    "\n",
    "        self.out1 = nn.Linear(3,10)\n",
    "        self.out2 = nn.Linear(10,5)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.in1(x)\n",
    "        x = self.in2(x)\n",
    "\n",
    "        x = self.out1(x)\n",
    "        x = self.out2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "net       = simple_endecoder()\n",
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print initial parameters:\n",
      "Parameter containing:\n",
      " 0.0739 -0.3110  0.4376 -0.1562 -0.0179\n",
      "-0.2952  0.3878 -0.4344 -0.1913 -0.0279\n",
      "-0.3822 -0.2136 -0.4224  0.0113 -0.4258\n",
      "-0.1513  0.4441  0.2813  0.3158  0.2699\n",
      "-0.4375 -0.3381 -0.3325 -0.2684  0.1049\n",
      " 0.4400 -0.1166  0.3623  0.3031  0.0531\n",
      "-0.3310 -0.4216  0.1247  0.3781 -0.2407\n",
      "-0.3880  0.4385 -0.1115  0.4035 -0.2945\n",
      "-0.1649 -0.3810 -0.1165 -0.3391 -0.1641\n",
      "-0.4268  0.2517  0.3613  0.1369 -0.0570\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n",
      "Parameter containing:\n",
      "-0.3540\n",
      "-0.3228\n",
      "-0.0770\n",
      "-0.1696\n",
      " 0.1735\n",
      " 0.2130\n",
      " 0.3497\n",
      " 0.1850\n",
      "-0.3522\n",
      "-0.1141\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "Parameter containing:\n",
      "-0.2097  0.0041  0.0264  0.0751 -0.1694 -0.1477  0.2190 -0.0362  0.1562  0.0253\n",
      "-0.0195  0.0137  0.2168  0.1241  0.2493  0.1618  0.0559 -0.0275  0.1287 -0.1708\n",
      "-0.0345  0.2716 -0.2889 -0.1978  0.1139 -0.0299 -0.2597  0.0279  0.2358  0.2660\n",
      "[torch.FloatTensor of size 3x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1313\n",
      " 0.1911\n",
      "-0.1688\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2495 -0.0974 -0.5631\n",
      " 0.2982 -0.3789 -0.0317\n",
      "-0.3555  0.3909  0.1170\n",
      "-0.0769  0.1716  0.0361\n",
      "-0.0409  0.1704  0.4671\n",
      " 0.2370  0.2347  0.3840\n",
      "-0.4435  0.3343 -0.2005\n",
      " 0.3165 -0.0846 -0.3833\n",
      " 0.3256  0.4221  0.4224\n",
      "-0.3365 -0.3635 -0.4368\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "Parameter containing:\n",
      "-0.3495\n",
      "-0.4476\n",
      "-0.0137\n",
      "-0.1503\n",
      "-0.0879\n",
      "-0.0892\n",
      " 0.1816\n",
      "-0.5279\n",
      " 0.2223\n",
      "-0.2078\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2402  0.1501  0.0174 -0.0126 -0.2817  0.0477 -0.1466  0.2529  0.1419  0.0262\n",
      " 0.1871 -0.3029 -0.0042 -0.1429 -0.0663  0.1194  0.0665  0.1712  0.2073 -0.0864\n",
      " 0.0650  0.1184 -0.2961  0.1040  0.2639  0.1521 -0.0522 -0.3071  0.1140 -0.0851\n",
      "-0.2969 -0.0624 -0.0053 -0.1419  0.0372 -0.0165  0.1538  0.1966 -0.0250 -0.0038\n",
      "-0.0371  0.2694  0.0408  0.2683 -0.1875  0.0064 -0.1699  0.0758 -0.2760  0.1632\n",
      "[torch.FloatTensor of size 5x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1425\n",
      " 0.0747\n",
      " 0.2685\n",
      "-0.2711\n",
      " 0.0110\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[1] loss: 0.351\n",
      "[2] loss: 0.347\n",
      "[3] loss: 0.343\n",
      "[4] loss: 0.339\n",
      "[5] loss: 0.334\n",
      "[6] loss: 0.330\n",
      "[7] loss: 0.327\n",
      "[8] loss: 0.323\n",
      "[9] loss: 0.319\n",
      "[10] loss: 0.315\n",
      "[11] loss: 0.312\n",
      "[12] loss: 0.308\n",
      "[13] loss: 0.304\n",
      "[14] loss: 0.301\n",
      "[15] loss: 0.298\n",
      "[16] loss: 0.294\n",
      "[17] loss: 0.291\n",
      "[18] loss: 0.288\n",
      "[19] loss: 0.284\n",
      "[20] loss: 0.281\n",
      "[21] loss: 0.278\n",
      "[22] loss: 0.275\n",
      "[23] loss: 0.272\n",
      "[24] loss: 0.269\n",
      "[25] loss: 0.266\n",
      "[26] loss: 0.263\n",
      "[27] loss: 0.261\n",
      "[28] loss: 0.258\n",
      "[29] loss: 0.255\n",
      "[30] loss: 0.252\n",
      "[31] loss: 0.250\n",
      "[32] loss: 0.247\n",
      "[33] loss: 0.244\n",
      "[34] loss: 0.242\n",
      "[35] loss: 0.239\n",
      "[36] loss: 0.237\n",
      "[37] loss: 0.234\n",
      "[38] loss: 0.232\n",
      "[39] loss: 0.230\n",
      "[40] loss: 0.227\n",
      "[41] loss: 0.225\n",
      "[42] loss: 0.223\n",
      "[43] loss: 0.220\n",
      "[44] loss: 0.218\n",
      "[45] loss: 0.216\n",
      "[46] loss: 0.214\n",
      "[47] loss: 0.212\n",
      "[48] loss: 0.210\n",
      "[49] loss: 0.208\n",
      "[50] loss: 0.206\n",
      "[51] loss: 0.203\n",
      "[52] loss: 0.201\n",
      "[53] loss: 0.199\n",
      "[54] loss: 0.198\n",
      "[55] loss: 0.196\n",
      "[56] loss: 0.194\n",
      "[57] loss: 0.192\n",
      "[58] loss: 0.190\n",
      "[59] loss: 0.188\n",
      "[60] loss: 0.186\n",
      "[61] loss: 0.185\n",
      "[62] loss: 0.183\n",
      "[63] loss: 0.181\n",
      "[64] loss: 0.179\n",
      "[65] loss: 0.178\n",
      "[66] loss: 0.176\n",
      "[67] loss: 0.174\n",
      "[68] loss: 0.173\n",
      "[69] loss: 0.171\n",
      "[70] loss: 0.169\n",
      "[71] loss: 0.168\n",
      "[72] loss: 0.166\n",
      "[73] loss: 0.165\n",
      "[74] loss: 0.163\n",
      "[75] loss: 0.162\n",
      "[76] loss: 0.160\n",
      "[77] loss: 0.159\n",
      "[78] loss: 0.157\n",
      "[79] loss: 0.156\n",
      "[80] loss: 0.154\n",
      "[81] loss: 0.153\n",
      "[82] loss: 0.152\n",
      "[83] loss: 0.150\n",
      "[84] loss: 0.149\n",
      "[85] loss: 0.148\n",
      "[86] loss: 0.146\n",
      "[87] loss: 0.145\n",
      "[88] loss: 0.144\n",
      "[89] loss: 0.142\n",
      "[90] loss: 0.141\n",
      "[91] loss: 0.140\n",
      "[92] loss: 0.139\n",
      "[93] loss: 0.138\n",
      "[94] loss: 0.136\n",
      "[95] loss: 0.135\n",
      "[96] loss: 0.134\n",
      "[97] loss: 0.133\n",
      "[98] loss: 0.132\n",
      "[99] loss: 0.131\n",
      "[100] loss: 0.130\n",
      "[1] loss: 0.129\n",
      "[2] loss: 0.133\n",
      "[3] loss: 0.133\n",
      "[4] loss: 0.132\n",
      "[5] loss: 0.132\n",
      "[6] loss: 0.131\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.130\n",
      "[9] loss: 0.130\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.127\n",
      "[12] loss: 0.127\n",
      "[13] loss: 0.126\n",
      "[14] loss: 0.126\n",
      "[15] loss: 0.125\n",
      "[16] loss: 0.125\n",
      "[17] loss: 0.124\n",
      "[18] loss: 0.124\n",
      "[19] loss: 0.123\n",
      "[20] loss: 0.123\n",
      "[21] loss: 0.122\n",
      "[22] loss: 0.122\n",
      "[23] loss: 0.122\n",
      "[24] loss: 0.121\n",
      "[25] loss: 0.121\n",
      "[26] loss: 0.120\n",
      "[27] loss: 0.120\n",
      "[28] loss: 0.120\n",
      "[29] loss: 0.119\n",
      "[30] loss: 0.119\n",
      "[31] loss: 0.118\n",
      "[32] loss: 0.118\n",
      "[33] loss: 0.118\n",
      "[34] loss: 0.117\n",
      "[35] loss: 0.117\n",
      "[36] loss: 0.116\n",
      "[37] loss: 0.116\n",
      "[38] loss: 0.116\n",
      "[39] loss: 0.115\n",
      "[40] loss: 0.115\n",
      "[41] loss: 0.115\n",
      "[42] loss: 0.114\n",
      "[43] loss: 0.114\n",
      "[44] loss: 0.114\n",
      "[45] loss: 0.113\n",
      "[46] loss: 0.113\n",
      "[47] loss: 0.113\n",
      "[48] loss: 0.112\n",
      "[49] loss: 0.112\n",
      "[50] loss: 0.112\n",
      "[51] loss: 0.111\n",
      "[52] loss: 0.111\n",
      "[53] loss: 0.111\n",
      "[54] loss: 0.110\n",
      "[55] loss: 0.110\n",
      "[56] loss: 0.110\n",
      "[57] loss: 0.109\n",
      "[58] loss: 0.109\n",
      "[59] loss: 0.109\n",
      "[60] loss: 0.108\n",
      "[61] loss: 0.108\n",
      "[62] loss: 0.108\n",
      "[63] loss: 0.108\n",
      "[64] loss: 0.107\n",
      "[65] loss: 0.107\n",
      "[66] loss: 0.107\n",
      "[67] loss: 0.106\n",
      "[68] loss: 0.106\n",
      "[69] loss: 0.106\n",
      "[70] loss: 0.106\n",
      "[71] loss: 0.103\n",
      "[72] loss: 0.103\n",
      "[73] loss: 0.103\n",
      "[74] loss: 0.103\n",
      "[75] loss: 0.102\n",
      "[76] loss: 0.102\n",
      "[77] loss: 0.102\n",
      "[78] loss: 0.102\n",
      "[79] loss: 0.101\n",
      "[80] loss: 0.101\n",
      "[81] loss: 0.101\n",
      "[82] loss: 0.101\n",
      "[83] loss: 0.100\n",
      "[84] loss: 0.100\n",
      "[85] loss: 0.100\n",
      "[86] loss: 0.100\n",
      "[87] loss: 0.100\n",
      "[88] loss: 0.099\n",
      "[89] loss: 0.099\n",
      "[90] loss: 0.099\n",
      "[91] loss: 0.099\n",
      "[92] loss: 0.098\n",
      "[93] loss: 0.098\n",
      "[94] loss: 0.098\n",
      "[95] loss: 0.098\n",
      "[96] loss: 0.098\n",
      "[97] loss: 0.097\n",
      "[98] loss: 0.097\n",
      "[99] loss: 0.097\n",
      "[100] loss: 0.097\n",
      "[101] loss: 0.096\n",
      "[102] loss: 0.096\n",
      "[103] loss: 0.096\n",
      "[104] loss: 0.096\n",
      "[105] loss: 0.096\n",
      "[106] loss: 0.095\n",
      "[107] loss: 0.095\n",
      "[108] loss: 0.095\n",
      "[109] loss: 0.095\n",
      "[110] loss: 0.095\n",
      "[111] loss: 0.094\n",
      "[112] loss: 0.094\n",
      "[113] loss: 0.094\n",
      "[114] loss: 0.094\n",
      "[115] loss: 0.094\n",
      "[116] loss: 0.093\n",
      "[117] loss: 0.093\n",
      "[118] loss: 0.093\n",
      "[119] loss: 0.093\n",
      "[120] loss: 0.093\n",
      "[121] loss: 0.093\n",
      "[122] loss: 0.092\n",
      "[123] loss: 0.092\n",
      "[124] loss: 0.092\n",
      "[125] loss: 0.092\n",
      "[126] loss: 0.092\n",
      "[127] loss: 0.091\n",
      "[128] loss: 0.091\n",
      "[129] loss: 0.091\n",
      "[130] loss: 0.091\n",
      "[131] loss: 0.091\n",
      "[132] loss: 0.091\n",
      "[133] loss: 0.090\n",
      "[134] loss: 0.090\n",
      "[135] loss: 0.090\n",
      "[136] loss: 0.090\n",
      "[137] loss: 0.090\n",
      "[138] loss: 0.090\n",
      "[139] loss: 0.089\n",
      "[140] loss: 0.089\n",
      "[141] loss: 0.089\n",
      "[142] loss: 0.089\n",
      "[143] loss: 0.089\n",
      "[144] loss: 0.089\n",
      "[145] loss: 0.088\n",
      "[146] loss: 0.088\n",
      "[147] loss: 0.088\n",
      "[148] loss: 0.088\n",
      "[149] loss: 0.088\n",
      "[150] loss: 0.088\n",
      "[151] loss: 0.087\n",
      "[152] loss: 0.087\n",
      "[153] loss: 0.087\n",
      "[154] loss: 0.087\n",
      "[155] loss: 0.087\n",
      "[156] loss: 0.087\n",
      "[157] loss: 0.087\n",
      "[158] loss: 0.086\n",
      "[159] loss: 0.086\n",
      "[160] loss: 0.086\n",
      "[161] loss: 0.086\n",
      "[162] loss: 0.086\n",
      "[163] loss: 0.086\n",
      "[164] loss: 0.085\n",
      "[165] loss: 0.085\n",
      "[166] loss: 0.085\n",
      "[167] loss: 0.085\n",
      "[168] loss: 0.085\n",
      "[169] loss: 0.085\n",
      "[170] loss: 0.085\n",
      "[171] loss: 0.084\n",
      "[172] loss: 0.084\n",
      "[173] loss: 0.084\n",
      "[174] loss: 0.084\n",
      "[175] loss: 0.084\n",
      "[176] loss: 0.084\n",
      "[177] loss: 0.083\n",
      "[178] loss: 0.083\n",
      "[179] loss: 0.083\n",
      "[180] loss: 0.083\n",
      "[181] loss: 0.083\n",
      "[182] loss: 0.083\n",
      "[183] loss: 0.083\n",
      "[184] loss: 0.083\n",
      "[185] loss: 0.082\n",
      "[186] loss: 0.082\n",
      "[187] loss: 0.082\n",
      "[188] loss: 0.082\n",
      "[189] loss: 0.082\n",
      "[190] loss: 0.082\n",
      "[191] loss: 0.082\n",
      "[192] loss: 0.081\n",
      "[193] loss: 0.081\n",
      "[194] loss: 0.081\n",
      "[195] loss: 0.081\n",
      "[196] loss: 0.081\n",
      "[197] loss: 0.081\n",
      "[198] loss: 0.081\n",
      "[199] loss: 0.081\n",
      "[200] loss: 0.080\n",
      "[201] loss: 0.080\n",
      "[202] loss: 0.080\n",
      "[203] loss: 0.080\n",
      "[204] loss: 0.080\n",
      "[205] loss: 0.080\n",
      "[206] loss: 0.080\n",
      "[207] loss: 0.080\n",
      "[208] loss: 0.079\n",
      "[209] loss: 0.079\n",
      "[210] loss: 0.079\n",
      "[211] loss: 0.079\n",
      "[212] loss: 0.079\n",
      "[213] loss: 0.079\n",
      "[214] loss: 0.079\n",
      "[215] loss: 0.079\n",
      "[216] loss: 0.079\n",
      "[217] loss: 0.078\n",
      "[218] loss: 0.078\n",
      "[219] loss: 0.078\n",
      "[220] loss: 0.078\n",
      "[221] loss: 0.078\n",
      "[222] loss: 0.078\n",
      "[223] loss: 0.078\n",
      "[224] loss: 0.078\n",
      "[225] loss: 0.077\n",
      "[226] loss: 0.077\n",
      "[227] loss: 0.077\n",
      "[228] loss: 0.077\n",
      "[229] loss: 0.077\n",
      "[230] loss: 0.077\n",
      "[231] loss: 0.077\n",
      "[232] loss: 0.077\n",
      "[233] loss: 0.077\n",
      "[234] loss: 0.077\n",
      "[235] loss: 0.076\n",
      "[236] loss: 0.076\n",
      "[237] loss: 0.076\n",
      "[238] loss: 0.076\n",
      "[239] loss: 0.076\n",
      "[240] loss: 0.076\n",
      "[241] loss: 0.076\n",
      "[242] loss: 0.076\n",
      "[243] loss: 0.076\n",
      "[244] loss: 0.075\n",
      "[245] loss: 0.075\n",
      "[246] loss: 0.075\n",
      "[247] loss: 0.075\n",
      "[248] loss: 0.075\n",
      "[249] loss: 0.075\n",
      "[250] loss: 0.075\n",
      "[251] loss: 0.075\n",
      "[252] loss: 0.075\n",
      "[253] loss: 0.075\n",
      "[254] loss: 0.074\n",
      "[255] loss: 0.074\n",
      "[256] loss: 0.074\n",
      "[257] loss: 0.074\n",
      "[258] loss: 0.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259] loss: 0.074\n",
      "[260] loss: 0.074\n",
      "[261] loss: 0.074\n",
      "[262] loss: 0.074\n",
      "[263] loss: 0.074\n",
      "[264] loss: 0.074\n",
      "[265] loss: 0.073\n",
      "[266] loss: 0.073\n",
      "[267] loss: 0.073\n",
      "[268] loss: 0.073\n",
      "[269] loss: 0.073\n",
      "[270] loss: 0.073\n",
      "[271] loss: 0.073\n",
      "[272] loss: 0.073\n",
      "[273] loss: 0.073\n",
      "[274] loss: 0.073\n",
      "[275] loss: 0.073\n",
      "[276] loss: 0.072\n",
      "[277] loss: 0.072\n",
      "[278] loss: 0.072\n",
      "[279] loss: 0.072\n",
      "[280] loss: 0.072\n",
      "[281] loss: 0.072\n",
      "[282] loss: 0.072\n",
      "[283] loss: 0.072\n",
      "[284] loss: 0.072\n",
      "[285] loss: 0.072\n",
      "[286] loss: 0.072\n",
      "[287] loss: 0.072\n",
      "[288] loss: 0.071\n",
      "[289] loss: 0.071\n",
      "[290] loss: 0.071\n",
      "[291] loss: 0.071\n",
      "[292] loss: 0.071\n",
      "[293] loss: 0.071\n",
      "[294] loss: 0.071\n",
      "[295] loss: 0.071\n",
      "[296] loss: 0.071\n",
      "[297] loss: 0.071\n",
      "[298] loss: 0.071\n",
      "[299] loss: 0.071\n",
      "[300] loss: 0.071\n",
      "[301] loss: 0.070\n",
      "[302] loss: 0.070\n",
      "[303] loss: 0.070\n",
      "[304] loss: 0.070\n",
      "[305] loss: 0.070\n",
      "[306] loss: 0.070\n",
      "[307] loss: 0.070\n",
      "[308] loss: 0.070\n",
      "[309] loss: 0.070\n",
      "[310] loss: 0.070\n",
      "[311] loss: 0.070\n",
      "[312] loss: 0.070\n",
      "[313] loss: 0.070\n",
      "[314] loss: 0.070\n",
      "[315] loss: 0.069\n",
      "[316] loss: 0.069\n",
      "[317] loss: 0.069\n",
      "[318] loss: 0.069\n",
      "[319] loss: 0.069\n",
      "[320] loss: 0.069\n",
      "[321] loss: 0.069\n",
      "[322] loss: 0.069\n",
      "[323] loss: 0.069\n",
      "[324] loss: 0.069\n",
      "[325] loss: 0.069\n",
      "[326] loss: 0.069\n",
      "[327] loss: 0.069\n",
      "[328] loss: 0.069\n",
      "[329] loss: 0.069\n",
      "[330] loss: 0.069\n",
      "[331] loss: 0.068\n",
      "[332] loss: 0.068\n",
      "[333] loss: 0.068\n",
      "[334] loss: 0.068\n",
      "[335] loss: 0.068\n",
      "[336] loss: 0.068\n",
      "[337] loss: 0.068\n",
      "[338] loss: 0.068\n",
      "[339] loss: 0.068\n",
      "[340] loss: 0.068\n",
      "[341] loss: 0.068\n",
      "[342] loss: 0.068\n",
      "[343] loss: 0.067\n",
      "[344] loss: 0.067\n",
      "[345] loss: 0.066\n",
      "[346] loss: 0.066\n",
      "[347] loss: 0.066\n",
      "[348] loss: 0.066\n",
      "[349] loss: 0.066\n",
      "[350] loss: 0.066\n",
      "[351] loss: 0.066\n",
      "[352] loss: 0.066\n",
      "[353] loss: 0.066\n",
      "[354] loss: 0.066\n",
      "[355] loss: 0.066\n",
      "[356] loss: 0.066\n",
      "[357] loss: 0.066\n",
      "[358] loss: 0.066\n",
      "[359] loss: 0.066\n",
      "[360] loss: 0.066\n",
      "[361] loss: 0.066\n",
      "[362] loss: 0.066\n",
      "[363] loss: 0.066\n",
      "[364] loss: 0.066\n",
      "[365] loss: 0.066\n",
      "[366] loss: 0.066\n",
      "[367] loss: 0.065\n",
      "[368] loss: 0.065\n",
      "[369] loss: 0.065\n",
      "[370] loss: 0.065\n",
      "[371] loss: 0.065\n",
      "[372] loss: 0.065\n",
      "[373] loss: 0.065\n",
      "[374] loss: 0.065\n",
      "[375] loss: 0.065\n",
      "[376] loss: 0.065\n",
      "[377] loss: 0.065\n",
      "[378] loss: 0.065\n",
      "[379] loss: 0.065\n",
      "[380] loss: 0.065\n",
      "[381] loss: 0.065\n",
      "[382] loss: 0.065\n",
      "[383] loss: 0.065\n",
      "[384] loss: 0.065\n",
      "[385] loss: 0.065\n",
      "[386] loss: 0.065\n",
      "[387] loss: 0.065\n",
      "[388] loss: 0.065\n",
      "[389] loss: 0.065\n",
      "[390] loss: 0.065\n",
      "[391] loss: 0.065\n",
      "[392] loss: 0.065\n",
      "[393] loss: 0.064\n",
      "[394] loss: 0.064\n",
      "[395] loss: 0.064\n",
      "[396] loss: 0.064\n",
      "[397] loss: 0.064\n",
      "[398] loss: 0.064\n",
      "[399] loss: 0.064\n",
      "[400] loss: 0.064\n",
      "[401] loss: 0.064\n",
      "[402] loss: 0.064\n",
      "[403] loss: 0.064\n",
      "[404] loss: 0.064\n",
      "[405] loss: 0.064\n",
      "[406] loss: 0.064\n",
      "[407] loss: 0.064\n",
      "[408] loss: 0.064\n",
      "[409] loss: 0.064\n",
      "[410] loss: 0.064\n",
      "[411] loss: 0.064\n",
      "[412] loss: 0.064\n",
      "[413] loss: 0.064\n",
      "[414] loss: 0.064\n",
      "[415] loss: 0.064\n",
      "[416] loss: 0.064\n",
      "[417] loss: 0.064\n",
      "[418] loss: 0.064\n",
      "[419] loss: 0.064\n",
      "[420] loss: 0.064\n",
      "[421] loss: 0.064\n",
      "[422] loss: 0.064\n",
      "[423] loss: 0.064\n",
      "[424] loss: 0.064\n",
      "[425] loss: 0.064\n",
      "[426] loss: 0.063\n",
      "[427] loss: 0.063\n",
      "[428] loss: 0.063\n",
      "[429] loss: 0.063\n",
      "[430] loss: 0.063\n",
      "[431] loss: 0.063\n",
      "[432] loss: 0.063\n",
      "[433] loss: 0.063\n",
      "[434] loss: 0.063\n",
      "[435] loss: 0.063\n",
      "[436] loss: 0.063\n",
      "[437] loss: 0.063\n",
      "[438] loss: 0.063\n",
      "[439] loss: 0.063\n",
      "[440] loss: 0.063\n",
      "[441] loss: 0.063\n",
      "[442] loss: 0.063\n",
      "[443] loss: 0.063\n",
      "[444] loss: 0.063\n",
      "[445] loss: 0.063\n",
      "[446] loss: 0.063\n",
      "[447] loss: 0.063\n",
      "[448] loss: 0.063\n",
      "[449] loss: 0.063\n",
      "[450] loss: 0.063\n",
      "[451] loss: 0.063\n",
      "[452] loss: 0.063\n",
      "[453] loss: 0.063\n",
      "[454] loss: 0.063\n",
      "[455] loss: 0.063\n",
      "[456] loss: 0.063\n",
      "[457] loss: 0.063\n",
      "[458] loss: 0.063\n",
      "[459] loss: 0.063\n",
      "[460] loss: 0.063\n",
      "[461] loss: 0.063\n",
      "[462] loss: 0.063\n",
      "[463] loss: 0.063\n",
      "[464] loss: 0.063\n",
      "[465] loss: 0.063\n",
      "[466] loss: 0.063\n",
      "[467] loss: 0.062\n",
      "[468] loss: 0.062\n",
      "[469] loss: 0.062\n",
      "[470] loss: 0.062\n",
      "[471] loss: 0.062\n",
      "[472] loss: 0.062\n",
      "[473] loss: 0.062\n",
      "[474] loss: 0.062\n",
      "[475] loss: 0.062\n",
      "[476] loss: 0.062\n",
      "[477] loss: 0.062\n",
      "[478] loss: 0.062\n",
      "[479] loss: 0.062\n",
      "[480] loss: 0.062\n",
      "[481] loss: 0.062\n",
      "[482] loss: 0.062\n",
      "[483] loss: 0.062\n",
      "[484] loss: 0.062\n",
      "[485] loss: 0.062\n",
      "[486] loss: 0.062\n",
      "[487] loss: 0.062\n",
      "[488] loss: 0.062\n",
      "[489] loss: 0.062\n",
      "[490] loss: 0.062\n",
      "[491] loss: 0.062\n",
      "[492] loss: 0.062\n",
      "[493] loss: 0.062\n",
      "[494] loss: 0.062\n",
      "[495] loss: 0.062\n",
      "[496] loss: 0.062\n",
      "[497] loss: 0.062\n",
      "[498] loss: 0.061\n",
      "[499] loss: 0.061\n",
      "[500] loss: 0.061\n",
      "[501] loss: 0.061\n",
      "[502] loss: 0.061\n",
      "[503] loss: 0.061\n",
      "[504] loss: 0.061\n",
      "[505] loss: 0.061\n",
      "[506] loss: 0.061\n",
      "[507] loss: 0.061\n",
      "[508] loss: 0.061\n",
      "[509] loss: 0.061\n",
      "[510] loss: 0.061\n",
      "[511] loss: 0.061\n",
      "[512] loss: 0.061\n",
      "[513] loss: 0.061\n",
      "[514] loss: 0.061\n",
      "[515] loss: 0.061\n",
      "[516] loss: 0.061\n",
      "[517] loss: 0.061\n",
      "[518] loss: 0.061\n",
      "[519] loss: 0.061\n",
      "[520] loss: 0.061\n",
      "[521] loss: 0.061\n",
      "[522] loss: 0.061\n",
      "[523] loss: 0.061\n",
      "[524] loss: 0.061\n",
      "[525] loss: 0.061\n",
      "[526] loss: 0.061\n",
      "[527] loss: 0.061\n",
      "[528] loss: 0.061\n",
      "[529] loss: 0.061\n",
      "[530] loss: 0.061\n",
      "[531] loss: 0.061\n",
      "[532] loss: 0.061\n",
      "[533] loss: 0.061\n",
      "[534] loss: 0.061\n",
      "[535] loss: 0.061\n",
      "[536] loss: 0.061\n",
      "[537] loss: 0.061\n",
      "[538] loss: 0.061\n",
      "[539] loss: 0.061\n",
      "[540] loss: 0.061\n",
      "[541] loss: 0.061\n",
      "[542] loss: 0.061\n",
      "[543] loss: 0.061\n",
      "[544] loss: 0.061\n",
      "[545] loss: 0.061\n",
      "[546] loss: 0.061\n",
      "[547] loss: 0.061\n",
      "[548] loss: 0.061\n",
      "[549] loss: 0.061\n",
      "[550] loss: 0.061\n",
      "[551] loss: 0.061\n",
      "[552] loss: 0.061\n",
      "[553] loss: 0.061\n",
      "[554] loss: 0.061\n",
      "[555] loss: 0.061\n",
      "[556] loss: 0.061\n",
      "[557] loss: 0.061\n",
      "[558] loss: 0.061\n",
      "[559] loss: 0.061\n",
      "[560] loss: 0.061\n",
      "[561] loss: 0.061\n",
      "[562] loss: 0.061\n",
      "[563] loss: 0.061\n",
      "[564] loss: 0.061\n",
      "[565] loss: 0.061\n",
      "[566] loss: 0.061\n",
      "[567] loss: 0.061\n",
      "[568] loss: 0.061\n",
      "[569] loss: 0.061\n",
      "[570] loss: 0.060\n",
      "[571] loss: 0.060\n",
      "[572] loss: 0.060\n",
      "[573] loss: 0.060\n",
      "[574] loss: 0.060\n",
      "[575] loss: 0.060\n",
      "[576] loss: 0.060\n",
      "[577] loss: 0.060\n",
      "[578] loss: 0.060\n",
      "[579] loss: 0.060\n",
      "[580] loss: 0.060\n",
      "[581] loss: 0.060\n",
      "[582] loss: 0.060\n",
      "[583] loss: 0.060\n",
      "[584] loss: 0.060\n",
      "[585] loss: 0.060\n",
      "[586] loss: 0.060\n",
      "[587] loss: 0.060\n",
      "[588] loss: 0.060\n",
      "[589] loss: 0.060\n",
      "[590] loss: 0.060\n",
      "[591] loss: 0.060\n",
      "[592] loss: 0.060\n",
      "[593] loss: 0.060\n",
      "[594] loss: 0.060\n",
      "[595] loss: 0.060\n",
      "[596] loss: 0.060\n",
      "[597] loss: 0.060\n",
      "[598] loss: 0.060\n",
      "[599] loss: 0.060\n",
      "[600] loss: 0.060\n",
      "[601] loss: 0.060\n",
      "[602] loss: 0.060\n",
      "[603] loss: 0.060\n",
      "[604] loss: 0.060\n",
      "[605] loss: 0.060\n",
      "[606] loss: 0.060\n",
      "[607] loss: 0.060\n",
      "[608] loss: 0.060\n",
      "[609] loss: 0.060\n",
      "[610] loss: 0.060\n",
      "[611] loss: 0.060\n",
      "[612] loss: 0.060\n",
      "[613] loss: 0.060\n",
      "[614] loss: 0.060\n",
      "[615] loss: 0.060\n",
      "[616] loss: 0.060\n",
      "[617] loss: 0.060\n",
      "[618] loss: 0.060\n",
      "[619] loss: 0.060\n",
      "[620] loss: 0.060\n",
      "[621] loss: 0.060\n",
      "[622] loss: 0.060\n",
      "[623] loss: 0.060\n",
      "[624] loss: 0.060\n",
      "[625] loss: 0.060\n",
      "[626] loss: 0.060\n",
      "[627] loss: 0.060\n",
      "[628] loss: 0.060\n",
      "[629] loss: 0.060\n",
      "[630] loss: 0.060\n",
      "[631] loss: 0.059\n",
      "[632] loss: 0.059\n",
      "[633] loss: 0.059\n",
      "[634] loss: 0.059\n",
      "[635] loss: 0.059\n",
      "[636] loss: 0.059\n",
      "[637] loss: 0.059\n",
      "[638] loss: 0.059\n",
      "[639] loss: 0.059\n",
      "[640] loss: 0.059\n",
      "[641] loss: 0.059\n",
      "[642] loss: 0.059\n",
      "[643] loss: 0.059\n",
      "[644] loss: 0.059\n",
      "[645] loss: 0.059\n",
      "[646] loss: 0.059\n",
      "[647] loss: 0.059\n",
      "[648] loss: 0.059\n",
      "[649] loss: 0.059\n",
      "[650] loss: 0.059\n",
      "[651] loss: 0.059\n",
      "[652] loss: 0.059\n",
      "[653] loss: 0.059\n",
      "[654] loss: 0.059\n",
      "[655] loss: 0.059\n",
      "[656] loss: 0.059\n",
      "[657] loss: 0.059\n",
      "[658] loss: 0.059\n",
      "[659] loss: 0.059\n",
      "[660] loss: 0.059\n",
      "[661] loss: 0.059\n",
      "[662] loss: 0.059\n",
      "[663] loss: 0.059\n",
      "[664] loss: 0.059\n",
      "[665] loss: 0.059\n",
      "[666] loss: 0.059\n",
      "[667] loss: 0.059\n",
      "[668] loss: 0.059\n",
      "[669] loss: 0.059\n",
      "[670] loss: 0.059\n",
      "[671] loss: 0.059\n",
      "[672] loss: 0.059\n",
      "[673] loss: 0.059\n",
      "[674] loss: 0.059\n",
      "[675] loss: 0.059\n",
      "[676] loss: 0.059\n",
      "[677] loss: 0.059\n",
      "[678] loss: 0.059\n",
      "[679] loss: 0.059\n",
      "[680] loss: 0.059\n",
      "[681] loss: 0.059\n",
      "[682] loss: 0.059\n",
      "[683] loss: 0.059\n",
      "[684] loss: 0.059\n",
      "[685] loss: 0.059\n",
      "[686] loss: 0.059\n",
      "[687] loss: 0.059\n",
      "[688] loss: 0.059\n",
      "[689] loss: 0.059\n",
      "[690] loss: 0.059\n",
      "[691] loss: 0.059\n",
      "[692] loss: 0.059\n",
      "[693] loss: 0.059\n",
      "[694] loss: 0.059\n",
      "[695] loss: 0.059\n",
      "[696] loss: 0.059\n",
      "[697] loss: 0.059\n",
      "[698] loss: 0.059\n",
      "[699] loss: 0.059\n",
      "[700] loss: 0.059\n",
      "[701] loss: 0.059\n",
      "[702] loss: 0.059\n",
      "[703] loss: 0.059\n",
      "[704] loss: 0.059\n",
      "[705] loss: 0.059\n",
      "[706] loss: 0.059\n",
      "[707] loss: 0.059\n",
      "[708] loss: 0.059\n",
      "[709] loss: 0.059\n",
      "[710] loss: 0.059\n",
      "[711] loss: 0.059\n",
      "[712] loss: 0.059\n",
      "[713] loss: 0.059\n",
      "[714] loss: 0.059\n",
      "[715] loss: 0.059\n",
      "[716] loss: 0.059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[717] loss: 0.059\n",
      "[718] loss: 0.059\n",
      "[719] loss: 0.059\n",
      "[720] loss: 0.059\n",
      "[721] loss: 0.059\n",
      "[722] loss: 0.059\n",
      "[723] loss: 0.059\n",
      "[724] loss: 0.059\n",
      "[725] loss: 0.059\n",
      "[726] loss: 0.059\n",
      "[727] loss: 0.058\n",
      "[728] loss: 0.058\n",
      "[729] loss: 0.058\n",
      "[730] loss: 0.058\n",
      "[731] loss: 0.058\n",
      "[732] loss: 0.058\n",
      "[733] loss: 0.058\n",
      "[734] loss: 0.058\n",
      "[735] loss: 0.058\n",
      "[736] loss: 0.058\n",
      "[737] loss: 0.058\n",
      "[738] loss: 0.058\n",
      "[739] loss: 0.058\n",
      "[740] loss: 0.058\n",
      "[741] loss: 0.058\n",
      "[742] loss: 0.058\n",
      "[743] loss: 0.058\n",
      "[744] loss: 0.058\n",
      "[745] loss: 0.058\n",
      "[746] loss: 0.058\n",
      "[747] loss: 0.058\n",
      "[748] loss: 0.058\n",
      "[749] loss: 0.058\n",
      "[750] loss: 0.058\n",
      "[751] loss: 0.058\n",
      "[752] loss: 0.058\n",
      "[753] loss: 0.058\n",
      "[754] loss: 0.058\n",
      "[755] loss: 0.058\n",
      "[756] loss: 0.058\n",
      "[757] loss: 0.058\n",
      "[758] loss: 0.058\n",
      "[759] loss: 0.058\n",
      "[760] loss: 0.058\n",
      "[761] loss: 0.058\n",
      "[762] loss: 0.058\n",
      "[763] loss: 0.058\n",
      "[764] loss: 0.058\n",
      "[765] loss: 0.058\n",
      "[766] loss: 0.058\n",
      "[767] loss: 0.058\n",
      "[768] loss: 0.058\n",
      "[769] loss: 0.058\n",
      "[770] loss: 0.058\n",
      "[771] loss: 0.058\n",
      "[772] loss: 0.058\n",
      "[773] loss: 0.058\n",
      "[774] loss: 0.058\n",
      "[775] loss: 0.058\n",
      "[776] loss: 0.058\n",
      "[777] loss: 0.058\n",
      "[778] loss: 0.058\n",
      "[779] loss: 0.058\n",
      "[780] loss: 0.058\n",
      "[781] loss: 0.058\n",
      "[782] loss: 0.058\n",
      "[783] loss: 0.058\n",
      "[784] loss: 0.058\n",
      "[785] loss: 0.058\n",
      "[786] loss: 0.058\n",
      "[787] loss: 0.058\n",
      "[788] loss: 0.058\n",
      "[789] loss: 0.058\n",
      "[790] loss: 0.058\n",
      "[791] loss: 0.058\n",
      "[792] loss: 0.058\n",
      "[793] loss: 0.058\n",
      "[794] loss: 0.058\n",
      "[795] loss: 0.058\n",
      "[796] loss: 0.058\n",
      "[797] loss: 0.058\n",
      "[798] loss: 0.058\n",
      "[799] loss: 0.058\n",
      "[800] loss: 0.058\n",
      "[801] loss: 0.058\n",
      "[802] loss: 0.058\n",
      "[803] loss: 0.058\n",
      "[804] loss: 0.058\n",
      "[805] loss: 0.058\n",
      "[806] loss: 0.058\n",
      "[807] loss: 0.058\n",
      "[808] loss: 0.058\n",
      "[809] loss: 0.058\n",
      "[810] loss: 0.058\n",
      "[811] loss: 0.058\n",
      "[812] loss: 0.058\n",
      "[813] loss: 0.058\n",
      "[814] loss: 0.058\n",
      "[815] loss: 0.058\n",
      "[816] loss: 0.058\n",
      "[817] loss: 0.058\n",
      "[818] loss: 0.058\n",
      "[819] loss: 0.058\n",
      "[820] loss: 0.058\n",
      "[821] loss: 0.058\n",
      "[822] loss: 0.058\n",
      "[823] loss: 0.058\n",
      "[824] loss: 0.058\n",
      "[825] loss: 0.058\n",
      "[826] loss: 0.058\n",
      "[827] loss: 0.058\n",
      "[828] loss: 0.058\n",
      "[829] loss: 0.058\n",
      "[830] loss: 0.058\n",
      "[831] loss: 0.058\n",
      "[832] loss: 0.058\n",
      "[833] loss: 0.058\n",
      "[834] loss: 0.058\n",
      "[835] loss: 0.058\n",
      "[836] loss: 0.058\n",
      "[837] loss: 0.058\n",
      "[838] loss: 0.058\n",
      "[839] loss: 0.058\n",
      "[840] loss: 0.058\n",
      "[841] loss: 0.058\n",
      "[842] loss: 0.058\n",
      "[843] loss: 0.058\n",
      "[844] loss: 0.058\n",
      "[845] loss: 0.058\n",
      "[846] loss: 0.058\n",
      "[847] loss: 0.058\n",
      "[848] loss: 0.058\n",
      "[849] loss: 0.058\n",
      "[850] loss: 0.058\n",
      "[851] loss: 0.058\n",
      "[852] loss: 0.058\n",
      "[853] loss: 0.058\n",
      "[854] loss: 0.058\n",
      "[855] loss: 0.058\n",
      "[856] loss: 0.057\n",
      "[857] loss: 0.057\n",
      "[858] loss: 0.057\n",
      "[859] loss: 0.057\n",
      "[860] loss: 0.057\n",
      "[861] loss: 0.057\n",
      "[862] loss: 0.057\n",
      "[863] loss: 0.057\n",
      "[864] loss: 0.057\n",
      "[865] loss: 0.057\n",
      "[866] loss: 0.057\n",
      "[867] loss: 0.057\n",
      "[868] loss: 0.057\n",
      "[869] loss: 0.057\n",
      "[870] loss: 0.057\n",
      "[871] loss: 0.057\n",
      "[872] loss: 0.057\n",
      "[873] loss: 0.057\n",
      "[874] loss: 0.057\n",
      "[875] loss: 0.057\n",
      "[876] loss: 0.057\n",
      "[877] loss: 0.057\n",
      "[878] loss: 0.057\n",
      "[879] loss: 0.057\n",
      "[880] loss: 0.057\n",
      "[881] loss: 0.057\n",
      "[882] loss: 0.057\n",
      "[883] loss: 0.057\n",
      "[884] loss: 0.057\n",
      "[885] loss: 0.057\n",
      "[886] loss: 0.057\n",
      "[887] loss: 0.057\n",
      "[888] loss: 0.057\n",
      "[889] loss: 0.057\n",
      "[890] loss: 0.057\n",
      "[891] loss: 0.057\n",
      "[892] loss: 0.057\n",
      "[893] loss: 0.057\n",
      "[894] loss: 0.057\n",
      "[895] loss: 0.057\n",
      "[896] loss: 0.057\n",
      "[897] loss: 0.057\n",
      "[898] loss: 0.057\n",
      "[899] loss: 0.057\n",
      "[900] loss: 0.057\n",
      "[901] loss: 0.057\n",
      "[902] loss: 0.057\n",
      "[903] loss: 0.057\n",
      "[904] loss: 0.057\n",
      "[905] loss: 0.057\n",
      "[906] loss: 0.057\n",
      "[907] loss: 0.057\n",
      "[908] loss: 0.057\n",
      "[909] loss: 0.057\n",
      "[910] loss: 0.057\n",
      "[911] loss: 0.057\n",
      "[912] loss: 0.057\n",
      "[913] loss: 0.057\n",
      "[914] loss: 0.057\n",
      "[915] loss: 0.057\n",
      "[916] loss: 0.057\n",
      "[917] loss: 0.057\n",
      "[918] loss: 0.057\n",
      "[919] loss: 0.057\n",
      "[920] loss: 0.057\n",
      "[921] loss: 0.057\n",
      "[922] loss: 0.057\n",
      "[923] loss: 0.057\n",
      "[924] loss: 0.057\n",
      "[925] loss: 0.057\n",
      "[926] loss: 0.057\n",
      "[927] loss: 0.057\n",
      "[928] loss: 0.057\n",
      "[929] loss: 0.057\n",
      "[930] loss: 0.057\n",
      "[931] loss: 0.057\n",
      "[932] loss: 0.057\n",
      "[933] loss: 0.057\n",
      "[934] loss: 0.057\n",
      "[935] loss: 0.057\n",
      "[936] loss: 0.057\n",
      "[937] loss: 0.057\n",
      "[938] loss: 0.057\n",
      "[939] loss: 0.057\n",
      "[940] loss: 0.057\n",
      "[941] loss: 0.057\n",
      "[942] loss: 0.057\n",
      "[943] loss: 0.057\n",
      "[944] loss: 0.057\n",
      "[945] loss: 0.057\n",
      "[946] loss: 0.057\n",
      "[947] loss: 0.057\n",
      "[948] loss: 0.057\n",
      "[949] loss: 0.057\n",
      "[950] loss: 0.057\n",
      "[951] loss: 0.057\n",
      "[952] loss: 0.057\n",
      "[953] loss: 0.057\n",
      "[954] loss: 0.057\n",
      "[955] loss: 0.057\n",
      "[956] loss: 0.057\n",
      "[957] loss: 0.057\n",
      "[958] loss: 0.057\n",
      "[959] loss: 0.057\n",
      "[960] loss: 0.057\n",
      "[961] loss: 0.057\n",
      "[962] loss: 0.057\n",
      "[963] loss: 0.057\n",
      "[964] loss: 0.057\n",
      "[965] loss: 0.057\n",
      "[966] loss: 0.057\n",
      "[967] loss: 0.057\n",
      "[968] loss: 0.057\n",
      "[969] loss: 0.057\n",
      "[970] loss: 0.057\n",
      "[971] loss: 0.057\n",
      "[972] loss: 0.057\n",
      "[973] loss: 0.057\n",
      "[974] loss: 0.057\n",
      "[975] loss: 0.057\n",
      "[976] loss: 0.057\n",
      "[977] loss: 0.057\n",
      "[978] loss: 0.057\n",
      "[979] loss: 0.057\n",
      "[980] loss: 0.057\n",
      "[981] loss: 0.057\n",
      "[982] loss: 0.057\n",
      "[983] loss: 0.057\n",
      "[984] loss: 0.057\n",
      "[985] loss: 0.057\n",
      "[986] loss: 0.057\n",
      "[987] loss: 0.057\n",
      "[988] loss: 0.057\n",
      "[989] loss: 0.057\n",
      "[990] loss: 0.057\n",
      "[991] loss: 0.057\n",
      "[992] loss: 0.057\n",
      "[993] loss: 0.057\n",
      "[994] loss: 0.057\n",
      "[995] loss: 0.057\n",
      "[996] loss: 0.056\n",
      "[997] loss: 0.056\n",
      "[998] loss: 0.056\n",
      "[999] loss: 0.056\n",
      "[1000] loss: 0.056\n",
      "print trained parameters:\n",
      "Parameter containing:\n",
      " 0.0000 -0.3116  0.4476 -0.1411  0.0000\n",
      "-0.3503  0.3641 -0.4267 -0.2111  0.0000\n",
      "-0.3453 -0.1940 -0.4201  0.0000 -0.4149\n",
      "-0.1100  0.4635  0.2788  0.3271  0.2949\n",
      "-0.4770 -0.3487 -0.3141 -0.2742  0.0000\n",
      " 0.4323 -0.1152  0.3731  0.3104  0.0000\n",
      "-0.2794 -0.4027  0.1108  0.3793 -0.2245\n",
      "-0.3891  0.4377 -0.1120  0.4030 -0.2948\n",
      "-0.2102 -0.4017 -0.1128 -0.3762 -0.2422\n",
      "-0.4774  0.2289  0.3663  0.1248  0.0000\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n",
      "Parameter containing:\n",
      "-0.3408\n",
      "-0.3457\n",
      " 0.0000\n",
      "-0.1539\n",
      " 0.1702\n",
      " 0.2217\n",
      " 0.3571\n",
      " 0.1840\n",
      "-0.3840\n",
      "-0.1308\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "Parameter containing:\n",
      "-0.2007  0.0000  0.0000  0.0000 -0.1708 -0.1730  0.1661  0.0000  0.2165  0.0000\n",
      " 0.0000  0.0000  0.2465  0.1026  0.2441  0.1531  0.0000  0.0000  0.1554 -0.1630\n",
      " 0.0000  0.3280 -0.1545 -0.2521  0.2147  0.0000 -0.1909  0.0000  0.3738  0.3364\n",
      "[torch.FloatTensor of size 3x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.0000\n",
      " 0.1762\n",
      "-0.2439\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2374  0.0000 -0.4595\n",
      " 0.3069 -0.3144  0.0000\n",
      "-0.3573  0.4443  0.1932\n",
      " 0.0000  0.2296  0.0000\n",
      " 0.0000  0.2102  0.5510\n",
      " 0.2390  0.2292  0.3719\n",
      "-0.4440  0.2944 -0.3153\n",
      " 0.3221  0.0000 -0.4727\n",
      " 0.3208  0.4073  0.4286\n",
      "-0.3333 -0.3691 -0.4640\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "Parameter containing:\n",
      "-0.4863\n",
      "-0.5219\n",
      " 0.0000\n",
      "-0.2523\n",
      "-0.1735\n",
      " 0.0000\n",
      " 0.2911\n",
      "-0.4087\n",
      " 0.2007\n",
      "-0.1728\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2644  0.0000  0.0000  0.0000 -0.3909  0.0000 -0.1127  0.2404  0.0000  0.0000\n",
      " 0.2321 -0.3128  0.0000 -0.1855 -0.1868  0.0000  0.0000  0.1790  0.1186  0.0000\n",
      " 0.0000  0.0000 -0.3969  0.0000  0.0000  0.0000  0.0000 -0.2903  0.0000  0.0000\n",
      "-0.2422 -0.3004  0.0000 -0.3185  0.0000  0.0000  0.3679  0.1607  0.0000  0.0000\n",
      " 0.0000  0.1787  0.0000  0.2113 -0.3348  0.0000 -0.1056  0.0000 -0.3131  0.2079\n",
      "[torch.FloatTensor of size 5x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2990\n",
      " 0.2023\n",
      " 0.4378\n",
      " 0.0000\n",
      " 0.2862\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss0   = 0\n",
    "epsilon = 0.00001\n",
    "print('print initial parameters:')\n",
    "for i in net.parameters():\n",
    "    print(i)\n",
    "    \n",
    "for epoch in range(100):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out  = net.forward(data)\n",
    "    loss = criterion(out,data)\n",
    "    print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1, loss.data[0]))\n",
    "    if abs(loss.data[0]-loss0) < epsilon:\n",
    "        break\n",
    "    loss0 = loss.data[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()        # Does the update\n",
    "    \n",
    "for epoch in range(1000):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out  = net.forward(data)\n",
    "    loss = criterion(out,data)\n",
    "    print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1, loss.data[0]))\n",
    "    #if abs(loss.data[0]-loss0) < epsilon:\n",
    "        #break\n",
    "    loss0 = loss.data[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()        # Does the update\n",
    "    for i in net.parameters():\n",
    "        para = i.data\n",
    "        mask = (torch.abs(para)<0.1)\n",
    "        mask = mask.type(torch.FloatTensor)\n",
    "        i.data.addcmul_(-1.0,i.data,mask)\n",
    "\n",
    "        \n",
    "print('print trained parameters:')\n",
    "for i in net.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis\n",
    "To show the relation ship between each col of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   7.07733549e-01   5.27606298e-04   2.87293145e-03\n",
      "    3.53224538e-03]\n",
      " [  7.07733549e-01   1.00000000e+00   7.06852764e-01   5.01114907e-01\n",
      "    2.17728536e-03]\n",
      " [  5.27606298e-04   7.06852764e-01   1.00000000e+00   7.06436164e-01\n",
      "   -4.54771235e-04]\n",
      " [  2.87293145e-03   5.01114907e-01   7.06436164e-01   1.00000000e+00\n",
      "    7.07455423e-01]\n",
      " [  3.53224538e-03   2.17728536e-03  -4.54771235e-04   7.07455423e-01\n",
      "    1.00000000e+00]]\n",
      "[[ 0.          0.          0.86749462  0.36361875  0.26400134]\n",
      " [ 0.          0.          0.          0.          0.49113149]\n",
      " [ 0.86749462  0.          0.          0.          0.8856509 ]\n",
      " [ 0.36361875  0.          0.          0.          0.        ]\n",
      " [ 0.26400134  0.49113149  0.8856509   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "r = np.zeros([5,5])\n",
    "p = np.zeros([5,5])\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        r[i,j],p[i,j] = stats.pearsonr(np_data[:,i],np_data[:,j])\n",
    "print(r)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "Density-based spatial clustering of applications with noise is a cluster algorithm proposed by Martin Ester .etc. It is able to find cluster in any shape and does not need to specify the number of clusters in advance.\n",
    "\n",
    "We require that for every point p in a cluster C there is a point q in C so that p is inside of the Eps-\n",
    "neighborhood of q and NEps(q) contains at least MinPts points.\n",
    "\n",
    "We use the scikit-learn tool kits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 3\n",
      "Silhouette Coefficient: 0.962\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nASbK094AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d24f00da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# extract continuous data\n",
    "continuous_data = post_data[:,0:11]\n",
    "X               = MinMaxScaler().fit_transform(continuous_data)\n",
    "# #############################################################################\n",
    "# 5-NN distance\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.2, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "#      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAN classifier\n",
    "TAN classifier is the Tree Augmented Naive Baysian, which is created by adding a spaned tree into naive baysian network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class node:\n",
    "    '''\n",
    "    The class to store the information of a node in a graph\n",
    "    '''\n",
    "    def __init__(self,name,domain):\n",
    "        self.name          = name       #a string\n",
    "        self.domain        = domain     #a list for string\n",
    "        self.father        = []         #a list\n",
    "        self.children      = []         #a list\n",
    "        self.pro_table     = {}         #init as an empty dict\n",
    "                                        #for example\n",
    "                                        #{(on,on):[0.1,0.9],(on,off):[0.2,0.8],(off,on):[0.3,0.7],(off,off):[0.4,0.6]}\n",
    "        \n",
    "    def add_father(self,fa):            #fa is a node\n",
    "        self.father.append(fa)\n",
    "        \n",
    "    def add_child(self,ch):             #ch is a node\n",
    "        self.children.append(ch)\n",
    "        \n",
    "    def add_pro(self,instance,table):   #instance is a string list, table is a float list\n",
    "        self.pro_table[instance] = table\n",
    "        \n",
    "class graph:\n",
    "    '''\n",
    "    The class to store a graph\n",
    "    The code will not check the validation of data, so please preprocess the data to ensure\n",
    "    1) there are heandings: name1,name2,...\n",
    "    2) no empty data\n",
    "    3) all data is discretized\n",
    "    4) last colum is the label\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.nodes         = []         #a list\n",
    "        \n",
    "    def get_node(self,the_node):\n",
    "        '''\n",
    "        functional function\n",
    "        get the node by name\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "        output:\n",
    "            type         description\n",
    "            ------------------------------\n",
    "            class node,  the found node\n",
    "        '''\n",
    "        names              = [i.name for i in self.nodes]\n",
    "        node_index         = names.index(the_node)\n",
    "        return self.nodes[node_index]\n",
    "    \n",
    "    def get_node_index(self,the_node):\n",
    "        '''\n",
    "        functional function\n",
    "        get the node by name\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "        output:\n",
    "            type         description\n",
    "            ------------------------------\n",
    "            int,         index\n",
    "        '''\n",
    "        names              = [i.name for i in self.nodes]\n",
    "        node_index         = names.index(the_node)\n",
    "        return node_index\n",
    "        \n",
    "    def add_node(self,name,domain):\n",
    "        '''\n",
    "        functional function\n",
    "        add a node by name and domain\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            name,     string, node name\n",
    "            domain,   list,   domain of the node\n",
    "        '''\n",
    "        self.nodes.append(node(name,domain))\n",
    "        \n",
    "    def add_connection(self,from_node1,to_node2):#both from_node1 and to_node2 are strings\n",
    "        '''\n",
    "        functional function\n",
    "        add a connection betwen to nodes\n",
    "        input:\n",
    "            variable     type         description\n",
    "            -------------------------------------------\n",
    "            from_node1,  class node,  the first node\n",
    "            to_node2,    class node,  the second node\n",
    "        '''\n",
    "        from_node1         = self.get_node(from_node1)\n",
    "        to_node2           = self.get_node(to_node2)\n",
    "        \n",
    "        from_node1.add_child(to_node2)\n",
    "        to_node2.add_father(from_node1)\n",
    "        \n",
    "    def add_pro(self,the_node,instance,table):#the_node is a string\n",
    "        '''\n",
    "        functional function\n",
    "        add a conditioanal probability P(a|b) into a node\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "            instance, list,   b part in P(a|b)\n",
    "                              where each entry in it is a value in domain\n",
    "            table,    list,   a part in P(a|b)\n",
    "                              where each entry in it is a probability responding to a value\n",
    "        '''\n",
    "        the_node           = self.get_node(the_node)\n",
    "        the_node.add_pro(instance,table)\n",
    "        \n",
    "    def get_nodes_from_data(self,headings,data):#headings = ['data1','data2',...], data = numpy 2d vector\n",
    "        '''\n",
    "        interface/functional function\n",
    "        get the basic information about node from data and insert them into the class\n",
    "        input:\n",
    "            variable  type            description\n",
    "            ------------------------------------------------\n",
    "            headings, numpy arrary,   the names of variables\n",
    "            data,     numpy 2darrary, all the data where row means different instance\n",
    "                                      and col means each sensor\n",
    "        '''\n",
    "        for i,name in enumerate(headings):\n",
    "            domain = list(set(data[:,i]))\n",
    "            self.add_node(name,domain)\n",
    "            \n",
    "    def mutual_information(self,data1,data2):\n",
    "        '''\n",
    "        functional function\n",
    "        get the mutual information between two cols\n",
    "        input:\n",
    "            variable  type            description\n",
    "            ------------------------------------------------\n",
    "            data1,    numpy arrary,   data of the first col\n",
    "            data2,    numpy arrary,   data of the second col\n",
    "        output:\n",
    "            type    description\n",
    "            -----------------------------\n",
    "            float,  mutual information entropy with laplace smooth\n",
    "        '''\n",
    "        assert(len(data1)==len(data2))\n",
    "        \n",
    "        mi       = 0.0\n",
    "        length   = len(data1)\n",
    "        \n",
    "        domain1 = list(set(data1))\n",
    "        domain2 = list(set(data2))\n",
    "        len1    = len(domain1)\n",
    "        len2    = len(domain2)\n",
    "        \n",
    "        for x in domain1:\n",
    "            for y in domain2:\n",
    "                Px     = float(data1.count(x) + 1)/(length+len1)    #laplace smooth\n",
    "                Py     = float(data2.count(y) + 1)/(length+len2)    #laplace smooth\n",
    "                \n",
    "                indexxy= [1 if (i==x)and(j==y) else 0 for i,j in zip(data1,data2)]\n",
    "                Pxy    = float(sum(indexxy) + 1)/(length+len1*len2) #laplace smooth\n",
    "                \n",
    "                mi     = mi + Pxy*math.log(Pxy/(Px*Py))\n",
    "        return mi\n",
    "    \n",
    "    def get_max_span_tree_prime(self,mi_dict):\n",
    "        '''\n",
    "        functional function\n",
    "        judge if some edges compose a span tree\n",
    "        input:\n",
    "            variable  type    description\n",
    "            -----------------------------\n",
    "            edges,    dict,   all the edges\n",
    "        output:\n",
    "            type    description\n",
    "            -----------------------------\n",
    "            set,    all the edges that compose a max span tree\n",
    "        '''\n",
    "        #prepare structure to qurey\n",
    "        Vnew  = set(max(mi_dict.items(),key=lambda d: d[1])[0][0])\n",
    "        V     = set()\n",
    "        Enew  = set()\n",
    "        edges  = defaultdict(list)\n",
    "        for i in mi_dict:\n",
    "            edges[i[0]].append(i[1])\n",
    "            edges[i[1]].append(i[0])\n",
    "            V.add(i[0])\n",
    "            V.add(i[1])\n",
    "        while Vnew != V:\n",
    "            new_edges = {}\n",
    "            for u in Vnew:\n",
    "                v = [i for i in edges[u] if i not in Vnew]\n",
    "                for vi in v:\n",
    "                    edge = (u,vi) if (u,vi) in mi_dict else (vi,u)\n",
    "                    new_edges[edge] = mi_dict[edge]\n",
    "            new_edges = sorted(new_edges.items(),key=lambda d:d[1],reverse = True)\n",
    "            best_edge = new_edges[0]\n",
    "            Enew.add(best_edge[0])\n",
    "            Vnew.add(best_edge[0][0])\n",
    "            Vnew.add(best_edge[0][1])\n",
    "        return Enew\n",
    "        \n",
    "        \n",
    "    def get_basic_tan_structure(self,headings,data):#headings = ['data1','data2',...], data = numpy 2d vector\n",
    "        '''\n",
    "        interface function\n",
    "        get the basic structure of TAN and connect the nodes in the class.\n",
    "        we assumpe that the last col is the label colum and is ignored when handle features\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "        '''\n",
    "        #computing mutual information entropy between features\n",
    "        mi_dict = {}\n",
    "        for i in range(len(headings)-1):\n",
    "            for j in range(i+1,len(headings)-1):\n",
    "                data1   = data[:,i]\n",
    "                data2   = data[:,j]\n",
    "                mi      = self.mutual_information(data1,data2)\n",
    "                mi_dict[(headings[1],headings[2])] = mi\n",
    "        \n",
    "        #set maximal span tree, mst\n",
    "        mst             = self.get_max_span_tree_prime(mi_dict)\n",
    "        Visited         = set()\n",
    "        edges           = defaultdict(list)\n",
    "        for i in mst:\n",
    "            edges[i[0]].append(i[1])\n",
    "            edges[i[1]].append(i[0])\n",
    "        initial_node    = self.node[0].name\n",
    "        start           = [initial_node]\n",
    "        Visited.add(initial_node)\n",
    "        while len(start)!=0:\n",
    "            new_start = []\n",
    "            for i in start:\n",
    "                ends = edges[i]\n",
    "                for j in ends:\n",
    "                    if j not in Visited:\n",
    "                        Visited.add(j)\n",
    "                        new_start.append(j)\n",
    "                        self.add_connection(i,j)\n",
    "            start = new_start\n",
    "        \n",
    "        #set the connection between label and features\n",
    "        for i in range(len(self.nodes)-1):\n",
    "            self.add_connection(self.node[-1],self.node[i])\n",
    "            \n",
    "    def get_cpt(self,headings,data,node_name):\n",
    "        '''\n",
    "        interface/functional function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "            node_name,string,         the node name\n",
    "        '''\n",
    "        the_index     = self.get_node_index(node_name)\n",
    "        the_node      = self.nodes[the_index]\n",
    "        father_nodes  = the_node.father\n",
    "        \n",
    "        domain_x = the_node.domain\n",
    "        data_x   = data[:,the_index]\n",
    "        len_x    = len(domain_x)\n",
    "        \n",
    "        length   = len(data)\n",
    "        \n",
    "        if len(father_nodes) == 0:\n",
    "            return\n",
    "        else if len(father_nodes)==1:#y-->x\n",
    "            domain_y = father_nodes[0].domain\n",
    "            data_y   = data[:,self.get_node_index(father_nodes[0].name)]\n",
    "            len_y    = len(domain_y)\n",
    "            \n",
    "            for y in domain_y:\n",
    "                table    = []\n",
    "                Py     = float(data_y.count(y) + 1)/(length+len_y)    #laplace smooth\n",
    "                for x in domain_x:\n",
    "                    indexxy= [1 if (i==x)and(j==y) else 0 for i,j in zip(data_x,data_y)]\n",
    "                    Pxy    = float(sum(indexxy) + 1)/(length+len_x*len_y) #laplace smooth\n",
    "                    table.append(float(Pxy)/Py)\n",
    "                self.add_pro(node_name,(y),table)\n",
    "                    \n",
    "        else if len(father_nodes)==2:\n",
    "            domain_y = father_nodes[0].domain\n",
    "            data_y   = data[:,self.get_node_index(father_nodes[0].name)]\n",
    "            len_y    = len(domain_y)\n",
    "            domain_z = father_nodes[1].domain\n",
    "            data_z   = data[:,self.get_node_index(father_nodes[1].name)]\n",
    "            len_z    = len(domain_z)\n",
    "            \n",
    "            for y in domain_y:\n",
    "                for z in domian_z:\n",
    "                    table = []\n",
    "                    indexyz= [1 if (i==y)and(j==z) else 0 for i,j in zip(data_y,data_z)]\n",
    "                    Pyz    = float(sum(indexyz) + 1)/(length+len_y*len_z) #laplace smooth\n",
    "                    for x in domain_x:\n",
    "                        indexxyz= [1 if (i==y)and(j==z)and(k==x) else 0 for i,j,k in zip(data_y,data_z,data_x)]\n",
    "                        Pxyz    = float(sum(indexxyz) + 1)/(length+len_y*len_z*len_x) #laplace smooth\n",
    "                        table.append(float(Pxyz)/Pyz)\n",
    "                    self.add_pro(node_name,(y,z),table)\n",
    "        else:\n",
    "            assert(len(father_nodes)<=2\n",
    "\n",
    "        \n",
    "    def train(self,headings,data):\n",
    "        '''\n",
    "        interface function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
