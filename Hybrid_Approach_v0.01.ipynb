{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Approach V0.01\n",
    "\n",
    "This document records the process to implement the first simple version of hybrid approach based on G.B.'s approach.\n",
    "\n",
    "## Auto encoder decoder\n",
    "This subsetion describes how to design an auto encoder decoder to extract features of data.\n",
    "\n",
    "### What is auto encoder decoder ?\n",
    "Auto encoder decoder is a special artificial neural network that tranlate input data and ouput the same/similar data. Because the there is no information lost in the translating process, we can use the data inthe middle layer as the feathures of the input data.\n",
    "\n",
    "When middle layer's hidden variables are less than input variables, auto encoder decoder compresses the data.\n",
    "When middle layer's hidden variables are more than input variables, auto encoder decoder represents data sparsely.\n",
    "\n",
    "#### Auto encoder decoder test\n",
    "##### Test data\n",
    "   Test data comes from a simple function. \n",
    "+ Generate feature vector randomly\n",
    "+ Generate observation data\n",
    "[v1,v2,...,vn]   ==>   [v1,((v1+v2)/2),v2,((v2+v3)/2),...,vn] and then max min normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import torch\n",
    "\n",
    "#input dimension\n",
    "D = 3\n",
    "#input number\n",
    "N = 100000\n",
    "\n",
    "\n",
    "feature = random.rand(N,D)\n",
    "def transfer(input):\n",
    "    n,dim   = input.shape\n",
    "    new_dim = dim*2-1\n",
    "    output  = np.zeros((n,new_dim))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(new_dim):\n",
    "            if j%2 == 0:\n",
    "                output[i][j] = input[i][int(j/2)]\n",
    "            else:\n",
    "                output[i][j] = ((input[i][int(j/2)]+input[i][int(j/2)+1])/2)\n",
    "    return output\n",
    "\n",
    "np_test_data   = transfer(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Air Unit Data\n",
    "Read and pre-process data by panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "file_name = './OAU1201.csv'\n",
    "raw_oau_data  = pandas.read_csv(file_name)\n",
    "\n",
    "oau_headings = np.array(raw_oau_data.columns[1:])\n",
    "oau_data     = np.array(raw_oau_data[oau_headings])\n",
    "\n",
    "#delete nan\n",
    "mask     = [[1 if type(j)==float and np.isnan(j) else 0 for j in i] for i in oau_data]\n",
    "oau_data = np.array([i for i,j in zip(oau_data,mask) if sum(j)==0])\n",
    "\n",
    "# #############################################################################\n",
    "# extract continuous data\n",
    "continuous_oau_data = oau_data[:,0:12]\n",
    "new_data        = oau_data\n",
    "for i in range(12,len(oau_data[0])):\n",
    "    dis_data = oau_data[:,i]\n",
    "    domain   = list(set(dis_data))\n",
    "    new_data[:,i] = [domain.index(k) for k in dis_data]\n",
    "\n",
    "#print(new_data)\n",
    "    \n",
    "norm_oau_data                    = MinMaxScaler().fit_transform(new_data)\n",
    "continuous_norm_oau_data         = MinMaxScaler().fit_transform(continuous_oau_data)\n",
    "discrete_oau_data                = oau_data[:,12:]\n",
    "# #############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### auto encoder decoder structure design\n",
    "Now, we just use some simple structure.\n",
    "      \n",
    "The structure works!\n",
    "The \"works\" mean just works! But the result is bad! The good thing is that we can start now. The bad thing is that the structure need to be re-designed!\n",
    "\n",
    "TODO: use practical data and design new auto encoder decoder structrue. Quite importance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_endecoder (\n",
      "  (in1): Linear (26 -> 6)\n",
      "  (out1): Linear (6 -> 26)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#input_dim  = len(continuous_norm_oau_data[0])\n",
    "input_dim  = len(norm_oau_data[0])\n",
    "\n",
    "class simple_endecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_endecoder,self).__init__()\n",
    "        #linear\n",
    "        self.in1  = nn.Linear(input_dim,int(input_dim/4))\n",
    "\n",
    "        self.out1 = nn.Linear(int(input_dim/4),input_dim)            \n",
    "    def forward(self,x):\n",
    "        x = self.in1(x)\n",
    "\n",
    "        x = self.out1(x)\n",
    "        return x\n",
    "    \n",
    "    def encode_data(self,x):\n",
    "        x = self.in1(x)\n",
    "        return x\n",
    "    \n",
    "net       = simple_endecoder()\n",
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.555\n",
      "[2] loss: 0.548\n",
      "[3] loss: 0.541\n",
      "[4] loss: 0.534\n",
      "[5] loss: 0.528\n",
      "[6] loss: 0.521\n",
      "[7] loss: 0.515\n",
      "[8] loss: 0.509\n",
      "[9] loss: 0.503\n",
      "[10] loss: 0.498\n",
      "[11] loss: 0.492\n",
      "[12] loss: 0.487\n",
      "[13] loss: 0.482\n",
      "[14] loss: 0.476\n",
      "[15] loss: 0.472\n",
      "[16] loss: 0.467\n",
      "[17] loss: 0.462\n",
      "[18] loss: 0.457\n",
      "[19] loss: 0.453\n",
      "[20] loss: 0.448\n",
      "[21] loss: 0.444\n",
      "[22] loss: 0.439\n",
      "[23] loss: 0.435\n",
      "[24] loss: 0.431\n",
      "[25] loss: 0.427\n",
      "[26] loss: 0.423\n",
      "[27] loss: 0.419\n",
      "[28] loss: 0.416\n",
      "[29] loss: 0.412\n",
      "[30] loss: 0.408\n",
      "[31] loss: 0.405\n",
      "[32] loss: 0.402\n",
      "[33] loss: 0.398\n",
      "[34] loss: 0.395\n",
      "[35] loss: 0.391\n",
      "[36] loss: 0.388\n",
      "[37] loss: 0.385\n",
      "[38] loss: 0.382\n",
      "[39] loss: 0.379\n",
      "[40] loss: 0.376\n",
      "[41] loss: 0.373\n",
      "[42] loss: 0.370\n",
      "[43] loss: 0.367\n",
      "[44] loss: 0.364\n",
      "[45] loss: 0.361\n",
      "[46] loss: 0.359\n",
      "[47] loss: 0.356\n",
      "[48] loss: 0.353\n",
      "[49] loss: 0.350\n",
      "[50] loss: 0.348\n",
      "[51] loss: 0.345\n",
      "[52] loss: 0.343\n",
      "[53] loss: 0.340\n",
      "[54] loss: 0.338\n",
      "[55] loss: 0.335\n",
      "[56] loss: 0.333\n",
      "[57] loss: 0.331\n",
      "[58] loss: 0.328\n",
      "[59] loss: 0.326\n",
      "[60] loss: 0.324\n",
      "[61] loss: 0.321\n",
      "[62] loss: 0.319\n",
      "[63] loss: 0.317\n",
      "[64] loss: 0.315\n",
      "[65] loss: 0.313\n",
      "[66] loss: 0.310\n",
      "[67] loss: 0.308\n",
      "[68] loss: 0.306\n",
      "[69] loss: 0.304\n",
      "[70] loss: 0.302\n",
      "[71] loss: 0.300\n",
      "[72] loss: 0.298\n",
      "[73] loss: 0.296\n",
      "[74] loss: 0.294\n",
      "[75] loss: 0.292\n",
      "[76] loss: 0.290\n",
      "[77] loss: 0.288\n",
      "[78] loss: 0.287\n",
      "[79] loss: 0.285\n",
      "[80] loss: 0.283\n",
      "[81] loss: 0.281\n",
      "[82] loss: 0.279\n",
      "[83] loss: 0.277\n",
      "[84] loss: 0.276\n",
      "[85] loss: 0.274\n",
      "[86] loss: 0.272\n",
      "[87] loss: 0.270\n",
      "[88] loss: 0.269\n",
      "[89] loss: 0.267\n",
      "[90] loss: 0.265\n",
      "[91] loss: 0.263\n",
      "[92] loss: 0.262\n",
      "[93] loss: 0.260\n",
      "[94] loss: 0.258\n",
      "[95] loss: 0.257\n",
      "[96] loss: 0.255\n",
      "[97] loss: 0.253\n",
      "[98] loss: 0.252\n",
      "[99] loss: 0.250\n",
      "[100] loss: 0.249\n",
      "[1] loss: 0.247\n",
      "[2] loss: 0.246\n",
      "[3] loss: 0.244\n",
      "[4] loss: 0.242\n",
      "[5] loss: 0.241\n",
      "[6] loss: 0.239\n",
      "[7] loss: 0.238\n",
      "[8] loss: 0.236\n",
      "[9] loss: 0.235\n",
      "[10] loss: 0.233\n",
      "[11] loss: 0.232\n",
      "[12] loss: 0.230\n",
      "[13] loss: 0.229\n",
      "[14] loss: 0.227\n",
      "[15] loss: 0.226\n",
      "[16] loss: 0.225\n",
      "[17] loss: 0.223\n",
      "[18] loss: 0.222\n",
      "[19] loss: 0.220\n",
      "[20] loss: 0.219\n",
      "[21] loss: 0.218\n",
      "[22] loss: 0.216\n",
      "[23] loss: 0.215\n",
      "[24] loss: 0.213\n",
      "[25] loss: 0.212\n",
      "[26] loss: 0.211\n",
      "[27] loss: 0.209\n",
      "[28] loss: 0.208\n",
      "[29] loss: 0.207\n",
      "[30] loss: 0.205\n",
      "[31] loss: 0.204\n",
      "[32] loss: 0.203\n",
      "[33] loss: 0.201\n",
      "[34] loss: 0.200\n",
      "[35] loss: 0.199\n",
      "[36] loss: 0.198\n",
      "[37] loss: 0.196\n",
      "[38] loss: 0.195\n",
      "[39] loss: 0.194\n",
      "[40] loss: 0.192\n",
      "[41] loss: 0.191\n",
      "[42] loss: 0.190\n",
      "[43] loss: 0.189\n",
      "[44] loss: 0.188\n",
      "[45] loss: 0.186\n",
      "[46] loss: 0.185\n",
      "[47] loss: 0.184\n",
      "[48] loss: 0.183\n",
      "[49] loss: 0.182\n",
      "[50] loss: 0.180\n",
      "[51] loss: 0.179\n",
      "[52] loss: 0.178\n",
      "[53] loss: 0.177\n",
      "[54] loss: 0.176\n",
      "[55] loss: 0.175\n",
      "[56] loss: 0.173\n",
      "[57] loss: 0.172\n",
      "[58] loss: 0.171\n",
      "[59] loss: 0.170\n",
      "[60] loss: 0.169\n",
      "[61] loss: 0.168\n",
      "[62] loss: 0.167\n",
      "[63] loss: 0.166\n",
      "[64] loss: 0.165\n",
      "[65] loss: 0.163\n",
      "[66] loss: 0.162\n",
      "[67] loss: 0.161\n",
      "[68] loss: 0.160\n",
      "[69] loss: 0.159\n",
      "[70] loss: 0.158\n",
      "[71] loss: 0.157\n",
      "[72] loss: 0.156\n",
      "[73] loss: 0.155\n",
      "[74] loss: 0.154\n",
      "[75] loss: 0.153\n",
      "[76] loss: 0.152\n",
      "[77] loss: 0.151\n",
      "[78] loss: 0.150\n",
      "[79] loss: 0.149\n",
      "[80] loss: 0.148\n",
      "[81] loss: 0.147\n",
      "[82] loss: 0.146\n",
      "[83] loss: 0.145\n",
      "[84] loss: 0.144\n",
      "[85] loss: 0.143\n",
      "[86] loss: 0.142\n",
      "[87] loss: 0.141\n",
      "[88] loss: 0.140\n",
      "[89] loss: 0.139\n",
      "[90] loss: 0.138\n",
      "[91] loss: 0.137\n",
      "[92] loss: 0.136\n",
      "[93] loss: 0.136\n",
      "[94] loss: 0.135\n",
      "[95] loss: 0.134\n",
      "[96] loss: 0.133\n",
      "[97] loss: 0.132\n",
      "[98] loss: 0.131\n",
      "[99] loss: 0.130\n",
      "[100] loss: 0.129\n",
      "[101] loss: 0.128\n",
      "[102] loss: 0.128\n",
      "[103] loss: 0.127\n",
      "[104] loss: 0.126\n",
      "[105] loss: 0.125\n",
      "[106] loss: 0.124\n",
      "[107] loss: 0.123\n",
      "[108] loss: 0.122\n",
      "[109] loss: 0.122\n",
      "[110] loss: 0.121\n",
      "[111] loss: 0.120\n",
      "[112] loss: 0.119\n",
      "[113] loss: 0.118\n",
      "[114] loss: 0.118\n",
      "[115] loss: 0.117\n",
      "[116] loss: 0.116\n",
      "[117] loss: 0.115\n",
      "[118] loss: 0.114\n",
      "[119] loss: 0.114\n",
      "[120] loss: 0.113\n",
      "[121] loss: 0.112\n",
      "[122] loss: 0.111\n",
      "[123] loss: 0.111\n",
      "[124] loss: 0.110\n",
      "[125] loss: 0.109\n",
      "[126] loss: 0.108\n",
      "[127] loss: 0.108\n",
      "[128] loss: 0.107\n",
      "[129] loss: 0.106\n",
      "[130] loss: 0.105\n",
      "[131] loss: 0.105\n",
      "[132] loss: 0.104\n",
      "[133] loss: 0.103\n",
      "[134] loss: 0.103\n",
      "[135] loss: 0.102\n",
      "[136] loss: 0.101\n",
      "[137] loss: 0.101\n",
      "[138] loss: 0.100\n",
      "[139] loss: 0.099\n",
      "[140] loss: 0.099\n",
      "[141] loss: 0.098\n",
      "[142] loss: 0.097\n",
      "[143] loss: 0.097\n",
      "[144] loss: 0.096\n",
      "[145] loss: 0.095\n",
      "[146] loss: 0.095\n",
      "[147] loss: 0.094\n",
      "[148] loss: 0.093\n",
      "[149] loss: 0.093\n",
      "[150] loss: 0.092\n",
      "[151] loss: 0.092\n",
      "[152] loss: 0.091\n",
      "[153] loss: 0.090\n",
      "[154] loss: 0.090\n",
      "[155] loss: 0.089\n",
      "[156] loss: 0.089\n",
      "[157] loss: 0.088\n",
      "[158] loss: 0.087\n",
      "[159] loss: 0.087\n",
      "[160] loss: 0.086\n",
      "[161] loss: 0.086\n",
      "[162] loss: 0.085\n",
      "[163] loss: 0.084\n",
      "[164] loss: 0.084\n",
      "[165] loss: 0.083\n",
      "[166] loss: 0.083\n",
      "[167] loss: 0.082\n",
      "[168] loss: 0.082\n",
      "[169] loss: 0.081\n",
      "[170] loss: 0.081\n",
      "[171] loss: 0.080\n",
      "[172] loss: 0.080\n",
      "[173] loss: 0.079\n",
      "[174] loss: 0.079\n",
      "[175] loss: 0.078\n",
      "[176] loss: 0.078\n",
      "[177] loss: 0.077\n",
      "[178] loss: 0.077\n",
      "[179] loss: 0.076\n",
      "[180] loss: 0.076\n",
      "[181] loss: 0.075\n",
      "[182] loss: 0.075\n",
      "[183] loss: 0.074\n",
      "[184] loss: 0.074\n",
      "[185] loss: 0.073\n",
      "[186] loss: 0.073\n",
      "[187] loss: 0.072\n",
      "[188] loss: 0.072\n",
      "[189] loss: 0.071\n",
      "[190] loss: 0.071\n",
      "[191] loss: 0.070\n",
      "[192] loss: 0.070\n",
      "[193] loss: 0.069\n",
      "[194] loss: 0.069\n",
      "[195] loss: 0.069\n",
      "[196] loss: 0.068\n",
      "[197] loss: 0.068\n",
      "[198] loss: 0.067\n",
      "[199] loss: 0.067\n",
      "[200] loss: 0.066\n",
      "[201] loss: 0.066\n",
      "[202] loss: 0.066\n",
      "[203] loss: 0.065\n",
      "[204] loss: 0.065\n",
      "[205] loss: 0.064\n",
      "[206] loss: 0.064\n",
      "[207] loss: 0.064\n",
      "[208] loss: 0.063\n",
      "[209] loss: 0.063\n",
      "[210] loss: 0.062\n",
      "[211] loss: 0.062\n",
      "[212] loss: 0.062\n",
      "[213] loss: 0.061\n",
      "[214] loss: 0.061\n",
      "[215] loss: 0.061\n",
      "[216] loss: 0.060\n",
      "[217] loss: 0.060\n",
      "[218] loss: 0.059\n",
      "[219] loss: 0.059\n",
      "[220] loss: 0.059\n",
      "[221] loss: 0.058\n",
      "[222] loss: 0.058\n",
      "[223] loss: 0.058\n",
      "[224] loss: 0.057\n",
      "[225] loss: 0.057\n",
      "[226] loss: 0.057\n",
      "[227] loss: 0.056\n",
      "[228] loss: 0.056\n",
      "[229] loss: 0.056\n",
      "[230] loss: 0.055\n",
      "[231] loss: 0.055\n",
      "[232] loss: 0.055\n",
      "[233] loss: 0.054\n",
      "[234] loss: 0.054\n",
      "[235] loss: 0.054\n",
      "[236] loss: 0.053\n",
      "[237] loss: 0.053\n",
      "[238] loss: 0.053\n",
      "[239] loss: 0.053\n",
      "[240] loss: 0.052\n",
      "[241] loss: 0.052\n",
      "[242] loss: 0.052\n",
      "[243] loss: 0.051\n",
      "[244] loss: 0.051\n",
      "[245] loss: 0.051\n",
      "[246] loss: 0.050\n",
      "[247] loss: 0.050\n",
      "[248] loss: 0.050\n",
      "[249] loss: 0.050\n",
      "[250] loss: 0.049\n",
      "[251] loss: 0.049\n",
      "[252] loss: 0.049\n",
      "[253] loss: 0.049\n",
      "[254] loss: 0.048\n",
      "[255] loss: 0.048\n",
      "[256] loss: 0.048\n",
      "[257] loss: 0.048\n",
      "[258] loss: 0.047\n",
      "[259] loss: 0.047\n",
      "[260] loss: 0.047\n",
      "[261] loss: 0.046\n",
      "[262] loss: 0.046\n",
      "[263] loss: 0.046\n",
      "[264] loss: 0.046\n",
      "[265] loss: 0.046\n",
      "[266] loss: 0.045\n",
      "[267] loss: 0.045\n",
      "[268] loss: 0.045\n",
      "[269] loss: 0.045\n",
      "[270] loss: 0.044\n",
      "[271] loss: 0.044\n",
      "[272] loss: 0.044\n",
      "[273] loss: 0.044\n",
      "[274] loss: 0.043\n",
      "[275] loss: 0.043\n",
      "[276] loss: 0.043\n",
      "[277] loss: 0.043\n",
      "[278] loss: 0.043\n",
      "[279] loss: 0.042\n",
      "[280] loss: 0.042\n",
      "[281] loss: 0.042\n",
      "[282] loss: 0.042\n",
      "[283] loss: 0.042\n",
      "[284] loss: 0.041\n",
      "[285] loss: 0.041\n",
      "[286] loss: 0.041\n",
      "[287] loss: 0.041\n",
      "[288] loss: 0.041\n",
      "[289] loss: 0.040\n",
      "[290] loss: 0.040\n",
      "[291] loss: 0.040\n",
      "[292] loss: 0.040\n",
      "[293] loss: 0.040\n",
      "[294] loss: 0.039\n",
      "[295] loss: 0.039\n",
      "[296] loss: 0.039\n",
      "[297] loss: 0.039\n",
      "[298] loss: 0.039\n",
      "[299] loss: 0.038\n",
      "[300] loss: 0.038\n",
      "[301] loss: 0.038\n",
      "[302] loss: 0.038\n",
      "[303] loss: 0.038\n",
      "[304] loss: 0.038\n",
      "[305] loss: 0.037\n",
      "[306] loss: 0.037\n",
      "[307] loss: 0.037\n",
      "[308] loss: 0.037\n",
      "[309] loss: 0.037\n",
      "[310] loss: 0.037\n",
      "[311] loss: 0.036\n",
      "[312] loss: 0.036\n",
      "[313] loss: 0.036\n",
      "[314] loss: 0.036\n",
      "[315] loss: 0.036\n",
      "[316] loss: 0.036\n",
      "[317] loss: 0.036\n",
      "[318] loss: 0.035\n",
      "[319] loss: 0.035\n",
      "[320] loss: 0.035\n",
      "[321] loss: 0.035\n",
      "[322] loss: 0.035\n",
      "[323] loss: 0.035\n",
      "[324] loss: 0.035\n",
      "[325] loss: 0.034\n",
      "[326] loss: 0.034\n",
      "[327] loss: 0.034\n",
      "[328] loss: 0.034\n",
      "[329] loss: 0.034\n",
      "[330] loss: 0.034\n",
      "[331] loss: 0.034\n",
      "[332] loss: 0.033\n",
      "[333] loss: 0.033\n",
      "[334] loss: 0.033\n",
      "[335] loss: 0.033\n",
      "[336] loss: 0.033\n",
      "[337] loss: 0.033\n",
      "[338] loss: 0.033\n",
      "[339] loss: 0.033\n",
      "[340] loss: 0.032\n",
      "[341] loss: 0.032\n",
      "[342] loss: 0.032\n",
      "[343] loss: 0.032\n",
      "[344] loss: 0.032\n",
      "[345] loss: 0.032\n",
      "[346] loss: 0.032\n",
      "[347] loss: 0.032\n",
      "[348] loss: 0.031\n",
      "[349] loss: 0.031\n",
      "[350] loss: 0.031\n",
      "[351] loss: 0.031\n",
      "[352] loss: 0.031\n",
      "[353] loss: 0.031\n",
      "[354] loss: 0.031\n",
      "[355] loss: 0.031\n",
      "[356] loss: 0.031\n",
      "[357] loss: 0.031\n",
      "[358] loss: 0.030\n",
      "[359] loss: 0.030\n",
      "[360] loss: 0.030\n",
      "[361] loss: 0.030\n",
      "[362] loss: 0.030\n",
      "[363] loss: 0.030\n",
      "[364] loss: 0.030\n",
      "[365] loss: 0.030\n",
      "[366] loss: 0.030\n",
      "[367] loss: 0.030\n",
      "[368] loss: 0.029\n",
      "[369] loss: 0.029\n",
      "[370] loss: 0.029\n",
      "[371] loss: 0.029\n",
      "[372] loss: 0.029\n",
      "[373] loss: 0.029\n",
      "[374] loss: 0.029\n",
      "[375] loss: 0.029\n",
      "[376] loss: 0.029\n",
      "[377] loss: 0.029\n",
      "[378] loss: 0.029\n",
      "[379] loss: 0.028\n",
      "[380] loss: 0.028\n",
      "[381] loss: 0.028\n",
      "[382] loss: 0.028\n",
      "[383] loss: 0.028\n",
      "[384] loss: 0.028\n",
      "[385] loss: 0.028\n",
      "[386] loss: 0.028\n",
      "[387] loss: 0.028\n",
      "[388] loss: 0.028\n",
      "[389] loss: 0.028\n",
      "[390] loss: 0.028\n",
      "[391] loss: 0.027\n",
      "[392] loss: 0.027\n",
      "[393] loss: 0.027\n",
      "[394] loss: 0.027\n",
      "[395] loss: 0.027\n",
      "[396] loss: 0.027\n",
      "[397] loss: 0.027\n",
      "[398] loss: 0.027\n",
      "[399] loss: 0.027\n",
      "[400] loss: 0.027\n",
      "[401] loss: 0.027\n",
      "[402] loss: 0.027\n",
      "[403] loss: 0.027\n",
      "[404] loss: 0.027\n",
      "[405] loss: 0.026\n",
      "[406] loss: 0.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[407] loss: 0.026\n",
      "[408] loss: 0.026\n",
      "[409] loss: 0.026\n",
      "[410] loss: 0.026\n",
      "[411] loss: 0.026\n",
      "[412] loss: 0.026\n",
      "[413] loss: 0.026\n",
      "[414] loss: 0.026\n",
      "[415] loss: 0.026\n",
      "[416] loss: 0.026\n",
      "[417] loss: 0.026\n",
      "[418] loss: 0.026\n",
      "[419] loss: 0.026\n",
      "[420] loss: 0.025\n",
      "[421] loss: 0.025\n",
      "[422] loss: 0.025\n",
      "[423] loss: 0.025\n",
      "[424] loss: 0.025\n",
      "[425] loss: 0.025\n",
      "[426] loss: 0.025\n",
      "[427] loss: 0.025\n",
      "[428] loss: 0.025\n",
      "[429] loss: 0.025\n",
      "[430] loss: 0.025\n",
      "[431] loss: 0.025\n",
      "[432] loss: 0.025\n",
      "[433] loss: 0.025\n",
      "[434] loss: 0.025\n",
      "[435] loss: 0.025\n",
      "[436] loss: 0.025\n",
      "[437] loss: 0.025\n",
      "[438] loss: 0.024\n",
      "[439] loss: 0.024\n",
      "[440] loss: 0.024\n",
      "[441] loss: 0.024\n",
      "[442] loss: 0.024\n",
      "[443] loss: 0.024\n",
      "[444] loss: 0.024\n",
      "[445] loss: 0.024\n",
      "[446] loss: 0.024\n",
      "[447] loss: 0.024\n",
      "[448] loss: 0.024\n",
      "[449] loss: 0.024\n",
      "[450] loss: 0.024\n",
      "[451] loss: 0.024\n",
      "[452] loss: 0.024\n",
      "[453] loss: 0.024\n",
      "[454] loss: 0.024\n",
      "[455] loss: 0.024\n",
      "[456] loss: 0.024\n",
      "[457] loss: 0.024\n",
      "[458] loss: 0.024\n",
      "[459] loss: 0.024\n",
      "[460] loss: 0.023\n",
      "[461] loss: 0.023\n",
      "[462] loss: 0.023\n",
      "[463] loss: 0.023\n",
      "[464] loss: 0.023\n",
      "[465] loss: 0.023\n",
      "[466] loss: 0.023\n",
      "[467] loss: 0.023\n",
      "[468] loss: 0.023\n",
      "[469] loss: 0.023\n",
      "[470] loss: 0.023\n",
      "[471] loss: 0.023\n",
      "[472] loss: 0.023\n",
      "[473] loss: 0.023\n",
      "[474] loss: 0.023\n",
      "[475] loss: 0.023\n",
      "[476] loss: 0.023\n",
      "[477] loss: 0.023\n",
      "[478] loss: 0.023\n",
      "[479] loss: 0.023\n",
      "[480] loss: 0.023\n",
      "[481] loss: 0.023\n",
      "[482] loss: 0.023\n",
      "[483] loss: 0.023\n",
      "[484] loss: 0.023\n",
      "[485] loss: 0.023\n",
      "[486] loss: 0.023\n",
      "[487] loss: 0.022\n",
      "[488] loss: 0.022\n",
      "[489] loss: 0.022\n",
      "[490] loss: 0.022\n",
      "[491] loss: 0.022\n",
      "[492] loss: 0.022\n",
      "[493] loss: 0.022\n",
      "[494] loss: 0.022\n",
      "[495] loss: 0.022\n",
      "[496] loss: 0.022\n",
      "[497] loss: 0.022\n",
      "[498] loss: 0.022\n",
      "[499] loss: 0.022\n",
      "[500] loss: 0.022\n",
      "[501] loss: 0.022\n",
      "[502] loss: 0.022\n",
      "[503] loss: 0.022\n",
      "[504] loss: 0.022\n",
      "[505] loss: 0.022\n",
      "[506] loss: 0.022\n",
      "[507] loss: 0.022\n",
      "[508] loss: 0.022\n",
      "[509] loss: 0.022\n",
      "[510] loss: 0.022\n",
      "[511] loss: 0.022\n",
      "[512] loss: 0.022\n",
      "[513] loss: 0.022\n",
      "[514] loss: 0.022\n",
      "[515] loss: 0.022\n",
      "[516] loss: 0.022\n",
      "[517] loss: 0.022\n",
      "[518] loss: 0.022\n",
      "[519] loss: 0.022\n",
      "[520] loss: 0.022\n",
      "[521] loss: 0.021\n",
      "[522] loss: 0.021\n",
      "[523] loss: 0.021\n",
      "[524] loss: 0.021\n",
      "[525] loss: 0.021\n",
      "[526] loss: 0.021\n",
      "[527] loss: 0.021\n",
      "[528] loss: 0.021\n",
      "[529] loss: 0.021\n",
      "[530] loss: 0.021\n",
      "[531] loss: 0.021\n",
      "[532] loss: 0.021\n",
      "[533] loss: 0.021\n",
      "[534] loss: 0.021\n",
      "[535] loss: 0.021\n",
      "[536] loss: 0.021\n",
      "[537] loss: 0.021\n",
      "[538] loss: 0.021\n",
      "[539] loss: 0.021\n",
      "[540] loss: 0.021\n",
      "[541] loss: 0.021\n",
      "[542] loss: 0.021\n",
      "[543] loss: 0.021\n",
      "[544] loss: 0.021\n",
      "[545] loss: 0.021\n",
      "[546] loss: 0.021\n",
      "[547] loss: 0.021\n",
      "[548] loss: 0.021\n",
      "[549] loss: 0.021\n",
      "[550] loss: 0.021\n",
      "[551] loss: 0.021\n",
      "[552] loss: 0.021\n",
      "[553] loss: 0.021\n",
      "[554] loss: 0.021\n",
      "[555] loss: 0.021\n",
      "[556] loss: 0.021\n",
      "[557] loss: 0.021\n",
      "[558] loss: 0.021\n",
      "[559] loss: 0.021\n",
      "[560] loss: 0.021\n",
      "[561] loss: 0.021\n",
      "[562] loss: 0.021\n",
      "[563] loss: 0.021\n",
      "[564] loss: 0.021\n",
      "[565] loss: 0.021\n",
      "[566] loss: 0.021\n",
      "[567] loss: 0.021\n",
      "[568] loss: 0.021\n",
      "[569] loss: 0.020\n",
      "[570] loss: 0.020\n",
      "[571] loss: 0.020\n",
      "[572] loss: 0.020\n",
      "[573] loss: 0.020\n",
      "[574] loss: 0.020\n",
      "[575] loss: 0.020\n",
      "[576] loss: 0.020\n",
      "[577] loss: 0.020\n",
      "[578] loss: 0.020\n",
      "[579] loss: 0.020\n",
      "[580] loss: 0.020\n",
      "[581] loss: 0.020\n",
      "[582] loss: 0.020\n",
      "[583] loss: 0.020\n",
      "[584] loss: 0.020\n",
      "[585] loss: 0.020\n",
      "[586] loss: 0.020\n",
      "[587] loss: 0.020\n",
      "[588] loss: 0.020\n",
      "[589] loss: 0.020\n",
      "[590] loss: 0.020\n",
      "[591] loss: 0.020\n",
      "[592] loss: 0.020\n",
      "[593] loss: 0.020\n",
      "[594] loss: 0.020\n",
      "[595] loss: 0.020\n",
      "[596] loss: 0.020\n",
      "[597] loss: 0.020\n",
      "[598] loss: 0.020\n",
      "[599] loss: 0.020\n",
      "[600] loss: 0.020\n",
      "[601] loss: 0.020\n",
      "[602] loss: 0.020\n",
      "[603] loss: 0.020\n",
      "[604] loss: 0.020\n",
      "[605] loss: 0.020\n",
      "[606] loss: 0.020\n",
      "[607] loss: 0.020\n",
      "[608] loss: 0.020\n",
      "[609] loss: 0.020\n",
      "[610] loss: 0.020\n",
      "[611] loss: 0.020\n",
      "[612] loss: 0.020\n",
      "[613] loss: 0.020\n",
      "[614] loss: 0.020\n",
      "[615] loss: 0.020\n",
      "[616] loss: 0.020\n",
      "[617] loss: 0.020\n",
      "[618] loss: 0.020\n",
      "[619] loss: 0.020\n",
      "[620] loss: 0.020\n",
      "[621] loss: 0.020\n",
      "[622] loss: 0.020\n",
      "[623] loss: 0.020\n",
      "[624] loss: 0.020\n",
      "[625] loss: 0.020\n",
      "[626] loss: 0.020\n",
      "[627] loss: 0.020\n",
      "[628] loss: 0.020\n",
      "[629] loss: 0.020\n",
      "[630] loss: 0.020\n",
      "[631] loss: 0.020\n",
      "[632] loss: 0.020\n",
      "[633] loss: 0.020\n",
      "[634] loss: 0.020\n",
      "[635] loss: 0.020\n",
      "[636] loss: 0.020\n",
      "[637] loss: 0.020\n",
      "[638] loss: 0.020\n",
      "[639] loss: 0.020\n",
      "[640] loss: 0.020\n",
      "[641] loss: 0.020\n",
      "[642] loss: 0.020\n",
      "[643] loss: 0.019\n",
      "[644] loss: 0.019\n",
      "[645] loss: 0.019\n",
      "[646] loss: 0.019\n",
      "[647] loss: 0.019\n",
      "[648] loss: 0.019\n",
      "[649] loss: 0.019\n",
      "[650] loss: 0.019\n",
      "[651] loss: 0.019\n",
      "[652] loss: 0.019\n",
      "[653] loss: 0.019\n",
      "[654] loss: 0.019\n",
      "[655] loss: 0.019\n",
      "[656] loss: 0.019\n",
      "[657] loss: 0.019\n",
      "[658] loss: 0.019\n",
      "[659] loss: 0.019\n",
      "[660] loss: 0.019\n",
      "[661] loss: 0.019\n",
      "[662] loss: 0.019\n",
      "[663] loss: 0.019\n",
      "[664] loss: 0.019\n",
      "[665] loss: 0.019\n",
      "[666] loss: 0.019\n",
      "[667] loss: 0.019\n",
      "[668] loss: 0.019\n",
      "[669] loss: 0.019\n",
      "[670] loss: 0.019\n",
      "[671] loss: 0.019\n",
      "[672] loss: 0.019\n",
      "[673] loss: 0.019\n",
      "[674] loss: 0.019\n",
      "[675] loss: 0.019\n",
      "[676] loss: 0.019\n",
      "[677] loss: 0.019\n",
      "[678] loss: 0.019\n",
      "[679] loss: 0.019\n",
      "[680] loss: 0.019\n",
      "[681] loss: 0.019\n",
      "[682] loss: 0.019\n",
      "[683] loss: 0.019\n",
      "[684] loss: 0.019\n",
      "[685] loss: 0.019\n",
      "[686] loss: 0.019\n",
      "[687] loss: 0.019\n",
      "[688] loss: 0.019\n",
      "[689] loss: 0.019\n",
      "[690] loss: 0.019\n",
      "[691] loss: 0.019\n",
      "[692] loss: 0.019\n",
      "[693] loss: 0.019\n",
      "[694] loss: 0.019\n",
      "[695] loss: 0.019\n",
      "[696] loss: 0.019\n",
      "[697] loss: 0.019\n",
      "[698] loss: 0.019\n",
      "[699] loss: 0.019\n",
      "[700] loss: 0.019\n",
      "[701] loss: 0.019\n",
      "[702] loss: 0.019\n",
      "[703] loss: 0.019\n",
      "[704] loss: 0.019\n",
      "[705] loss: 0.019\n",
      "[706] loss: 0.019\n",
      "[707] loss: 0.019\n",
      "[708] loss: 0.019\n",
      "[709] loss: 0.019\n",
      "[710] loss: 0.019\n",
      "[711] loss: 0.019\n",
      "[712] loss: 0.019\n",
      "[713] loss: 0.019\n",
      "[714] loss: 0.019\n",
      "[715] loss: 0.019\n",
      "[716] loss: 0.019\n",
      "[717] loss: 0.019\n",
      "[718] loss: 0.019\n",
      "[719] loss: 0.019\n",
      "[720] loss: 0.019\n",
      "[721] loss: 0.019\n",
      "[722] loss: 0.019\n",
      "[723] loss: 0.019\n",
      "[724] loss: 0.019\n",
      "[725] loss: 0.019\n",
      "[726] loss: 0.019\n",
      "[727] loss: 0.019\n",
      "[728] loss: 0.019\n",
      "[729] loss: 0.019\n",
      "[730] loss: 0.019\n",
      "[731] loss: 0.019\n",
      "[732] loss: 0.019\n",
      "[733] loss: 0.019\n",
      "[734] loss: 0.019\n",
      "[735] loss: 0.019\n",
      "[736] loss: 0.019\n",
      "[737] loss: 0.019\n",
      "[738] loss: 0.019\n",
      "[739] loss: 0.019\n",
      "[740] loss: 0.019\n",
      "[741] loss: 0.019\n",
      "[742] loss: 0.019\n",
      "[743] loss: 0.019\n",
      "[744] loss: 0.019\n",
      "[745] loss: 0.019\n",
      "[746] loss: 0.019\n",
      "[747] loss: 0.019\n",
      "[748] loss: 0.019\n",
      "[749] loss: 0.019\n",
      "[750] loss: 0.019\n",
      "[751] loss: 0.019\n",
      "[752] loss: 0.019\n",
      "[753] loss: 0.019\n",
      "[754] loss: 0.019\n",
      "[755] loss: 0.019\n",
      "[756] loss: 0.019\n",
      "[757] loss: 0.019\n",
      "[758] loss: 0.019\n",
      "[759] loss: 0.019\n",
      "[760] loss: 0.019\n",
      "[761] loss: 0.019\n",
      "[762] loss: 0.019\n",
      "[763] loss: 0.019\n",
      "[764] loss: 0.019\n",
      "[765] loss: 0.019\n",
      "[766] loss: 0.019\n",
      "[767] loss: 0.019\n",
      "[768] loss: 0.019\n",
      "[769] loss: 0.019\n",
      "[770] loss: 0.019\n",
      "[771] loss: 0.019\n",
      "[772] loss: 0.019\n",
      "[773] loss: 0.018\n",
      "[774] loss: 0.018\n",
      "[775] loss: 0.018\n",
      "[776] loss: 0.018\n",
      "[777] loss: 0.018\n",
      "[778] loss: 0.018\n",
      "[779] loss: 0.018\n",
      "[780] loss: 0.018\n",
      "[781] loss: 0.018\n",
      "[782] loss: 0.018\n",
      "[783] loss: 0.018\n",
      "[784] loss: 0.018\n",
      "[785] loss: 0.018\n",
      "[786] loss: 0.018\n",
      "[787] loss: 0.018\n",
      "[788] loss: 0.018\n",
      "[789] loss: 0.018\n",
      "[790] loss: 0.018\n",
      "[791] loss: 0.018\n",
      "[792] loss: 0.018\n",
      "[793] loss: 0.018\n",
      "[794] loss: 0.018\n",
      "[795] loss: 0.018\n",
      "[796] loss: 0.018\n",
      "[797] loss: 0.018\n",
      "[798] loss: 0.018\n",
      "[799] loss: 0.018\n",
      "[800] loss: 0.018\n",
      "[801] loss: 0.018\n",
      "[802] loss: 0.018\n",
      "[803] loss: 0.018\n",
      "[804] loss: 0.018\n",
      "[805] loss: 0.018\n",
      "[806] loss: 0.018\n",
      "[807] loss: 0.018\n",
      "[808] loss: 0.018\n",
      "[809] loss: 0.018\n",
      "[810] loss: 0.018\n",
      "[811] loss: 0.018\n",
      "[812] loss: 0.018\n",
      "[813] loss: 0.018\n",
      "[814] loss: 0.018\n",
      "[815] loss: 0.018\n",
      "[816] loss: 0.018\n",
      "[817] loss: 0.018\n",
      "[818] loss: 0.018\n",
      "[819] loss: 0.018\n",
      "[820] loss: 0.018\n",
      "[821] loss: 0.018\n",
      "[822] loss: 0.018\n",
      "[823] loss: 0.018\n",
      "[824] loss: 0.018\n",
      "[825] loss: 0.018\n",
      "[826] loss: 0.018\n",
      "[827] loss: 0.018\n",
      "[828] loss: 0.018\n",
      "[829] loss: 0.018\n",
      "[830] loss: 0.018\n",
      "[831] loss: 0.018\n",
      "[832] loss: 0.018\n",
      "[833] loss: 0.018\n",
      "[834] loss: 0.018\n",
      "[835] loss: 0.018\n",
      "[836] loss: 0.018\n",
      "[837] loss: 0.018\n",
      "[838] loss: 0.018\n",
      "[839] loss: 0.018\n",
      "[840] loss: 0.018\n",
      "[841] loss: 0.018\n",
      "[842] loss: 0.018\n",
      "[843] loss: 0.018\n",
      "[844] loss: 0.018\n",
      "[845] loss: 0.018\n",
      "[846] loss: 0.018\n",
      "[847] loss: 0.018\n",
      "[848] loss: 0.018\n",
      "[849] loss: 0.018\n",
      "[850] loss: 0.018\n",
      "[851] loss: 0.018\n",
      "[852] loss: 0.018\n",
      "[853] loss: 0.018\n",
      "[854] loss: 0.018\n",
      "[855] loss: 0.018\n",
      "[856] loss: 0.018\n",
      "[857] loss: 0.018\n",
      "[858] loss: 0.018\n",
      "[859] loss: 0.018\n",
      "[860] loss: 0.018\n",
      "[861] loss: 0.018\n",
      "[862] loss: 0.018\n",
      "[863] loss: 0.018\n",
      "[864] loss: 0.018\n",
      "[865] loss: 0.018\n",
      "[866] loss: 0.018\n",
      "[867] loss: 0.018\n",
      "[868] loss: 0.018\n",
      "[869] loss: 0.018\n",
      "[870] loss: 0.018\n",
      "[871] loss: 0.018\n",
      "[872] loss: 0.018\n",
      "[873] loss: 0.018\n",
      "[874] loss: 0.018\n",
      "[875] loss: 0.018\n",
      "[876] loss: 0.018\n",
      "[877] loss: 0.018\n",
      "[878] loss: 0.018\n",
      "[879] loss: 0.018\n",
      "[880] loss: 0.018\n",
      "[881] loss: 0.018\n",
      "[882] loss: 0.018\n",
      "[883] loss: 0.018\n",
      "[884] loss: 0.018\n",
      "[885] loss: 0.018\n",
      "[886] loss: 0.018\n",
      "[887] loss: 0.018\n",
      "[888] loss: 0.018\n",
      "[889] loss: 0.018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[890] loss: 0.018\n",
      "[891] loss: 0.018\n",
      "[892] loss: 0.018\n",
      "[893] loss: 0.018\n",
      "[894] loss: 0.018\n",
      "[895] loss: 0.018\n",
      "[896] loss: 0.018\n",
      "[897] loss: 0.018\n",
      "[898] loss: 0.018\n",
      "[899] loss: 0.018\n",
      "[900] loss: 0.018\n",
      "[901] loss: 0.018\n",
      "[902] loss: 0.018\n",
      "[903] loss: 0.018\n",
      "[904] loss: 0.018\n",
      "[905] loss: 0.018\n",
      "[906] loss: 0.018\n",
      "[907] loss: 0.018\n",
      "[908] loss: 0.018\n",
      "[909] loss: 0.018\n",
      "[910] loss: 0.018\n",
      "[911] loss: 0.018\n",
      "[912] loss: 0.018\n",
      "[913] loss: 0.018\n",
      "[914] loss: 0.018\n",
      "[915] loss: 0.018\n",
      "[916] loss: 0.018\n",
      "[917] loss: 0.018\n",
      "[918] loss: 0.018\n",
      "[919] loss: 0.018\n",
      "[920] loss: 0.018\n",
      "[921] loss: 0.018\n",
      "[922] loss: 0.018\n",
      "[923] loss: 0.018\n",
      "[924] loss: 0.018\n",
      "[925] loss: 0.018\n",
      "[926] loss: 0.018\n",
      "[927] loss: 0.018\n",
      "[928] loss: 0.018\n",
      "[929] loss: 0.018\n",
      "[930] loss: 0.018\n",
      "[931] loss: 0.018\n",
      "[932] loss: 0.018\n",
      "[933] loss: 0.018\n",
      "[934] loss: 0.018\n",
      "[935] loss: 0.018\n",
      "[936] loss: 0.018\n",
      "[937] loss: 0.018\n",
      "[938] loss: 0.018\n",
      "[939] loss: 0.018\n",
      "[940] loss: 0.018\n",
      "[941] loss: 0.018\n",
      "[942] loss: 0.018\n",
      "[943] loss: 0.018\n",
      "[944] loss: 0.018\n",
      "[945] loss: 0.018\n",
      "[946] loss: 0.018\n",
      "[947] loss: 0.018\n",
      "[948] loss: 0.018\n",
      "[949] loss: 0.018\n",
      "[950] loss: 0.018\n",
      "[951] loss: 0.018\n",
      "[952] loss: 0.018\n",
      "[953] loss: 0.018\n",
      "[954] loss: 0.018\n",
      "[955] loss: 0.018\n",
      "[956] loss: 0.018\n",
      "[957] loss: 0.018\n",
      "[958] loss: 0.018\n",
      "[959] loss: 0.018\n",
      "[960] loss: 0.018\n",
      "[961] loss: 0.018\n",
      "[962] loss: 0.018\n",
      "[963] loss: 0.018\n",
      "[964] loss: 0.018\n",
      "[965] loss: 0.018\n",
      "[966] loss: 0.018\n",
      "[967] loss: 0.018\n",
      "[968] loss: 0.018\n",
      "[969] loss: 0.018\n",
      "[970] loss: 0.018\n",
      "[971] loss: 0.018\n",
      "[972] loss: 0.018\n",
      "[973] loss: 0.018\n",
      "[974] loss: 0.018\n",
      "[975] loss: 0.018\n",
      "[976] loss: 0.018\n",
      "[977] loss: 0.018\n",
      "[978] loss: 0.018\n",
      "[979] loss: 0.018\n",
      "[980] loss: 0.018\n",
      "[981] loss: 0.018\n",
      "[982] loss: 0.018\n",
      "[983] loss: 0.018\n",
      "[984] loss: 0.018\n",
      "[985] loss: 0.018\n",
      "[986] loss: 0.018\n",
      "[987] loss: 0.018\n",
      "[988] loss: 0.018\n",
      "[989] loss: 0.018\n",
      "[990] loss: 0.018\n",
      "[991] loss: 0.018\n",
      "[992] loss: 0.018\n",
      "[993] loss: 0.018\n",
      "[994] loss: 0.018\n",
      "[995] loss: 0.018\n",
      "[996] loss: 0.018\n",
      "[997] loss: 0.018\n",
      "[998] loss: 0.018\n",
      "[999] loss: 0.018\n",
      "[1000] loss: 0.018\n",
      "[1001] loss: 0.018\n",
      "[1002] loss: 0.018\n",
      "[1003] loss: 0.018\n",
      "[1004] loss: 0.017\n",
      "[1005] loss: 0.017\n",
      "[1006] loss: 0.017\n",
      "[1007] loss: 0.017\n",
      "[1008] loss: 0.017\n",
      "[1009] loss: 0.017\n",
      "[1010] loss: 0.017\n",
      "[1011] loss: 0.017\n",
      "[1012] loss: 0.017\n",
      "[1013] loss: 0.017\n",
      "[1014] loss: 0.017\n",
      "[1015] loss: 0.017\n",
      "[1016] loss: 0.017\n",
      "[1017] loss: 0.017\n",
      "[1018] loss: 0.017\n",
      "[1019] loss: 0.017\n",
      "[1020] loss: 0.017\n",
      "[1021] loss: 0.017\n",
      "[1022] loss: 0.017\n",
      "[1023] loss: 0.017\n",
      "[1024] loss: 0.017\n",
      "[1025] loss: 0.017\n",
      "[1026] loss: 0.017\n",
      "[1027] loss: 0.017\n",
      "[1028] loss: 0.017\n",
      "[1029] loss: 0.017\n",
      "[1030] loss: 0.017\n",
      "[1031] loss: 0.017\n",
      "[1032] loss: 0.017\n",
      "[1033] loss: 0.017\n",
      "[1034] loss: 0.017\n",
      "[1035] loss: 0.017\n",
      "[1036] loss: 0.017\n",
      "[1037] loss: 0.017\n",
      "[1038] loss: 0.017\n",
      "[1039] loss: 0.017\n",
      "[1040] loss: 0.017\n",
      "[1041] loss: 0.017\n",
      "[1042] loss: 0.017\n",
      "[1043] loss: 0.017\n",
      "[1044] loss: 0.017\n",
      "[1045] loss: 0.017\n",
      "[1046] loss: 0.017\n",
      "[1047] loss: 0.017\n",
      "[1048] loss: 0.017\n",
      "[1049] loss: 0.017\n",
      "[1050] loss: 0.017\n",
      "[1051] loss: 0.017\n",
      "[1052] loss: 0.017\n",
      "[1053] loss: 0.017\n",
      "[1054] loss: 0.017\n",
      "[1055] loss: 0.017\n",
      "[1056] loss: 0.017\n",
      "[1057] loss: 0.017\n",
      "[1058] loss: 0.017\n",
      "[1059] loss: 0.017\n",
      "[1060] loss: 0.017\n",
      "[1061] loss: 0.017\n",
      "[1062] loss: 0.017\n",
      "[1063] loss: 0.017\n",
      "[1064] loss: 0.017\n",
      "[1065] loss: 0.017\n",
      "[1066] loss: 0.017\n",
      "[1067] loss: 0.017\n",
      "[1068] loss: 0.017\n",
      "[1069] loss: 0.017\n",
      "[1070] loss: 0.017\n",
      "[1071] loss: 0.017\n",
      "[1072] loss: 0.017\n",
      "[1073] loss: 0.017\n",
      "[1074] loss: 0.017\n",
      "[1075] loss: 0.017\n",
      "[1076] loss: 0.017\n",
      "[1077] loss: 0.017\n",
      "[1078] loss: 0.017\n",
      "[1079] loss: 0.017\n",
      "[1080] loss: 0.017\n",
      "[1081] loss: 0.017\n",
      "[1082] loss: 0.017\n",
      "[1083] loss: 0.017\n",
      "[1084] loss: 0.017\n",
      "[1085] loss: 0.017\n",
      "[1086] loss: 0.017\n",
      "[1087] loss: 0.017\n",
      "[1088] loss: 0.017\n",
      "[1089] loss: 0.017\n",
      "[1090] loss: 0.017\n",
      "[1091] loss: 0.017\n",
      "[1092] loss: 0.017\n",
      "[1093] loss: 0.017\n",
      "[1094] loss: 0.017\n",
      "[1095] loss: 0.017\n",
      "[1096] loss: 0.017\n",
      "[1097] loss: 0.017\n",
      "[1098] loss: 0.017\n",
      "[1099] loss: 0.017\n",
      "[1100] loss: 0.017\n",
      "[1101] loss: 0.017\n",
      "[1102] loss: 0.017\n",
      "[1103] loss: 0.017\n",
      "[1104] loss: 0.017\n",
      "[1105] loss: 0.017\n",
      "[1106] loss: 0.017\n",
      "[1107] loss: 0.017\n",
      "[1108] loss: 0.017\n",
      "[1109] loss: 0.017\n",
      "[1110] loss: 0.017\n",
      "[1111] loss: 0.017\n",
      "[1112] loss: 0.017\n",
      "[1113] loss: 0.017\n",
      "[1114] loss: 0.017\n",
      "[1115] loss: 0.017\n",
      "[1116] loss: 0.017\n",
      "[1117] loss: 0.017\n",
      "[1118] loss: 0.017\n",
      "[1119] loss: 0.017\n",
      "[1120] loss: 0.017\n",
      "[1121] loss: 0.017\n",
      "[1122] loss: 0.017\n",
      "[1123] loss: 0.017\n",
      "[1124] loss: 0.017\n",
      "[1125] loss: 0.017\n",
      "[1126] loss: 0.017\n",
      "[1127] loss: 0.017\n",
      "[1128] loss: 0.017\n",
      "[1129] loss: 0.017\n",
      "[1130] loss: 0.017\n",
      "[1131] loss: 0.017\n",
      "[1132] loss: 0.017\n",
      "[1133] loss: 0.017\n",
      "[1134] loss: 0.017\n",
      "[1135] loss: 0.017\n",
      "[1136] loss: 0.017\n",
      "[1137] loss: 0.017\n",
      "[1138] loss: 0.017\n",
      "[1139] loss: 0.017\n",
      "[1140] loss: 0.017\n",
      "[1141] loss: 0.017\n",
      "[1142] loss: 0.017\n",
      "[1143] loss: 0.017\n",
      "[1144] loss: 0.017\n",
      "[1145] loss: 0.017\n",
      "[1146] loss: 0.017\n",
      "[1147] loss: 0.017\n",
      "[1148] loss: 0.017\n",
      "[1149] loss: 0.017\n",
      "[1150] loss: 0.017\n",
      "[1151] loss: 0.017\n",
      "[1152] loss: 0.017\n",
      "[1153] loss: 0.017\n",
      "[1154] loss: 0.017\n",
      "[1155] loss: 0.017\n",
      "[1156] loss: 0.017\n",
      "[1157] loss: 0.017\n",
      "[1158] loss: 0.017\n",
      "[1159] loss: 0.017\n",
      "[1160] loss: 0.017\n",
      "[1161] loss: 0.017\n",
      "[1162] loss: 0.017\n",
      "[1163] loss: 0.017\n",
      "[1164] loss: 0.017\n",
      "[1165] loss: 0.017\n",
      "[1166] loss: 0.017\n",
      "[1167] loss: 0.017\n",
      "[1168] loss: 0.017\n",
      "[1169] loss: 0.017\n",
      "[1170] loss: 0.017\n",
      "[1171] loss: 0.017\n",
      "[1172] loss: 0.017\n",
      "[1173] loss: 0.017\n",
      "[1174] loss: 0.017\n",
      "[1175] loss: 0.017\n",
      "[1176] loss: 0.017\n",
      "[1177] loss: 0.017\n",
      "[1178] loss: 0.017\n",
      "[1179] loss: 0.017\n",
      "[1180] loss: 0.017\n",
      "[1181] loss: 0.017\n",
      "[1182] loss: 0.017\n",
      "[1183] loss: 0.017\n",
      "[1184] loss: 0.017\n",
      "[1185] loss: 0.017\n",
      "[1186] loss: 0.017\n",
      "[1187] loss: 0.017\n",
      "[1188] loss: 0.017\n",
      "[1189] loss: 0.017\n",
      "[1190] loss: 0.017\n",
      "[1191] loss: 0.017\n",
      "[1192] loss: 0.017\n",
      "[1193] loss: 0.017\n",
      "[1194] loss: 0.017\n",
      "[1195] loss: 0.017\n",
      "[1196] loss: 0.017\n",
      "[1197] loss: 0.017\n",
      "[1198] loss: 0.017\n",
      "[1199] loss: 0.017\n",
      "[1200] loss: 0.017\n",
      "[1201] loss: 0.017\n",
      "[1202] loss: 0.017\n",
      "[1203] loss: 0.017\n",
      "[1204] loss: 0.017\n",
      "[1205] loss: 0.017\n",
      "[1206] loss: 0.017\n",
      "[1207] loss: 0.017\n",
      "[1208] loss: 0.017\n",
      "[1209] loss: 0.017\n",
      "[1210] loss: 0.017\n",
      "[1211] loss: 0.017\n",
      "[1212] loss: 0.017\n",
      "[1213] loss: 0.017\n",
      "[1214] loss: 0.017\n",
      "[1215] loss: 0.017\n",
      "[1216] loss: 0.017\n",
      "[1217] loss: 0.017\n",
      "[1218] loss: 0.017\n",
      "[1219] loss: 0.017\n",
      "[1220] loss: 0.017\n",
      "[1221] loss: 0.017\n",
      "[1222] loss: 0.017\n",
      "[1223] loss: 0.017\n",
      "[1224] loss: 0.017\n",
      "[1225] loss: 0.017\n",
      "[1226] loss: 0.017\n",
      "[1227] loss: 0.017\n",
      "[1228] loss: 0.017\n",
      "[1229] loss: 0.017\n",
      "[1230] loss: 0.017\n",
      "[1231] loss: 0.017\n",
      "[1232] loss: 0.017\n",
      "[1233] loss: 0.017\n",
      "[1234] loss: 0.017\n",
      "[1235] loss: 0.017\n",
      "[1236] loss: 0.017\n",
      "[1237] loss: 0.017\n",
      "[1238] loss: 0.017\n",
      "[1239] loss: 0.017\n",
      "[1240] loss: 0.017\n",
      "[1241] loss: 0.017\n",
      "[1242] loss: 0.017\n",
      "[1243] loss: 0.017\n",
      "[1244] loss: 0.017\n",
      "[1245] loss: 0.017\n",
      "[1246] loss: 0.017\n",
      "[1247] loss: 0.017\n",
      "[1248] loss: 0.017\n",
      "[1249] loss: 0.017\n",
      "[1250] loss: 0.017\n",
      "[1251] loss: 0.017\n",
      "[1252] loss: 0.017\n",
      "[1253] loss: 0.017\n",
      "[1254] loss: 0.017\n",
      "[1255] loss: 0.017\n",
      "[1256] loss: 0.017\n",
      "[1257] loss: 0.017\n",
      "[1258] loss: 0.017\n",
      "[1259] loss: 0.017\n",
      "[1260] loss: 0.017\n",
      "[1261] loss: 0.017\n",
      "[1262] loss: 0.017\n",
      "[1263] loss: 0.017\n",
      "[1264] loss: 0.017\n",
      "[1265] loss: 0.017\n",
      "[1266] loss: 0.017\n",
      "[1267] loss: 0.017\n",
      "[1268] loss: 0.017\n",
      "[1269] loss: 0.017\n",
      "[1270] loss: 0.017\n",
      "[1271] loss: 0.017\n",
      "[1272] loss: 0.017\n",
      "[1273] loss: 0.017\n",
      "[1274] loss: 0.017\n",
      "[1275] loss: 0.017\n",
      "[1276] loss: 0.017\n",
      "[1277] loss: 0.017\n",
      "[1278] loss: 0.017\n",
      "[1279] loss: 0.017\n",
      "[1280] loss: 0.017\n",
      "[1281] loss: 0.017\n",
      "[1282] loss: 0.017\n",
      "[1283] loss: 0.017\n",
      "[1284] loss: 0.017\n",
      "[1285] loss: 0.017\n",
      "[1286] loss: 0.017\n",
      "[1287] loss: 0.017\n",
      "[1288] loss: 0.017\n",
      "[1289] loss: 0.017\n",
      "[1290] loss: 0.017\n",
      "[1291] loss: 0.017\n",
      "[1292] loss: 0.017\n",
      "[1293] loss: 0.017\n",
      "[1294] loss: 0.017\n",
      "[1295] loss: 0.017\n",
      "[1296] loss: 0.017\n",
      "[1297] loss: 0.017\n",
      "[1298] loss: 0.017\n",
      "[1299] loss: 0.017\n",
      "[1300] loss: 0.017\n",
      "[1301] loss: 0.017\n",
      "[1302] loss: 0.017\n",
      "[1303] loss: 0.017\n",
      "[1304] loss: 0.017\n",
      "[1305] loss: 0.017\n",
      "[1306] loss: 0.017\n",
      "[1307] loss: 0.017\n",
      "[1308] loss: 0.017\n",
      "[1309] loss: 0.017\n",
      "[1310] loss: 0.017\n",
      "[1311] loss: 0.017\n",
      "[1312] loss: 0.017\n",
      "[1313] loss: 0.017\n",
      "[1314] loss: 0.017\n",
      "[1315] loss: 0.017\n",
      "[1316] loss: 0.017\n",
      "[1317] loss: 0.017\n",
      "[1318] loss: 0.017\n",
      "[1319] loss: 0.017\n",
      "[1320] loss: 0.017\n",
      "[1321] loss: 0.017\n",
      "[1322] loss: 0.017\n",
      "[1323] loss: 0.017\n",
      "[1324] loss: 0.017\n",
      "[1325] loss: 0.016\n",
      "[1326] loss: 0.016\n",
      "[1327] loss: 0.016\n",
      "[1328] loss: 0.016\n",
      "[1329] loss: 0.016\n",
      "[1330] loss: 0.016\n",
      "[1331] loss: 0.016\n",
      "[1332] loss: 0.016\n",
      "[1333] loss: 0.016\n",
      "[1334] loss: 0.016\n",
      "[1335] loss: 0.016\n",
      "[1336] loss: 0.016\n",
      "[1337] loss: 0.016\n",
      "[1338] loss: 0.016\n",
      "[1339] loss: 0.016\n",
      "[1340] loss: 0.016\n",
      "[1341] loss: 0.016\n",
      "[1342] loss: 0.016\n",
      "[1343] loss: 0.016\n",
      "[1344] loss: 0.016\n",
      "[1345] loss: 0.016\n",
      "[1346] loss: 0.016\n",
      "[1347] loss: 0.016\n",
      "[1348] loss: 0.016\n",
      "[1349] loss: 0.016\n",
      "[1350] loss: 0.016\n",
      "[1351] loss: 0.016\n",
      "[1352] loss: 0.016\n",
      "[1353] loss: 0.016\n",
      "[1354] loss: 0.016\n",
      "[1355] loss: 0.016\n",
      "[1356] loss: 0.016\n",
      "[1357] loss: 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1358] loss: 0.016\n",
      "[1359] loss: 0.016\n",
      "[1360] loss: 0.016\n",
      "[1361] loss: 0.016\n",
      "[1362] loss: 0.016\n",
      "[1363] loss: 0.016\n",
      "[1364] loss: 0.016\n",
      "[1365] loss: 0.016\n",
      "[1366] loss: 0.016\n",
      "[1367] loss: 0.016\n",
      "[1368] loss: 0.016\n",
      "[1369] loss: 0.016\n",
      "[1370] loss: 0.016\n",
      "[1371] loss: 0.016\n",
      "[1372] loss: 0.016\n",
      "[1373] loss: 0.016\n",
      "[1374] loss: 0.016\n",
      "[1375] loss: 0.016\n",
      "[1376] loss: 0.016\n",
      "[1377] loss: 0.016\n",
      "[1378] loss: 0.016\n",
      "[1379] loss: 0.016\n",
      "[1380] loss: 0.016\n",
      "[1381] loss: 0.016\n",
      "[1382] loss: 0.016\n",
      "[1383] loss: 0.016\n",
      "[1384] loss: 0.016\n",
      "[1385] loss: 0.016\n",
      "[1386] loss: 0.016\n",
      "[1387] loss: 0.016\n",
      "[1388] loss: 0.016\n",
      "[1389] loss: 0.016\n",
      "[1390] loss: 0.016\n",
      "[1391] loss: 0.016\n",
      "[1392] loss: 0.016\n",
      "[1393] loss: 0.016\n",
      "[1394] loss: 0.016\n",
      "[1395] loss: 0.016\n",
      "[1396] loss: 0.016\n",
      "[1397] loss: 0.016\n",
      "[1398] loss: 0.016\n",
      "[1399] loss: 0.016\n",
      "[1400] loss: 0.016\n",
      "[1401] loss: 0.016\n",
      "[1402] loss: 0.016\n",
      "[1403] loss: 0.016\n",
      "[1404] loss: 0.016\n",
      "[1405] loss: 0.016\n",
      "[1406] loss: 0.016\n",
      "[1407] loss: 0.016\n",
      "[1408] loss: 0.016\n",
      "[1409] loss: 0.016\n",
      "[1410] loss: 0.016\n",
      "[1411] loss: 0.016\n",
      "[1412] loss: 0.016\n",
      "[1413] loss: 0.016\n",
      "[1414] loss: 0.016\n",
      "[1415] loss: 0.016\n",
      "[1416] loss: 0.016\n",
      "[1417] loss: 0.016\n",
      "[1418] loss: 0.016\n",
      "[1419] loss: 0.016\n",
      "[1420] loss: 0.016\n",
      "[1421] loss: 0.016\n",
      "[1422] loss: 0.016\n",
      "[1423] loss: 0.016\n",
      "[1424] loss: 0.016\n",
      "[1425] loss: 0.016\n",
      "[1426] loss: 0.016\n",
      "[1427] loss: 0.016\n",
      "[1428] loss: 0.016\n",
      "[1429] loss: 0.016\n",
      "[1430] loss: 0.016\n",
      "[1431] loss: 0.016\n",
      "[1432] loss: 0.016\n",
      "[1433] loss: 0.016\n",
      "[1434] loss: 0.016\n",
      "[1435] loss: 0.016\n",
      "[1436] loss: 0.016\n",
      "[1437] loss: 0.016\n",
      "[1438] loss: 0.016\n",
      "[1439] loss: 0.016\n",
      "[1440] loss: 0.016\n",
      "[1441] loss: 0.016\n",
      "[1442] loss: 0.016\n",
      "[1443] loss: 0.016\n",
      "[1444] loss: 0.016\n",
      "[1445] loss: 0.016\n",
      "[1446] loss: 0.016\n",
      "[1447] loss: 0.016\n",
      "[1448] loss: 0.016\n",
      "[1449] loss: 0.016\n",
      "[1450] loss: 0.016\n",
      "[1451] loss: 0.016\n",
      "[1452] loss: 0.016\n",
      "[1453] loss: 0.016\n",
      "[1454] loss: 0.016\n",
      "[1455] loss: 0.016\n",
      "[1456] loss: 0.016\n",
      "[1457] loss: 0.016\n",
      "[1458] loss: 0.016\n",
      "[1459] loss: 0.016\n",
      "[1460] loss: 0.016\n",
      "[1461] loss: 0.016\n",
      "[1462] loss: 0.016\n",
      "[1463] loss: 0.016\n",
      "[1464] loss: 0.016\n",
      "[1465] loss: 0.016\n",
      "[1466] loss: 0.016\n",
      "[1467] loss: 0.016\n",
      "[1468] loss: 0.016\n",
      "[1469] loss: 0.016\n",
      "[1470] loss: 0.016\n",
      "[1471] loss: 0.016\n",
      "[1472] loss: 0.016\n",
      "[1473] loss: 0.016\n",
      "[1474] loss: 0.016\n",
      "[1475] loss: 0.016\n",
      "[1476] loss: 0.016\n",
      "[1477] loss: 0.016\n",
      "[1478] loss: 0.016\n",
      "[1479] loss: 0.016\n",
      "[1480] loss: 0.016\n",
      "[1481] loss: 0.016\n",
      "[1482] loss: 0.016\n",
      "[1483] loss: 0.016\n",
      "[1484] loss: 0.016\n",
      "[1485] loss: 0.016\n",
      "[1486] loss: 0.016\n",
      "[1487] loss: 0.016\n",
      "[1488] loss: 0.016\n",
      "[1489] loss: 0.016\n",
      "[1490] loss: 0.016\n",
      "[1491] loss: 0.016\n",
      "[1492] loss: 0.016\n",
      "[1493] loss: 0.016\n",
      "[1494] loss: 0.016\n",
      "[1495] loss: 0.016\n",
      "[1496] loss: 0.016\n",
      "[1497] loss: 0.016\n",
      "[1498] loss: 0.016\n",
      "[1499] loss: 0.016\n",
      "[1500] loss: 0.016\n",
      "[1501] loss: 0.016\n",
      "[1502] loss: 0.016\n",
      "[1503] loss: 0.016\n",
      "[1504] loss: 0.016\n",
      "[1505] loss: 0.016\n",
      "[1506] loss: 0.016\n",
      "[1507] loss: 0.016\n",
      "[1508] loss: 0.016\n",
      "[1509] loss: 0.016\n",
      "[1510] loss: 0.016\n",
      "[1511] loss: 0.016\n",
      "[1512] loss: 0.016\n",
      "[1513] loss: 0.016\n",
      "[1514] loss: 0.016\n",
      "[1515] loss: 0.016\n",
      "[1516] loss: 0.016\n",
      "[1517] loss: 0.016\n",
      "[1518] loss: 0.016\n",
      "[1519] loss: 0.016\n",
      "[1520] loss: 0.016\n",
      "[1521] loss: 0.016\n",
      "[1522] loss: 0.016\n",
      "[1523] loss: 0.016\n",
      "[1524] loss: 0.016\n",
      "[1525] loss: 0.016\n",
      "[1526] loss: 0.016\n",
      "[1527] loss: 0.016\n",
      "[1528] loss: 0.016\n",
      "[1529] loss: 0.016\n",
      "[1530] loss: 0.016\n",
      "[1531] loss: 0.016\n",
      "[1532] loss: 0.016\n",
      "[1533] loss: 0.016\n",
      "[1534] loss: 0.016\n",
      "[1535] loss: 0.016\n",
      "[1536] loss: 0.016\n",
      "[1537] loss: 0.016\n",
      "[1538] loss: 0.016\n",
      "[1539] loss: 0.016\n",
      "[1540] loss: 0.016\n",
      "[1541] loss: 0.016\n",
      "[1542] loss: 0.016\n",
      "[1543] loss: 0.016\n",
      "[1544] loss: 0.016\n",
      "[1545] loss: 0.016\n",
      "[1546] loss: 0.016\n",
      "[1547] loss: 0.016\n",
      "[1548] loss: 0.016\n",
      "[1549] loss: 0.016\n",
      "[1550] loss: 0.016\n",
      "[1551] loss: 0.016\n",
      "[1552] loss: 0.016\n",
      "[1553] loss: 0.016\n",
      "[1554] loss: 0.016\n",
      "[1555] loss: 0.016\n",
      "[1556] loss: 0.016\n",
      "[1557] loss: 0.016\n",
      "[1558] loss: 0.016\n",
      "[1559] loss: 0.016\n",
      "[1560] loss: 0.016\n",
      "[1561] loss: 0.016\n",
      "[1562] loss: 0.016\n",
      "[1563] loss: 0.016\n",
      "[1564] loss: 0.016\n",
      "[1565] loss: 0.016\n",
      "[1566] loss: 0.016\n",
      "[1567] loss: 0.016\n",
      "[1568] loss: 0.016\n",
      "[1569] loss: 0.016\n",
      "[1570] loss: 0.016\n",
      "[1571] loss: 0.016\n",
      "[1572] loss: 0.016\n",
      "[1573] loss: 0.016\n",
      "[1574] loss: 0.016\n",
      "[1575] loss: 0.016\n",
      "[1576] loss: 0.016\n",
      "[1577] loss: 0.016\n",
      "[1578] loss: 0.016\n",
      "[1579] loss: 0.016\n",
      "[1580] loss: 0.016\n",
      "[1581] loss: 0.016\n",
      "[1582] loss: 0.016\n",
      "[1583] loss: 0.016\n",
      "[1584] loss: 0.016\n",
      "[1585] loss: 0.016\n",
      "[1586] loss: 0.016\n",
      "[1587] loss: 0.016\n",
      "[1588] loss: 0.016\n",
      "[1589] loss: 0.016\n",
      "[1590] loss: 0.016\n",
      "[1591] loss: 0.016\n",
      "[1592] loss: 0.016\n",
      "[1593] loss: 0.016\n",
      "[1594] loss: 0.016\n",
      "[1595] loss: 0.016\n",
      "[1596] loss: 0.016\n",
      "[1597] loss: 0.016\n",
      "[1598] loss: 0.016\n",
      "[1599] loss: 0.016\n",
      "[1600] loss: 0.016\n",
      "[1601] loss: 0.016\n",
      "[1602] loss: 0.016\n",
      "[1603] loss: 0.016\n",
      "[1604] loss: 0.016\n",
      "[1605] loss: 0.016\n",
      "[1606] loss: 0.016\n",
      "[1607] loss: 0.016\n",
      "[1608] loss: 0.016\n",
      "[1609] loss: 0.016\n",
      "[1610] loss: 0.016\n",
      "[1611] loss: 0.016\n",
      "[1612] loss: 0.016\n",
      "[1613] loss: 0.016\n",
      "[1614] loss: 0.016\n",
      "[1615] loss: 0.016\n",
      "[1616] loss: 0.016\n",
      "[1617] loss: 0.016\n",
      "[1618] loss: 0.016\n",
      "[1619] loss: 0.016\n",
      "[1620] loss: 0.016\n",
      "[1621] loss: 0.016\n",
      "[1622] loss: 0.016\n",
      "[1623] loss: 0.016\n",
      "[1624] loss: 0.016\n",
      "[1625] loss: 0.016\n",
      "[1626] loss: 0.016\n",
      "[1627] loss: 0.016\n",
      "[1628] loss: 0.016\n",
      "[1629] loss: 0.016\n",
      "[1630] loss: 0.016\n",
      "[1631] loss: 0.016\n",
      "[1632] loss: 0.016\n",
      "[1633] loss: 0.016\n",
      "[1634] loss: 0.016\n",
      "[1635] loss: 0.016\n",
      "[1636] loss: 0.016\n",
      "[1637] loss: 0.016\n",
      "[1638] loss: 0.016\n",
      "[1639] loss: 0.016\n",
      "[1640] loss: 0.016\n",
      "[1641] loss: 0.016\n",
      "[1642] loss: 0.016\n",
      "[1643] loss: 0.016\n",
      "[1644] loss: 0.016\n",
      "[1645] loss: 0.016\n",
      "[1646] loss: 0.016\n",
      "[1647] loss: 0.016\n",
      "[1648] loss: 0.016\n",
      "[1649] loss: 0.016\n",
      "[1650] loss: 0.016\n",
      "[1651] loss: 0.016\n",
      "[1652] loss: 0.016\n",
      "[1653] loss: 0.016\n",
      "[1654] loss: 0.016\n",
      "[1655] loss: 0.016\n",
      "[1656] loss: 0.016\n",
      "[1657] loss: 0.016\n",
      "[1658] loss: 0.016\n",
      "[1659] loss: 0.016\n",
      "[1660] loss: 0.016\n",
      "[1661] loss: 0.016\n",
      "[1662] loss: 0.016\n",
      "[1663] loss: 0.016\n",
      "[1664] loss: 0.016\n",
      "[1665] loss: 0.016\n",
      "[1666] loss: 0.016\n",
      "[1667] loss: 0.016\n",
      "[1668] loss: 0.016\n",
      "[1669] loss: 0.016\n",
      "[1670] loss: 0.016\n",
      "[1671] loss: 0.016\n",
      "[1672] loss: 0.016\n",
      "[1673] loss: 0.016\n",
      "[1674] loss: 0.016\n",
      "[1675] loss: 0.016\n",
      "[1676] loss: 0.016\n",
      "[1677] loss: 0.016\n",
      "[1678] loss: 0.016\n",
      "[1679] loss: 0.016\n",
      "[1680] loss: 0.016\n",
      "[1681] loss: 0.016\n",
      "[1682] loss: 0.016\n",
      "[1683] loss: 0.016\n",
      "[1684] loss: 0.016\n",
      "[1685] loss: 0.016\n",
      "[1686] loss: 0.016\n",
      "[1687] loss: 0.016\n",
      "[1688] loss: 0.016\n",
      "[1689] loss: 0.016\n",
      "[1690] loss: 0.016\n",
      "[1691] loss: 0.016\n",
      "[1692] loss: 0.016\n",
      "[1693] loss: 0.016\n",
      "[1694] loss: 0.016\n",
      "[1695] loss: 0.016\n",
      "[1696] loss: 0.016\n",
      "[1697] loss: 0.016\n",
      "[1698] loss: 0.016\n",
      "[1699] loss: 0.016\n",
      "[1700] loss: 0.016\n",
      "[1701] loss: 0.016\n",
      "[1702] loss: 0.016\n",
      "[1703] loss: 0.016\n",
      "[1704] loss: 0.016\n",
      "[1705] loss: 0.016\n",
      "[1706] loss: 0.016\n",
      "[1707] loss: 0.016\n",
      "[1708] loss: 0.016\n",
      "[1709] loss: 0.016\n",
      "[1710] loss: 0.016\n",
      "[1711] loss: 0.016\n",
      "[1712] loss: 0.016\n",
      "[1713] loss: 0.016\n",
      "[1714] loss: 0.016\n",
      "[1715] loss: 0.016\n",
      "[1716] loss: 0.016\n",
      "[1717] loss: 0.015\n",
      "[1718] loss: 0.015\n",
      "[1719] loss: 0.015\n",
      "[1720] loss: 0.015\n",
      "[1721] loss: 0.015\n",
      "[1722] loss: 0.015\n",
      "[1723] loss: 0.015\n",
      "[1724] loss: 0.015\n",
      "[1725] loss: 0.015\n",
      "[1726] loss: 0.015\n",
      "[1727] loss: 0.015\n",
      "[1728] loss: 0.015\n",
      "[1729] loss: 0.015\n",
      "[1730] loss: 0.015\n",
      "[1731] loss: 0.015\n",
      "[1732] loss: 0.015\n",
      "[1733] loss: 0.015\n",
      "[1734] loss: 0.015\n",
      "[1735] loss: 0.015\n",
      "[1736] loss: 0.015\n",
      "[1737] loss: 0.015\n",
      "[1738] loss: 0.015\n",
      "[1739] loss: 0.015\n",
      "[1740] loss: 0.015\n",
      "[1741] loss: 0.015\n",
      "[1742] loss: 0.015\n",
      "[1743] loss: 0.015\n",
      "[1744] loss: 0.015\n",
      "[1745] loss: 0.015\n",
      "[1746] loss: 0.015\n",
      "[1747] loss: 0.015\n",
      "[1748] loss: 0.015\n",
      "[1749] loss: 0.015\n",
      "[1750] loss: 0.015\n",
      "[1751] loss: 0.015\n",
      "[1752] loss: 0.015\n",
      "[1753] loss: 0.015\n",
      "[1754] loss: 0.015\n",
      "[1755] loss: 0.015\n",
      "[1756] loss: 0.015\n",
      "[1757] loss: 0.015\n",
      "[1758] loss: 0.015\n",
      "[1759] loss: 0.015\n",
      "[1760] loss: 0.015\n",
      "[1761] loss: 0.015\n",
      "[1762] loss: 0.015\n",
      "[1763] loss: 0.015\n",
      "[1764] loss: 0.015\n",
      "[1765] loss: 0.015\n",
      "[1766] loss: 0.015\n",
      "[1767] loss: 0.015\n",
      "[1768] loss: 0.015\n",
      "[1769] loss: 0.015\n",
      "[1770] loss: 0.015\n",
      "[1771] loss: 0.015\n",
      "[1772] loss: 0.015\n",
      "[1773] loss: 0.015\n",
      "[1774] loss: 0.015\n",
      "[1775] loss: 0.015\n",
      "[1776] loss: 0.015\n",
      "[1777] loss: 0.015\n",
      "[1778] loss: 0.015\n",
      "[1779] loss: 0.015\n",
      "[1780] loss: 0.015\n",
      "[1781] loss: 0.015\n",
      "[1782] loss: 0.015\n",
      "[1783] loss: 0.015\n",
      "[1784] loss: 0.015\n",
      "[1785] loss: 0.015\n",
      "[1786] loss: 0.015\n",
      "[1787] loss: 0.015\n",
      "[1788] loss: 0.015\n",
      "[1789] loss: 0.015\n",
      "[1790] loss: 0.015\n",
      "[1791] loss: 0.015\n",
      "[1792] loss: 0.015\n",
      "[1793] loss: 0.015\n",
      "[1794] loss: 0.015\n",
      "[1795] loss: 0.015\n",
      "[1796] loss: 0.015\n",
      "[1797] loss: 0.015\n",
      "[1798] loss: 0.015\n",
      "[1799] loss: 0.015\n",
      "[1800] loss: 0.015\n",
      "[1801] loss: 0.015\n",
      "[1802] loss: 0.015\n",
      "[1803] loss: 0.015\n",
      "[1804] loss: 0.015\n",
      "[1805] loss: 0.015\n",
      "[1806] loss: 0.015\n",
      "[1807] loss: 0.015\n",
      "[1808] loss: 0.015\n",
      "[1809] loss: 0.015\n",
      "[1810] loss: 0.015\n",
      "[1811] loss: 0.015\n",
      "[1812] loss: 0.015\n",
      "[1813] loss: 0.015\n",
      "[1814] loss: 0.015\n",
      "[1815] loss: 0.015\n",
      "[1816] loss: 0.015\n",
      "[1817] loss: 0.015\n",
      "[1818] loss: 0.015\n",
      "[1819] loss: 0.015\n",
      "[1820] loss: 0.015\n",
      "[1821] loss: 0.015\n",
      "[1822] loss: 0.015\n",
      "[1823] loss: 0.015\n",
      "[1824] loss: 0.015\n",
      "[1825] loss: 0.015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1826] loss: 0.015\n",
      "[1827] loss: 0.015\n",
      "[1828] loss: 0.015\n",
      "[1829] loss: 0.015\n",
      "[1830] loss: 0.015\n",
      "[1831] loss: 0.015\n",
      "[1832] loss: 0.015\n",
      "[1833] loss: 0.015\n",
      "[1834] loss: 0.015\n",
      "[1835] loss: 0.015\n",
      "[1836] loss: 0.015\n",
      "[1837] loss: 0.015\n",
      "[1838] loss: 0.015\n",
      "[1839] loss: 0.015\n",
      "[1840] loss: 0.015\n",
      "[1841] loss: 0.015\n",
      "[1842] loss: 0.015\n",
      "[1843] loss: 0.015\n",
      "[1844] loss: 0.015\n",
      "[1845] loss: 0.015\n",
      "[1846] loss: 0.015\n",
      "[1847] loss: 0.015\n",
      "[1848] loss: 0.015\n",
      "[1849] loss: 0.015\n",
      "[1850] loss: 0.015\n",
      "[1851] loss: 0.015\n",
      "[1852] loss: 0.015\n",
      "[1853] loss: 0.015\n",
      "[1854] loss: 0.015\n",
      "[1855] loss: 0.015\n",
      "[1856] loss: 0.015\n",
      "[1857] loss: 0.015\n",
      "[1858] loss: 0.015\n",
      "[1859] loss: 0.015\n",
      "[1860] loss: 0.015\n",
      "[1861] loss: 0.015\n",
      "[1862] loss: 0.015\n",
      "[1863] loss: 0.015\n",
      "[1864] loss: 0.015\n",
      "[1865] loss: 0.015\n",
      "[1866] loss: 0.015\n",
      "[1867] loss: 0.015\n",
      "[1868] loss: 0.015\n",
      "[1869] loss: 0.015\n",
      "[1870] loss: 0.015\n",
      "[1871] loss: 0.015\n",
      "[1872] loss: 0.015\n",
      "[1873] loss: 0.015\n",
      "[1874] loss: 0.015\n",
      "[1875] loss: 0.015\n",
      "[1876] loss: 0.015\n",
      "[1877] loss: 0.015\n",
      "[1878] loss: 0.015\n",
      "[1879] loss: 0.015\n",
      "[1880] loss: 0.015\n",
      "[1881] loss: 0.015\n",
      "[1882] loss: 0.015\n",
      "[1883] loss: 0.015\n",
      "[1884] loss: 0.015\n",
      "[1885] loss: 0.015\n",
      "[1886] loss: 0.015\n",
      "[1887] loss: 0.015\n",
      "[1888] loss: 0.015\n",
      "[1889] loss: 0.015\n",
      "[1890] loss: 0.015\n",
      "[1891] loss: 0.015\n",
      "[1892] loss: 0.015\n",
      "[1893] loss: 0.015\n",
      "[1894] loss: 0.015\n",
      "[1895] loss: 0.015\n",
      "[1896] loss: 0.015\n",
      "[1897] loss: 0.015\n",
      "[1898] loss: 0.015\n",
      "[1899] loss: 0.015\n",
      "[1900] loss: 0.015\n",
      "[1901] loss: 0.015\n",
      "[1902] loss: 0.015\n",
      "[1903] loss: 0.015\n",
      "[1904] loss: 0.015\n",
      "[1905] loss: 0.015\n",
      "[1906] loss: 0.015\n",
      "[1907] loss: 0.015\n",
      "[1908] loss: 0.015\n",
      "[1909] loss: 0.015\n",
      "[1910] loss: 0.015\n",
      "[1911] loss: 0.015\n",
      "[1912] loss: 0.015\n",
      "[1913] loss: 0.015\n",
      "[1914] loss: 0.015\n",
      "[1915] loss: 0.015\n",
      "[1916] loss: 0.015\n",
      "[1917] loss: 0.015\n",
      "[1918] loss: 0.015\n",
      "[1919] loss: 0.015\n",
      "[1920] loss: 0.015\n",
      "[1921] loss: 0.015\n",
      "[1922] loss: 0.015\n",
      "[1923] loss: 0.015\n",
      "[1924] loss: 0.015\n",
      "[1925] loss: 0.015\n",
      "[1926] loss: 0.015\n",
      "[1927] loss: 0.015\n",
      "[1928] loss: 0.015\n",
      "[1929] loss: 0.015\n",
      "[1930] loss: 0.015\n",
      "[1931] loss: 0.015\n",
      "[1932] loss: 0.015\n",
      "[1933] loss: 0.015\n",
      "[1934] loss: 0.015\n",
      "[1935] loss: 0.015\n",
      "[1936] loss: 0.015\n",
      "[1937] loss: 0.015\n",
      "[1938] loss: 0.015\n",
      "[1939] loss: 0.015\n",
      "[1940] loss: 0.015\n",
      "[1941] loss: 0.015\n",
      "[1942] loss: 0.015\n",
      "[1943] loss: 0.015\n",
      "[1944] loss: 0.015\n",
      "[1945] loss: 0.015\n",
      "[1946] loss: 0.015\n",
      "[1947] loss: 0.015\n",
      "[1948] loss: 0.015\n",
      "[1949] loss: 0.015\n",
      "[1950] loss: 0.015\n",
      "[1951] loss: 0.015\n",
      "[1952] loss: 0.015\n",
      "[1953] loss: 0.015\n",
      "[1954] loss: 0.015\n",
      "[1955] loss: 0.015\n",
      "[1956] loss: 0.015\n",
      "[1957] loss: 0.015\n",
      "[1958] loss: 0.015\n",
      "[1959] loss: 0.015\n",
      "[1960] loss: 0.015\n",
      "[1961] loss: 0.015\n",
      "[1962] loss: 0.015\n",
      "[1963] loss: 0.015\n",
      "[1964] loss: 0.015\n",
      "[1965] loss: 0.015\n",
      "[1966] loss: 0.015\n",
      "[1967] loss: 0.015\n",
      "[1968] loss: 0.015\n",
      "[1969] loss: 0.015\n",
      "[1970] loss: 0.015\n",
      "[1971] loss: 0.015\n",
      "[1972] loss: 0.015\n",
      "[1973] loss: 0.015\n",
      "[1974] loss: 0.015\n",
      "[1975] loss: 0.015\n",
      "[1976] loss: 0.015\n",
      "[1977] loss: 0.015\n",
      "[1978] loss: 0.015\n",
      "[1979] loss: 0.015\n",
      "[1980] loss: 0.015\n",
      "[1981] loss: 0.015\n",
      "[1982] loss: 0.015\n",
      "[1983] loss: 0.015\n",
      "[1984] loss: 0.015\n",
      "[1985] loss: 0.015\n",
      "[1986] loss: 0.015\n",
      "[1987] loss: 0.015\n",
      "[1988] loss: 0.015\n",
      "[1989] loss: 0.015\n",
      "[1990] loss: 0.015\n",
      "[1991] loss: 0.015\n",
      "[1992] loss: 0.015\n",
      "[1993] loss: 0.015\n",
      "[1994] loss: 0.015\n",
      "[1995] loss: 0.015\n",
      "[1996] loss: 0.015\n",
      "[1997] loss: 0.015\n",
      "[1998] loss: 0.015\n",
      "[1999] loss: 0.015\n",
      "[2000] loss: 0.015\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCj0RURCRKEvIhIi/x+87OzHTV6uugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26297eb8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "pt_data      = torch.FloatTensor(norm_oau_data)#continuous_norm_oau_data\n",
    "pt_data      = torch.autograd.Variable(pt_data)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss0   = 0\n",
    "epsilon = 0.00001\n",
    "\n",
    "def low_parameter_to_zero(para,epsilon):\n",
    "    for i in para:\n",
    "        para = i.data\n",
    "        mask = (torch.abs(para)<epsilon)\n",
    "        mask = mask.type(torch.FloatTensor)\n",
    "        i.data.addcmul_(-1.0,i.data,mask)\n",
    "    \n",
    "for epoch in range(100):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out  = net.forward(pt_data)\n",
    "    loss = criterion(out,pt_data)\n",
    "    print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1, loss.data[0]))\n",
    "    if abs(loss.data[0]-loss0) < epsilon:\n",
    "        break\n",
    "    loss0 = loss.data[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()        # Does the update\n",
    "loss_recorder = []\n",
    "for epoch in range(2000):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out  = net.forward(pt_data)\n",
    "    loss = criterion(out,pt_data)\n",
    "    loss_recorder.append(loss.data[0])\n",
    "    print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1, loss.data[0]))\n",
    "    #if abs(loss.data[0]-loss0) < epsilon:\n",
    "        #break\n",
    "    loss0 = loss.data[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()        # Does the update\n",
    "    #low_parameter_to_zero(net.parameters(),0.1)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_recorder)\n",
    "plt.title('error curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### encode all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_oau_data      = net.encode_data(pt_data)\n",
    "encoded_oau_data      = encoded_oau_data.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "Density-based spatial clustering of applications with noise is a cluster algorithm proposed by Martin Ester .etc. It is able to find cluster in any shape and does not need to specify the number of clusters in advance.\n",
    "\n",
    "We require that for every point p in a cluster C there is a point q in C so that p is inside of the Eps-\n",
    "neighborhood of q and NEps(q) contains at least MinPts points.\n",
    "\n",
    "We use the scikit-learn tool kits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to determine epsilon ?\n",
    "k-distance plot for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmdwlqYNM7pLUQf8fIJm5Ehyf4TMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262972dc518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k = 5\n",
    "\n",
    "neigh = NearestNeighbors(k+1)\n",
    "nbrs  = neigh.fit(norm_oau_data)\n",
    "distances, _ = nbrs.kneighbors(norm_oau_data)\n",
    "distances    = np.sum(distances,axis=1)/k\n",
    "distances.sort()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.title('%d-NN distance' % k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-distance plot for encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262972dc8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k = 5\n",
    "\n",
    "neigh = NearestNeighbors(k+1)\n",
    "nbrs  = neigh.fit(encoded_oau_data)\n",
    "distances, _ = nbrs.kneighbors(encoded_oau_data)\n",
    "distances    = np.sum(distances,axis=1)/k\n",
    "distances.sort()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.title('%d-NN distance' % k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCN for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Estimated number of clusters: 6\n",
      "Silhouette Coefficient: 0.812\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/y3nu1kAivuiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262984efe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.02, min_samples=10).fit(norm_oau_data)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(norm_oau_data, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x   = np.arange(len(labels))\n",
    "plt.scatter(x,labels)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCAN for encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Estimated number of clusters: 5\n",
      "Silhouette Coefficient: 0.971\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZBTsIiKRUbCLiERGwS4iEhkFu4hIZP4/ZZb7aSYRgAIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26297f200b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.02, min_samples=10).fit(encoded_oau_data)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(encoded_oau_data, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x   = np.arange(len(labels))\n",
    "plt.scatter(x,labels)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...,  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(labels != -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### convert all data to discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def z_test(data,length,epsilon=None):\n",
    "    assert(length<len(data))\n",
    "    epsilon = 0.95 if epsilon == None else epsilon\n",
    "    thresh_hold = st.norm.ppf(epsilon)\n",
    "    result = np.zeros([len(data)-length,len(data[0])])\n",
    "    for i in range(length,len(data)):\n",
    "        mean = np.mean(data[i-length:i,:],0)\n",
    "        var  = np.var(data[i-length:i,:],0)\n",
    "        norm = (data[i,:]-mean)/var\n",
    "        z    = abs(norm) < thresh_hold\n",
    "        result[i-length,:] = [int(j) for j in z]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_data(data,label,length,epsilon=None):\n",
    "    mask = (label!=-1)\n",
    "    data = data[mask]\n",
    "    label= label[mask]\n",
    "    result = z_test(data,length,epsilon)\n",
    "    label  = label[length:]\n",
    "    headings = ['feature'+str(i) for i in range(len(result[0]))]\n",
    "    headings.append('mode')\n",
    "    labeled_data = np.hstack((result,[[i] for i in label]))\n",
    "    return headings, labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature0', 'feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'mode']\n",
      "10175\n",
      "[  167.   514.   473.  1443.  3019.  5337.  7221.]\n"
     ]
    }
   ],
   "source": [
    "z_len = 100\n",
    "\n",
    "headings, labeled_data = discrete_data(encoded_oau_data,labels,z_len,0.95)\n",
    "print(headings)\n",
    "print(len(labeled_data))\n",
    "print(sum(labeled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAN classifier\n",
    "TAN classifier is the Tree Augmented Naive Baysian, which is created by adding a spaned tree into naive baysian network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class node:\n",
    "    '''\n",
    "    The class to store the information of a node in a graph\n",
    "    '''\n",
    "    def __init__(self,name,domain):\n",
    "        self.name          = name       #a string\n",
    "        self.domain        = domain     #a list for string\n",
    "        self.father        = []         #a list\n",
    "        self.children      = []         #a list\n",
    "        self.pro_table     = {}         #init as an empty dict\n",
    "                                        #for example\n",
    "                                        #{(on,on):[0.1,0.9],(on,off):[0.2,0.8],(off,on):[0.3,0.7],(off,off):[0.4,0.6]}\n",
    "        \n",
    "    def add_father(self,fa):            #fa is a node\n",
    "        self.father.append(fa)\n",
    "        \n",
    "    def add_child(self,ch):             #ch is a node\n",
    "        self.children.append(ch)\n",
    "        \n",
    "    def add_pro(self,instance,table):   #instance is a string list, table is a float list\n",
    "        self.pro_table[instance] = table\n",
    "        \n",
    "    def print_self(self):\n",
    "        #structring information\n",
    "        father_info   = ''.join([i.name+' ' for i in self.father])\n",
    "        children_info = ''.join([i.name+' ' for i in self.children])\n",
    "        \n",
    "        print(father_info + '==>' + self.name + '==>' + children_info)\n",
    "        \n",
    "        #conditional probability table\n",
    "        for i in self.pro_table:\n",
    "            print(i,':',self.pro_table[i])\n",
    "        \n",
    "            \n",
    "        \n",
    "class graph:\n",
    "    '''\n",
    "    The class to store a graph\n",
    "    The code will not check the validation of data, so please preprocess the data to ensure\n",
    "    1) there are heandings: name1,name2,...\n",
    "    2) no empty data\n",
    "    3) all data is discretized\n",
    "    4) last colum is the label\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.nodes         = []         #a list\n",
    "        \n",
    "    def get_node(self,the_node):\n",
    "        '''\n",
    "        functional function\n",
    "        get the node by name\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "        output:\n",
    "            type         description\n",
    "            ------------------------------\n",
    "            class node,  the found node\n",
    "        '''\n",
    "        names              = [i.name for i in self.nodes]\n",
    "        #print('find node ',the_node,' in ',names)\n",
    "        node_index         = names.index(the_node)\n",
    "        return self.nodes[node_index]\n",
    "    \n",
    "    def get_node_index(self,the_node):\n",
    "        '''\n",
    "        functional function\n",
    "        get the node by name\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "        output:\n",
    "            type         description\n",
    "            ------------------------------\n",
    "            int,         index\n",
    "        '''\n",
    "        names              = [i.name for i in self.nodes]\n",
    "        node_index         = names.index(the_node)\n",
    "        return node_index\n",
    "        \n",
    "    def add_node(self,name,domain):\n",
    "        '''\n",
    "        functional function\n",
    "        add a node by name and domain\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            name,     string, node name\n",
    "            domain,   list,   domain of the node\n",
    "        '''\n",
    "        self.nodes.append(node(name,domain))\n",
    "        \n",
    "    def add_connection(self,from_node1,to_node2):#both from_node1 and to_node2 are strings\n",
    "        '''\n",
    "        functional function\n",
    "        add a connection betwen to nodes\n",
    "        input:\n",
    "            variable     type         description\n",
    "            -------------------------------------------\n",
    "            from_node1,  class node,  the first node\n",
    "            to_node2,    class node,  the second node\n",
    "        '''\n",
    "        #print('from node 1 =',from_node1)\n",
    "        #print('to node 2 =',to_node2)\n",
    "        from_node1         = self.get_node(from_node1)\n",
    "        to_node2           = self.get_node(to_node2)\n",
    "        \n",
    "        from_node1.add_child(to_node2)\n",
    "        to_node2.add_father(from_node1)\n",
    "        \n",
    "    def add_pro(self,the_node,instance,table):#the_node is a string\n",
    "        '''\n",
    "        functional function\n",
    "        add a conditioanal probability P(a|b) into a node\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "            instance, list,   b part in P(a|b)\n",
    "                              where each entry in it is a value in domain\n",
    "            table,    list,   a part in P(a|b)\n",
    "                              where each entry in it is a probability responding to a value\n",
    "        '''\n",
    "        the_node           = self.get_node(the_node)\n",
    "        the_node.add_pro(instance,table)\n",
    "        \n",
    "    def get_nodes_from_data(self,headings,data):#headings = ['data1','data2',...], data = numpy 2d vector\n",
    "        '''\n",
    "        interface/functional function\n",
    "        get the basic information about node from data and insert them into the class\n",
    "        input:\n",
    "            variable  type            description\n",
    "            ------------------------------------------------\n",
    "            headings, numpy arrary,   the names of variables\n",
    "            data,     numpy 2darrary, all the data where row means different instance\n",
    "                                      and col means each sensor\n",
    "        '''\n",
    "        for i,name in enumerate(headings):\n",
    "            domain = list(set(data[:,i]))\n",
    "            self.add_node(name,domain)\n",
    "            \n",
    "    def mutual_information(self,data1,data2):\n",
    "        '''\n",
    "        functional function\n",
    "        get the mutual information between two cols\n",
    "        input:\n",
    "            variable  type            description\n",
    "            ------------------------------------------------\n",
    "            data1,    numpy arrary,   data of the first col\n",
    "            data2,    numpy arrary,   data of the second col\n",
    "        output:\n",
    "            type    description\n",
    "            -----------------------------\n",
    "            float,  mutual information entropy with laplace smooth\n",
    "        '''\n",
    "        assert(len(data1)==len(data2))\n",
    "        \n",
    "        mi       = 0.0\n",
    "        length   = len(data1)\n",
    "        \n",
    "        domain1 = list(set(data1))\n",
    "        #print('domain1=',domain1)\n",
    "        domain2 = list(set(data2))\n",
    "        #print('domain2=',domain2)\n",
    "        len1    = len(domain1)\n",
    "        len2    = len(domain2)\n",
    "        \n",
    "        for x in domain1:\n",
    "            for y in domain2:\n",
    "                Px     = float(sum(data1==x) + 1)/(length+len1)    #laplace smooth\n",
    "                Py     = float(sum(data2==y) + 1)/(length+len2)    #laplace smooth\n",
    "                \n",
    "                indexxy= [1 if (i==x)and(j==y) else 0 for i,j in zip(data1,data2)]\n",
    "                Pxy    = float(sum(indexxy) + 1)/(length+len1*len2) #laplace smooth\n",
    "                \n",
    "                mi     = mi + Pxy*math.log(Pxy/(Px*Py))\n",
    "        return mi\n",
    "    \n",
    "    def get_max_span_tree_prime(self,mi_dict):\n",
    "        '''\n",
    "        functional function\n",
    "        judge if some edges compose a span tree\n",
    "        input:\n",
    "            variable  type    description\n",
    "            -----------------------------\n",
    "            edges,    dict,   all the edges\n",
    "        output:\n",
    "            type    description\n",
    "            -----------------------------\n",
    "            set,    all the edges that compose a max span tree\n",
    "        '''\n",
    "        #prepare structure to qurey\n",
    "        #test\n",
    "        #print(mi_dict)\n",
    "        \n",
    "        Vnew  = set()\n",
    "        V     = set()\n",
    "        Enew  = set()\n",
    "        edges  = defaultdict(list)\n",
    "        initial_node = max(mi_dict.items(),key=lambda d: d[1])[0][0]\n",
    "        Vnew.add(initial_node)\n",
    "        #print('Vnew=',Vnew)\n",
    "        for i in mi_dict:\n",
    "            edges[i[0]].append(i[1])\n",
    "            edges[i[1]].append(i[0])\n",
    "            V.add(i[0])\n",
    "            V.add(i[1])\n",
    "        #print('V=',V)\n",
    "        while Vnew != V:\n",
    "            #print('Vnew=',Vnew)\n",
    "            new_edges = {}\n",
    "            for u in Vnew:\n",
    "                v = [i for i in edges[u] if i not in Vnew]\n",
    "                for vi in v:\n",
    "                    edge = (u,vi) if (u,vi) in mi_dict else (vi,u)\n",
    "                    new_edges[edge] = mi_dict[edge]\n",
    "            new_edges = sorted(new_edges.items(),key=lambda d:d[1],reverse = True)\n",
    "            best_edge = new_edges[0]\n",
    "            Enew.add(best_edge[0])\n",
    "            Vnew.add(best_edge[0][0])\n",
    "            Vnew.add(best_edge[0][1])\n",
    "        return Enew\n",
    "        \n",
    "        \n",
    "    def get_basic_tan_structure(self,headings,data):#headings = ['data1','data2',...], data = numpy 2d vector\n",
    "        '''\n",
    "        interface function\n",
    "        get the basic structure of TAN and connect the nodes in the class.\n",
    "        we assumpe that the last col is the label colum and is ignored when handle features\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "        '''\n",
    "        #computing mutual information entropy between features\n",
    "        mi_dict = {}\n",
    "        for i in range(len(headings)-1):\n",
    "            for j in range(i+1,len(headings)-1):\n",
    "                data1   = data[:,i]\n",
    "                data2   = data[:,j]\n",
    "                mi      = self.mutual_information(data1,data2)\n",
    "                mi_dict[(headings[i],headings[j])] = mi\n",
    "        \n",
    "        #set maximal span tree, mst\n",
    "        mst             = self.get_max_span_tree_prime(mi_dict)\n",
    "        Visited         = set()\n",
    "        edges           = defaultdict(list)\n",
    "        for i in mst:\n",
    "            edges[i[0]].append(i[1])\n",
    "            edges[i[1]].append(i[0])\n",
    "        initial_node    = self.nodes[0].name\n",
    "        start           = [initial_node]\n",
    "        Visited.add(initial_node)\n",
    "        while len(start)!=0:\n",
    "            new_start = []\n",
    "            for i in start:\n",
    "                ends = edges[i]\n",
    "                for j in ends:\n",
    "                    if j not in Visited:\n",
    "                        Visited.add(j)\n",
    "                        new_start.append(j)\n",
    "                        self.add_connection(i,j)\n",
    "            start = new_start\n",
    "        \n",
    "        #set the connection between label and features\n",
    "        for i in range(len(self.nodes)-1):\n",
    "            #print('class node =',self.nodes[-1].name)\n",
    "            self.add_connection(self.nodes[-1].name,self.nodes[i].name)\n",
    "            \n",
    "    def get_cpt(self,headings,data,node_name):\n",
    "        '''\n",
    "        interface/functional function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        x, y==>x, y,z==>x\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "            node_name,string,         the node name\n",
    "        '''\n",
    "        the_index     = self.get_node_index(node_name)\n",
    "        the_node      = self.nodes[the_index]\n",
    "        father_nodes  = the_node.father\n",
    "        \n",
    "        domain_x = the_node.domain\n",
    "        data_x   = data[:,the_index]\n",
    "        len_x    = len(domain_x)\n",
    "        \n",
    "        length   = len(data)\n",
    "        \n",
    "        if len(father_nodes) == 0:\n",
    "            table = []\n",
    "            for x in domain_x:\n",
    "                Px = float(sum(data_x == x)+1)/(length + len_x)          #laplace smooth\n",
    "                table.append(Px)\n",
    "            self.add_pro(node_name,(),table)\n",
    "        elif len(father_nodes)==1:#y-->x\n",
    "            domain_y = father_nodes[0].domain\n",
    "            data_y   = data[:,self.get_node_index(father_nodes[0].name)]\n",
    "            len_y    = len(domain_y)\n",
    "            \n",
    "            for y in domain_y:\n",
    "                table    = []\n",
    "                Py     = float(sum(data_y == y) + 1)/(length+len_y)       #laplace smooth\n",
    "                for x in domain_x:\n",
    "                    indexxy= [1 if (i==x)and(j==y) else 0 for i,j in zip(data_x,data_y)]\n",
    "                    Pxy    = float(sum(indexxy) + 1)/(length+len_x*len_y) #laplace smooth\n",
    "                    table.append(float(Pxy)/Py)\n",
    "                self.add_pro(node_name,(y,),table)\n",
    "                    \n",
    "        elif len(father_nodes)==2:\n",
    "            domain_y = father_nodes[0].domain\n",
    "            data_y   = data[:,self.get_node_index(father_nodes[0].name)]\n",
    "            len_y    = len(domain_y)\n",
    "            domain_z = father_nodes[1].domain\n",
    "            data_z   = data[:,self.get_node_index(father_nodes[1].name)]\n",
    "            len_z    = len(domain_z)\n",
    "            \n",
    "            for y in domain_y:\n",
    "                for z in domain_z:\n",
    "                    table = []\n",
    "                    indexyz= [1 if (i==y)and(j==z) else 0 for i,j in zip(data_y,data_z)]\n",
    "                    Pyz    = float(sum(indexyz) + 1)/(length+len_y*len_z) #laplace smooth\n",
    "                    for x in domain_x:\n",
    "                        indexxyz= [1 if (i==y)and(j==z)and(k==x) else 0 for i,j,k in zip(data_y,data_z,data_x)]\n",
    "                        Pxyz    = float(sum(indexxyz) + 1)/(length+len_y*len_z*len_x) #laplace smooth\n",
    "                        table.append(float(Pxyz)/Pyz)\n",
    "                    self.add_pro(node_name,(y,z),table)\n",
    "        \n",
    "    def train_tan(self,headings,data):\n",
    "        '''\n",
    "        interface function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "        '''\n",
    "        for i in self.nodes:\n",
    "            self.get_cpt(headings,data,i.name)\n",
    "    \n",
    "    def print_tan(self):\n",
    "        for i in self.nodes:\n",
    "            i.print_self()\n",
    "        \n",
    "    def predict(self,features):\n",
    "        '''\n",
    "        interface function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            features, numpy arrary,   names of variables/nodes\n",
    "        '''\n",
    "        weight   = []\n",
    "        features = np.ndarray.tolist(features)\n",
    "        #print(self.nodes[-1].domain)\n",
    "        for i in self.nodes[-1].domain:\n",
    "            #print('i=',i)\n",
    "            Pi = 1\n",
    "            new_features = features + [i]\n",
    "            #print('new_features=',new_features)\n",
    "            for node in self.nodes:\n",
    "                #father\n",
    "                instance = []\n",
    "                father = node.father\n",
    "                for f in father:\n",
    "                    index = self.get_node_index(f.name)\n",
    "                    instance.append(new_features[index])\n",
    "                #print('instance=',instance)\n",
    "                table = node.pro_table[tuple(instance)]\n",
    "                #print('table = ',table)\n",
    "                #this node\n",
    "                index = self.get_node_index(node.name)           #index of this node in features\n",
    "                value = new_features[index]                          #value of this ndde in features\n",
    "                index = node.domain.index(value)                 #index value in domain\n",
    "                Pi = Pi*table[index]\n",
    "            weight.append(Pi)\n",
    "        norm = sum(weight)\n",
    "        weight = [i/norm for i in weight]\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train TAN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode ==>feature0==>feature1 \n",
      "feature0 mode ==>feature1==>feature2 feature4 \n",
      "feature1 mode ==>feature2==>\n",
      "feature4 mode ==>feature3==>\n",
      "feature1 mode ==>feature4==>feature5 feature3 \n",
      "feature4 mode ==>feature5==>\n",
      "==>mode==>feature0 feature1 feature2 feature3 feature4 feature5 \n",
      "mode ==>feature0==>feature1 \n",
      "(0.0,) : [0.9929301973531409, 0.006739345230903672]\n",
      "(1.0,) : [0.9983290240352765, 0.0023601158960644833]\n",
      "(2.0,) : [0.9576956163715572, 0.04214797333664555]\n",
      "(3.0,) : [0.9995090819833088, 0.017848376463987656]\n",
      "(4.0,) : [0.9995090819833088, 0.016658484699721813]\n",
      "feature0 mode ==>feature1==>feature2 feature4 \n",
      "(0.0, 0.0) : [0.9799686653248089, 0.01921190628890482]\n",
      "(0.0, 1.0) : [0.9990191270230505, 0.0011808736726040788]\n",
      "(0.0, 2.0) : [0.8751449425685682, 0.12422312581914287]\n",
      "(0.0, 3.0) : [0.9990191270230505, 0.017839627268268758]\n",
      "(0.0, 4.0) : [0.9990191270230506, 0.01665031878371751]\n",
      "(1.0, 0.0) : [0.9276606179499755, 0.09514467876410006]\n",
      "(1.0, 1.0) : [0.9990191270230505, 0.49950956351152526]\n",
      "(1.0, 2.0) : [0.6977276442700671, 0.30922020598332517]\n",
      "(1.0, 3.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "(1.0, 4.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "feature1 mode ==>feature2==>\n",
      "(0.0, 0.0) : [0.988060665741036, 0.011122020405626626]\n",
      "(0.0, 1.0) : [0.9990191270230505, 0.0011794794888111578]\n",
      "(0.0, 2.0) : [0.9474320121787003, 0.051972093313337894]\n",
      "(0.0, 3.0) : [0.8741417361451691, 0.14271701814615007]\n",
      "(0.0, 4.0) : [0.5994114762138303, 0.4162579695929377]\n",
      "(1.0, 0.0) : [0.8188681369041397, 0.18833967148795214]\n",
      "(1.0, 1.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "(1.0, 2.0) : [0.44372676961683716, 0.557827938946881]\n",
      "(1.0, 3.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "(1.0, 4.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "feature4 mode ==>feature3==>\n",
      "(0.0, 0.0) : [0.9869547011246789, 0.012276082493079857]\n",
      "(0.0, 1.0) : [0.9954681111687032, 0.004734687805796448]\n",
      "(0.0, 2.0) : [0.9910588152539426, 0.008623671083200304]\n",
      "(0.0, 3.0) : [0.9023398566659812, 0.12890569380942588]\n",
      "(0.0, 4.0) : [0.982368808239333, 0.03330063756743502]\n",
      "(1.0, 0.0) : [0.7886296684844214, 0.21105106060950538]\n",
      "(1.0, 1.0) : [0.9990191270230505, 0.24975478175576263]\n",
      "(1.0, 2.0) : [0.29303662862779967, 0.7066561458173836]\n",
      "(1.0, 3.0) : [0.9605953144452408, 0.07684762515561927]\n",
      "(1.0, 4.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "feature1 mode ==>feature4==>feature5 feature3 \n",
      "(0.0, 0.0) : [0.7666016123701764, 0.23258107377648624]\n",
      "(0.0, 1.0) : [0.995480688556617, 0.004717917955244631]\n",
      "(0.0, 2.0) : [0.5470544044315048, 0.4523497010605335]\n",
      "(0.0, 3.0) : [0.5530284453163314, 0.4638303089749877]\n",
      "(0.0, 4.0) : [0.9990191270230506, 0.01665031878371751]\n",
      "(1.0, 0.0) : [0.2784151665474075, 0.7287926418446844]\n",
      "(1.0, 1.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "(1.0, 2.0) : [0.2180600124974171, 0.783494696066301]\n",
      "(1.0, 3.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "(1.0, 4.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "feature4 mode ==>feature5==>\n",
      "(0.0, 0.0) : [0.6595219491109798, 0.3397088345067788]\n",
      "(0.0, 1.0) : [0.5231830025405075, 0.47701979643399217]\n",
      "(0.0, 2.0) : [0.8020014107376282, 0.19768107559951464]\n",
      "(0.0, 3.0) : [0.6123020455947729, 0.4189435048806341]\n",
      "(0.0, 4.0) : [0.5161598822952428, 0.4995095635115253]\n",
      "(1.0, 0.0) : [0.005954418637885732, 0.993726310456041]\n",
      "(1.0, 1.0) : [0.24975478175576263, 0.9990191270230505]\n",
      "(1.0, 2.0) : [0.00471553195493011, 0.9949772424902532]\n",
      "(1.0, 3.0) : [0.49950956351152526, 0.5379333760893349]\n",
      "(1.0, 4.0) : [0.9990191270230505, 0.9990191270230505]\n",
      "==>mode==>feature0 feature1 feature2 feature3 feature4 feature5 \n",
      "() : [0.61188605108055, 0.08320235756385069, 0.29351669941060904, 0.00550098231827112, 0.005893909626719057]\n"
     ]
    }
   ],
   "source": [
    "#%xmode Plain\n",
    "\n",
    "tan = graph()\n",
    "tan.get_nodes_from_data(headings,labeled_data)\n",
    "tan.get_basic_tan_structure(headings,labeled_data)\n",
    "tan.print_tan()\n",
    "tan.train_tan(headings,labeled_data)\n",
    "tan.print_tan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08795717539369663, 0.0004970052588744637, 0.8081365315895117, 0.01772090355650494, 0.08568838420141225]\n"
     ]
    }
   ],
   "source": [
    "w = tan.predict(np.array([1,0,1,0,0,0]))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### test all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  2.  2.  2.]\n",
      "[0 0 0 ..., 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "input_data   = labeled_data[:,:-1]\n",
    "output_label = labeled_data[:,-1]\n",
    "print(output_label)\n",
    "predict_labels = []\n",
    "for i in input_data:\n",
    "    w = tan.predict(i)\n",
    "    index = w.index(max(w))\n",
    "    predict_labels.append(index)\n",
    "\n",
    "\n",
    "predict_labels = np.array(predict_labels)\n",
    "print(predict_labels)\n",
    "\n",
    "error_rate = sum([i==j for i,j in zip(predict_labels,output_label)])/float(len(output_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.703783783784\n"
     ]
    }
   ],
   "source": [
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
