{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Approach V0.01\n",
    "\n",
    "This document records the process to implement the first simple version of hybrid approach based on G.B.'s approach.\n",
    "\n",
    "## Auto encoder decoder\n",
    "This subsetion describes how to design an auto encoder decoder to extract features of data.\n",
    "\n",
    "### What is auto encoder decoder ?\n",
    "Auto encoder decoder is a special artificial neural network that tranlate input data and ouput the same/similar data. Because the there is no information lost in the translating process, we can use the data inthe middle layer as the feathures of the input data.\n",
    "\n",
    "When middle layer's hidden variables are less than input variables, auto encoder decoder compresses the data.\n",
    "When middle layer's hidden variables are more than input variables, auto encoder decoder represents data sparsely.\n",
    "\n",
    "#### Auto encoder decoder test\n",
    "##### Test data\n",
    "   Test data comes from a simple function. \n",
    "+ Generate feature vector randomly\n",
    "+ Generate observation data\n",
    "[v1,v2,...,vn]   ==>   [v1,((v1+v2)/2),v2,((v2+v3)/2),...,vn] and then max min normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import torch\n",
    "\n",
    "#input dimension\n",
    "D = 3\n",
    "#input number\n",
    "N = 100000\n",
    "\n",
    "\n",
    "feature = random.rand(N,D)\n",
    "def transfer(input):\n",
    "    n,dim   = input.shape\n",
    "    new_dim = dim*2-1\n",
    "    output  = np.zeros((n,new_dim))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(new_dim):\n",
    "            if j%2 == 0:\n",
    "                output[i][j] = input[i][int(j/2)]\n",
    "            else:\n",
    "                output[i][j] = ((input[i][int(j/2)]+input[i][int(j/2)+1])/2)\n",
    "    return output\n",
    "\n",
    "np_test_data   = transfer(feature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Air Unit Data\n",
    "Read and pre-process data by panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "file_name = './OAU1201.csv'\n",
    "raw_oau_data  = pandas.read_csv(file_name)\n",
    "\n",
    "oau_headings = np.array(raw_oau_data.columns[1:])\n",
    "oau_data     = np.array(raw_oau_data[oau_headings])\n",
    "\n",
    "#delete nan\n",
    "mask     = [[1 if type(j)==float and np.isnan(j) else 0 for j in i] for i in oau_data]\n",
    "oau_data = np.array([i for i,j in zip(oau_data,mask) if sum(j)==0])\n",
    "\n",
    "# #############################################################################\n",
    "# extract continuous data\n",
    "continuous_oau_data = oau_data[:,0:12]\n",
    "new_data        = oau_data\n",
    "for i in range(12,len(oau_data[0])):\n",
    "    dis_data = oau_data[:,i]\n",
    "    domain   = list(set(dis_data))\n",
    "    new_data[:,i] = [domain.index(k) for k in dis_data]\n",
    "\n",
    "#print(new_data)\n",
    "    \n",
    "norm_oau_data                    = MinMaxScaler().fit_transform(new_data)\n",
    "continuous_norm_oau_data         = MinMaxScaler().fit_transform(continuous_oau_data)\n",
    "discrete_oau_data                = oau_data[:,12:]\n",
    "# #############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### auto encoder decoder structure design\n",
    "Now, we just use some simple structure.\n",
    "      \n",
    "The structure works!\n",
    "The \"works\" mean just works! But the result is bad! The good thing is that we can start now. The bad thing is that the structure need to be re-designed!\n",
    "\n",
    "TODO: use practical data and design new auto encoder decoder structrue. Quite importance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_endecoder (\n",
      "  (in1): Linear (26 -> 6)\n",
      "  (out1): Linear (6 -> 26)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#input_dim  = len(continuous_norm_oau_data[0])\n",
    "input_dim  = len(norm_oau_data[0])\n",
    "\n",
    "class simple_endecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_endecoder,self).__init__()\n",
    "        #linear\n",
    "        self.in1  = nn.Linear(input_dim,int(input_dim/4))\n",
    "\n",
    "        self.out1 = nn.Linear(int(input_dim/4),input_dim)            \n",
    "    def forward(self,x):\n",
    "        x = self.in1(x)\n",
    "\n",
    "        x = self.out1(x)\n",
    "        return x\n",
    "    \n",
    "    def encode_data(self,x):\n",
    "        x = self.in1(x)\n",
    "        return x\n",
    "    \n",
    "net       = simple_endecoder()\n",
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.029\n",
      "[2] loss: 0.029\n",
      "[1] loss: 0.029\n",
      "[2] loss: 0.029\n",
      "[3] loss: 0.029\n",
      "[4] loss: 0.029\n",
      "[5] loss: 0.029\n",
      "[6] loss: 0.029\n",
      "[7] loss: 0.029\n",
      "[8] loss: 0.029\n",
      "[9] loss: 0.029\n",
      "[10] loss: 0.029\n",
      "[11] loss: 0.029\n",
      "[12] loss: 0.029\n",
      "[13] loss: 0.029\n",
      "[14] loss: 0.029\n",
      "[15] loss: 0.029\n",
      "[16] loss: 0.029\n",
      "[17] loss: 0.029\n",
      "[18] loss: 0.029\n",
      "[19] loss: 0.029\n",
      "[20] loss: 0.029\n",
      "[21] loss: 0.029\n",
      "[22] loss: 0.029\n",
      "[23] loss: 0.029\n",
      "[24] loss: 0.029\n",
      "[25] loss: 0.029\n",
      "[26] loss: 0.029\n",
      "[27] loss: 0.029\n",
      "[28] loss: 0.029\n",
      "[29] loss: 0.029\n",
      "[30] loss: 0.029\n",
      "[31] loss: 0.029\n",
      "[32] loss: 0.029\n",
      "[33] loss: 0.029\n",
      "[34] loss: 0.029\n",
      "[35] loss: 0.029\n",
      "[36] loss: 0.029\n",
      "[37] loss: 0.029\n",
      "[38] loss: 0.029\n",
      "[39] loss: 0.029\n",
      "[40] loss: 0.029\n",
      "[41] loss: 0.029\n",
      "[42] loss: 0.029\n",
      "[43] loss: 0.029\n",
      "[44] loss: 0.029\n",
      "[45] loss: 0.029\n",
      "[46] loss: 0.029\n",
      "[47] loss: 0.029\n",
      "[48] loss: 0.029\n",
      "[49] loss: 0.029\n",
      "[50] loss: 0.029\n",
      "[51] loss: 0.029\n",
      "[52] loss: 0.029\n",
      "[53] loss: 0.029\n",
      "[54] loss: 0.029\n",
      "[55] loss: 0.029\n",
      "[56] loss: 0.029\n",
      "[57] loss: 0.029\n",
      "[58] loss: 0.029\n",
      "[59] loss: 0.029\n",
      "[60] loss: 0.029\n",
      "[61] loss: 0.029\n",
      "[62] loss: 0.029\n",
      "[63] loss: 0.029\n",
      "[64] loss: 0.029\n",
      "[65] loss: 0.029\n",
      "[66] loss: 0.029\n",
      "[67] loss: 0.029\n",
      "[68] loss: 0.029\n",
      "[69] loss: 0.029\n",
      "[70] loss: 0.029\n",
      "[71] loss: 0.029\n",
      "[72] loss: 0.029\n",
      "[73] loss: 0.029\n",
      "[74] loss: 0.029\n",
      "[75] loss: 0.029\n",
      "[76] loss: 0.029\n",
      "[77] loss: 0.029\n",
      "[78] loss: 0.028\n",
      "[79] loss: 0.028\n",
      "[80] loss: 0.028\n",
      "[81] loss: 0.028\n",
      "[82] loss: 0.028\n",
      "[83] loss: 0.028\n",
      "[84] loss: 0.028\n",
      "[85] loss: 0.028\n",
      "[86] loss: 0.028\n",
      "[87] loss: 0.028\n",
      "[88] loss: 0.028\n",
      "[89] loss: 0.028\n",
      "[90] loss: 0.028\n",
      "[91] loss: 0.028\n",
      "[92] loss: 0.028\n",
      "[93] loss: 0.028\n",
      "[94] loss: 0.028\n",
      "[95] loss: 0.028\n",
      "[96] loss: 0.028\n",
      "[97] loss: 0.028\n",
      "[98] loss: 0.028\n",
      "[99] loss: 0.028\n",
      "[100] loss: 0.028\n",
      "[101] loss: 0.028\n",
      "[102] loss: 0.028\n",
      "[103] loss: 0.028\n",
      "[104] loss: 0.028\n",
      "[105] loss: 0.028\n",
      "[106] loss: 0.028\n",
      "[107] loss: 0.028\n",
      "[108] loss: 0.028\n",
      "[109] loss: 0.028\n",
      "[110] loss: 0.028\n",
      "[111] loss: 0.028\n",
      "[112] loss: 0.028\n",
      "[113] loss: 0.028\n",
      "[114] loss: 0.028\n",
      "[115] loss: 0.028\n",
      "[116] loss: 0.028\n",
      "[117] loss: 0.028\n",
      "[118] loss: 0.028\n",
      "[119] loss: 0.028\n",
      "[120] loss: 0.028\n",
      "[121] loss: 0.028\n",
      "[122] loss: 0.028\n",
      "[123] loss: 0.028\n",
      "[124] loss: 0.028\n",
      "[125] loss: 0.028\n",
      "[126] loss: 0.028\n",
      "[127] loss: 0.028\n",
      "[128] loss: 0.028\n",
      "[129] loss: 0.028\n",
      "[130] loss: 0.028\n",
      "[131] loss: 0.028\n",
      "[132] loss: 0.028\n",
      "[133] loss: 0.028\n",
      "[134] loss: 0.028\n",
      "[135] loss: 0.028\n",
      "[136] loss: 0.028\n",
      "[137] loss: 0.028\n",
      "[138] loss: 0.028\n",
      "[139] loss: 0.028\n",
      "[140] loss: 0.028\n",
      "[141] loss: 0.028\n",
      "[142] loss: 0.028\n",
      "[143] loss: 0.028\n",
      "[144] loss: 0.028\n",
      "[145] loss: 0.028\n",
      "[146] loss: 0.028\n",
      "[147] loss: 0.028\n",
      "[148] loss: 0.028\n",
      "[149] loss: 0.028\n",
      "[150] loss: 0.028\n",
      "[151] loss: 0.028\n",
      "[152] loss: 0.028\n",
      "[153] loss: 0.028\n",
      "[154] loss: 0.028\n",
      "[155] loss: 0.028\n",
      "[156] loss: 0.028\n",
      "[157] loss: 0.028\n",
      "[158] loss: 0.028\n",
      "[159] loss: 0.028\n",
      "[160] loss: 0.028\n",
      "[161] loss: 0.028\n",
      "[162] loss: 0.028\n",
      "[163] loss: 0.028\n",
      "[164] loss: 0.028\n",
      "[165] loss: 0.028\n",
      "[166] loss: 0.028\n",
      "[167] loss: 0.028\n",
      "[168] loss: 0.028\n",
      "[169] loss: 0.028\n",
      "[170] loss: 0.028\n",
      "[171] loss: 0.028\n",
      "[172] loss: 0.028\n",
      "[173] loss: 0.028\n",
      "[174] loss: 0.028\n",
      "[175] loss: 0.028\n",
      "[176] loss: 0.028\n",
      "[177] loss: 0.028\n",
      "[178] loss: 0.028\n",
      "[179] loss: 0.028\n",
      "[180] loss: 0.028\n",
      "[181] loss: 0.028\n",
      "[182] loss: 0.028\n",
      "[183] loss: 0.028\n",
      "[184] loss: 0.028\n",
      "[185] loss: 0.028\n",
      "[186] loss: 0.028\n",
      "[187] loss: 0.028\n",
      "[188] loss: 0.028\n",
      "[189] loss: 0.028\n",
      "[190] loss: 0.028\n",
      "[191] loss: 0.028\n",
      "[192] loss: 0.028\n",
      "[193] loss: 0.028\n",
      "[194] loss: 0.028\n",
      "[195] loss: 0.028\n",
      "[196] loss: 0.028\n",
      "[197] loss: 0.028\n",
      "[198] loss: 0.028\n",
      "[199] loss: 0.028\n",
      "[200] loss: 0.028\n",
      "[201] loss: 0.028\n",
      "[202] loss: 0.028\n",
      "[203] loss: 0.028\n",
      "[204] loss: 0.028\n",
      "[205] loss: 0.028\n",
      "[206] loss: 0.028\n",
      "[207] loss: 0.028\n",
      "[208] loss: 0.028\n",
      "[209] loss: 0.028\n",
      "[210] loss: 0.028\n",
      "[211] loss: 0.028\n",
      "[212] loss: 0.028\n",
      "[213] loss: 0.028\n",
      "[214] loss: 0.028\n",
      "[215] loss: 0.028\n",
      "[216] loss: 0.028\n",
      "[217] loss: 0.028\n",
      "[218] loss: 0.028\n",
      "[219] loss: 0.028\n",
      "[220] loss: 0.028\n",
      "[221] loss: 0.028\n",
      "[222] loss: 0.027\n",
      "[223] loss: 0.027\n",
      "[224] loss: 0.027\n",
      "[225] loss: 0.027\n",
      "[226] loss: 0.027\n",
      "[227] loss: 0.027\n",
      "[228] loss: 0.027\n",
      "[229] loss: 0.027\n",
      "[230] loss: 0.027\n",
      "[231] loss: 0.027\n",
      "[232] loss: 0.027\n",
      "[233] loss: 0.027\n",
      "[234] loss: 0.027\n",
      "[235] loss: 0.027\n",
      "[236] loss: 0.027\n",
      "[237] loss: 0.027\n",
      "[238] loss: 0.027\n",
      "[239] loss: 0.027\n",
      "[240] loss: 0.027\n",
      "[241] loss: 0.027\n",
      "[242] loss: 0.027\n",
      "[243] loss: 0.027\n",
      "[244] loss: 0.027\n",
      "[245] loss: 0.027\n",
      "[246] loss: 0.027\n",
      "[247] loss: 0.027\n",
      "[248] loss: 0.027\n",
      "[249] loss: 0.027\n",
      "[250] loss: 0.027\n",
      "[251] loss: 0.027\n",
      "[252] loss: 0.027\n",
      "[253] loss: 0.027\n",
      "[254] loss: 0.027\n",
      "[255] loss: 0.027\n",
      "[256] loss: 0.027\n",
      "[257] loss: 0.027\n",
      "[258] loss: 0.027\n",
      "[259] loss: 0.027\n",
      "[260] loss: 0.027\n",
      "[261] loss: 0.027\n",
      "[262] loss: 0.027\n",
      "[263] loss: 0.027\n",
      "[264] loss: 0.027\n",
      "[265] loss: 0.027\n",
      "[266] loss: 0.027\n",
      "[267] loss: 0.027\n",
      "[268] loss: 0.027\n",
      "[269] loss: 0.027\n",
      "[270] loss: 0.027\n",
      "[271] loss: 0.027\n",
      "[272] loss: 0.027\n",
      "[273] loss: 0.027\n",
      "[274] loss: 0.027\n",
      "[275] loss: 0.027\n",
      "[276] loss: 0.027\n",
      "[277] loss: 0.027\n",
      "[278] loss: 0.027\n",
      "[279] loss: 0.027\n",
      "[280] loss: 0.027\n",
      "[281] loss: 0.027\n",
      "[282] loss: 0.027\n",
      "[283] loss: 0.027\n",
      "[284] loss: 0.027\n",
      "[285] loss: 0.027\n",
      "[286] loss: 0.027\n",
      "[287] loss: 0.027\n",
      "[288] loss: 0.027\n",
      "[289] loss: 0.027\n",
      "[290] loss: 0.027\n",
      "[291] loss: 0.027\n",
      "[292] loss: 0.027\n",
      "[293] loss: 0.027\n",
      "[294] loss: 0.027\n",
      "[295] loss: 0.027\n",
      "[296] loss: 0.027\n",
      "[297] loss: 0.027\n",
      "[298] loss: 0.027\n",
      "[299] loss: 0.027\n",
      "[300] loss: 0.027\n",
      "[301] loss: 0.027\n",
      "[302] loss: 0.027\n",
      "[303] loss: 0.027\n",
      "[304] loss: 0.027\n",
      "[305] loss: 0.027\n",
      "[306] loss: 0.027\n",
      "[307] loss: 0.027\n",
      "[308] loss: 0.027\n",
      "[309] loss: 0.027\n",
      "[310] loss: 0.027\n",
      "[311] loss: 0.027\n",
      "[312] loss: 0.027\n",
      "[313] loss: 0.027\n",
      "[314] loss: 0.027\n",
      "[315] loss: 0.027\n",
      "[316] loss: 0.027\n",
      "[317] loss: 0.027\n",
      "[318] loss: 0.027\n",
      "[319] loss: 0.027\n",
      "[320] loss: 0.027\n",
      "[321] loss: 0.027\n",
      "[322] loss: 0.027\n",
      "[323] loss: 0.027\n",
      "[324] loss: 0.027\n",
      "[325] loss: 0.027\n",
      "[326] loss: 0.027\n",
      "[327] loss: 0.027\n",
      "[328] loss: 0.027\n",
      "[329] loss: 0.027\n",
      "[330] loss: 0.027\n",
      "[331] loss: 0.027\n",
      "[332] loss: 0.027\n",
      "[333] loss: 0.027\n",
      "[334] loss: 0.027\n",
      "[335] loss: 0.027\n",
      "[336] loss: 0.027\n",
      "[337] loss: 0.027\n",
      "[338] loss: 0.027\n",
      "[339] loss: 0.027\n",
      "[340] loss: 0.027\n",
      "[341] loss: 0.027\n",
      "[342] loss: 0.027\n",
      "[343] loss: 0.027\n",
      "[344] loss: 0.027\n",
      "[345] loss: 0.027\n",
      "[346] loss: 0.027\n",
      "[347] loss: 0.027\n",
      "[348] loss: 0.027\n",
      "[349] loss: 0.027\n",
      "[350] loss: 0.027\n",
      "[351] loss: 0.027\n",
      "[352] loss: 0.027\n",
      "[353] loss: 0.027\n",
      "[354] loss: 0.027\n",
      "[355] loss: 0.027\n",
      "[356] loss: 0.027\n",
      "[357] loss: 0.027\n",
      "[358] loss: 0.027\n",
      "[359] loss: 0.027\n",
      "[360] loss: 0.027\n",
      "[361] loss: 0.027\n",
      "[362] loss: 0.027\n",
      "[363] loss: 0.027\n",
      "[364] loss: 0.026\n",
      "[365] loss: 0.026\n",
      "[366] loss: 0.026\n",
      "[367] loss: 0.026\n",
      "[368] loss: 0.026\n",
      "[369] loss: 0.026\n",
      "[370] loss: 0.026\n",
      "[371] loss: 0.026\n",
      "[372] loss: 0.026\n",
      "[373] loss: 0.026\n",
      "[374] loss: 0.026\n",
      "[375] loss: 0.026\n",
      "[376] loss: 0.026\n",
      "[377] loss: 0.026\n",
      "[378] loss: 0.026\n",
      "[379] loss: 0.026\n",
      "[380] loss: 0.026\n",
      "[381] loss: 0.026\n",
      "[382] loss: 0.026\n",
      "[383] loss: 0.026\n",
      "[384] loss: 0.026\n",
      "[385] loss: 0.026\n",
      "[386] loss: 0.026\n",
      "[387] loss: 0.026\n",
      "[388] loss: 0.026\n",
      "[389] loss: 0.026\n",
      "[390] loss: 0.026\n",
      "[391] loss: 0.026\n",
      "[392] loss: 0.026\n",
      "[393] loss: 0.026\n",
      "[394] loss: 0.026\n",
      "[395] loss: 0.026\n",
      "[396] loss: 0.026\n",
      "[397] loss: 0.026\n",
      "[398] loss: 0.026\n",
      "[399] loss: 0.026\n",
      "[400] loss: 0.026\n",
      "[401] loss: 0.026\n",
      "[402] loss: 0.026\n",
      "[403] loss: 0.026\n",
      "[404] loss: 0.026\n",
      "[405] loss: 0.026\n",
      "[406] loss: 0.026\n",
      "[407] loss: 0.026\n",
      "[408] loss: 0.026\n",
      "[409] loss: 0.026\n",
      "[410] loss: 0.026\n",
      "[411] loss: 0.026\n",
      "[412] loss: 0.026\n",
      "[413] loss: 0.026\n",
      "[414] loss: 0.026\n",
      "[415] loss: 0.026\n",
      "[416] loss: 0.026\n",
      "[417] loss: 0.026\n",
      "[418] loss: 0.026\n",
      "[419] loss: 0.026\n",
      "[420] loss: 0.026\n",
      "[421] loss: 0.026\n",
      "[422] loss: 0.026\n",
      "[423] loss: 0.026\n",
      "[424] loss: 0.026\n",
      "[425] loss: 0.026\n",
      "[426] loss: 0.026\n",
      "[427] loss: 0.026\n",
      "[428] loss: 0.026\n",
      "[429] loss: 0.026\n",
      "[430] loss: 0.026\n",
      "[431] loss: 0.026\n",
      "[432] loss: 0.026\n",
      "[433] loss: 0.026\n",
      "[434] loss: 0.026\n",
      "[435] loss: 0.026\n",
      "[436] loss: 0.026\n",
      "[437] loss: 0.026\n",
      "[438] loss: 0.026\n",
      "[439] loss: 0.026\n",
      "[440] loss: 0.026\n",
      "[441] loss: 0.026\n",
      "[442] loss: 0.026\n",
      "[443] loss: 0.026\n",
      "[444] loss: 0.026\n",
      "[445] loss: 0.026\n",
      "[446] loss: 0.026\n",
      "[447] loss: 0.026\n",
      "[448] loss: 0.026\n",
      "[449] loss: 0.026\n",
      "[450] loss: 0.026\n",
      "[451] loss: 0.026\n",
      "[452] loss: 0.026\n",
      "[453] loss: 0.026\n",
      "[454] loss: 0.026\n",
      "[455] loss: 0.026\n",
      "[456] loss: 0.026\n",
      "[457] loss: 0.026\n",
      "[458] loss: 0.026\n",
      "[459] loss: 0.026\n",
      "[460] loss: 0.026\n",
      "[461] loss: 0.026\n",
      "[462] loss: 0.026\n",
      "[463] loss: 0.026\n",
      "[464] loss: 0.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[465] loss: 0.026\n",
      "[466] loss: 0.026\n",
      "[467] loss: 0.026\n",
      "[468] loss: 0.026\n",
      "[469] loss: 0.026\n",
      "[470] loss: 0.026\n",
      "[471] loss: 0.026\n",
      "[472] loss: 0.026\n",
      "[473] loss: 0.026\n",
      "[474] loss: 0.026\n",
      "[475] loss: 0.026\n",
      "[476] loss: 0.026\n",
      "[477] loss: 0.026\n",
      "[478] loss: 0.026\n",
      "[479] loss: 0.026\n",
      "[480] loss: 0.026\n",
      "[481] loss: 0.026\n",
      "[482] loss: 0.026\n",
      "[483] loss: 0.026\n",
      "[484] loss: 0.026\n",
      "[485] loss: 0.026\n",
      "[486] loss: 0.026\n",
      "[487] loss: 0.026\n",
      "[488] loss: 0.026\n",
      "[489] loss: 0.026\n",
      "[490] loss: 0.026\n",
      "[491] loss: 0.026\n",
      "[492] loss: 0.026\n",
      "[493] loss: 0.026\n",
      "[494] loss: 0.026\n",
      "[495] loss: 0.026\n",
      "[496] loss: 0.026\n",
      "[497] loss: 0.026\n",
      "[498] loss: 0.026\n",
      "[499] loss: 0.026\n",
      "[500] loss: 0.026\n",
      "[501] loss: 0.026\n",
      "[502] loss: 0.026\n",
      "[503] loss: 0.026\n",
      "[504] loss: 0.025\n",
      "[505] loss: 0.025\n",
      "[506] loss: 0.025\n",
      "[507] loss: 0.025\n",
      "[508] loss: 0.025\n",
      "[509] loss: 0.025\n",
      "[510] loss: 0.025\n",
      "[511] loss: 0.025\n",
      "[512] loss: 0.025\n",
      "[513] loss: 0.025\n",
      "[514] loss: 0.025\n",
      "[515] loss: 0.025\n",
      "[516] loss: 0.025\n",
      "[517] loss: 0.025\n",
      "[518] loss: 0.025\n",
      "[519] loss: 0.025\n",
      "[520] loss: 0.025\n",
      "[521] loss: 0.025\n",
      "[522] loss: 0.025\n",
      "[523] loss: 0.025\n",
      "[524] loss: 0.025\n",
      "[525] loss: 0.025\n",
      "[526] loss: 0.025\n",
      "[527] loss: 0.025\n",
      "[528] loss: 0.025\n",
      "[529] loss: 0.025\n",
      "[530] loss: 0.025\n",
      "[531] loss: 0.025\n",
      "[532] loss: 0.025\n",
      "[533] loss: 0.025\n",
      "[534] loss: 0.025\n",
      "[535] loss: 0.025\n",
      "[536] loss: 0.025\n",
      "[537] loss: 0.025\n",
      "[538] loss: 0.025\n",
      "[539] loss: 0.025\n",
      "[540] loss: 0.025\n",
      "[541] loss: 0.025\n",
      "[542] loss: 0.025\n",
      "[543] loss: 0.025\n",
      "[544] loss: 0.025\n",
      "[545] loss: 0.025\n",
      "[546] loss: 0.025\n",
      "[547] loss: 0.025\n",
      "[548] loss: 0.025\n",
      "[549] loss: 0.025\n",
      "[550] loss: 0.025\n",
      "[551] loss: 0.025\n",
      "[552] loss: 0.025\n",
      "[553] loss: 0.025\n",
      "[554] loss: 0.025\n",
      "[555] loss: 0.025\n",
      "[556] loss: 0.025\n",
      "[557] loss: 0.025\n",
      "[558] loss: 0.025\n",
      "[559] loss: 0.025\n",
      "[560] loss: 0.025\n",
      "[561] loss: 0.025\n",
      "[562] loss: 0.025\n",
      "[563] loss: 0.025\n",
      "[564] loss: 0.025\n",
      "[565] loss: 0.025\n",
      "[566] loss: 0.025\n",
      "[567] loss: 0.025\n",
      "[568] loss: 0.025\n",
      "[569] loss: 0.025\n",
      "[570] loss: 0.025\n",
      "[571] loss: 0.025\n",
      "[572] loss: 0.025\n",
      "[573] loss: 0.025\n",
      "[574] loss: 0.025\n",
      "[575] loss: 0.025\n",
      "[576] loss: 0.025\n",
      "[577] loss: 0.025\n",
      "[578] loss: 0.025\n",
      "[579] loss: 0.025\n",
      "[580] loss: 0.025\n",
      "[581] loss: 0.025\n",
      "[582] loss: 0.025\n",
      "[583] loss: 0.025\n",
      "[584] loss: 0.025\n",
      "[585] loss: 0.025\n",
      "[586] loss: 0.025\n",
      "[587] loss: 0.025\n",
      "[588] loss: 0.025\n",
      "[589] loss: 0.025\n",
      "[590] loss: 0.025\n",
      "[591] loss: 0.025\n",
      "[592] loss: 0.025\n",
      "[593] loss: 0.025\n",
      "[594] loss: 0.025\n",
      "[595] loss: 0.025\n",
      "[596] loss: 0.025\n",
      "[597] loss: 0.025\n",
      "[598] loss: 0.025\n",
      "[599] loss: 0.025\n",
      "[600] loss: 0.025\n",
      "[601] loss: 0.025\n",
      "[602] loss: 0.025\n",
      "[603] loss: 0.025\n",
      "[604] loss: 0.025\n",
      "[605] loss: 0.025\n",
      "[606] loss: 0.025\n",
      "[607] loss: 0.025\n",
      "[608] loss: 0.025\n",
      "[609] loss: 0.025\n",
      "[610] loss: 0.025\n",
      "[611] loss: 0.025\n",
      "[612] loss: 0.025\n",
      "[613] loss: 0.025\n",
      "[614] loss: 0.025\n",
      "[615] loss: 0.025\n",
      "[616] loss: 0.025\n",
      "[617] loss: 0.025\n",
      "[618] loss: 0.025\n",
      "[619] loss: 0.025\n",
      "[620] loss: 0.025\n",
      "[621] loss: 0.025\n",
      "[622] loss: 0.025\n",
      "[623] loss: 0.025\n",
      "[624] loss: 0.025\n",
      "[625] loss: 0.025\n",
      "[626] loss: 0.025\n",
      "[627] loss: 0.025\n",
      "[628] loss: 0.025\n",
      "[629] loss: 0.025\n",
      "[630] loss: 0.025\n",
      "[631] loss: 0.025\n",
      "[632] loss: 0.025\n",
      "[633] loss: 0.025\n",
      "[634] loss: 0.025\n",
      "[635] loss: 0.025\n",
      "[636] loss: 0.025\n",
      "[637] loss: 0.025\n",
      "[638] loss: 0.025\n",
      "[639] loss: 0.025\n",
      "[640] loss: 0.024\n",
      "[641] loss: 0.024\n",
      "[642] loss: 0.024\n",
      "[643] loss: 0.024\n",
      "[644] loss: 0.024\n",
      "[645] loss: 0.024\n",
      "[646] loss: 0.024\n",
      "[647] loss: 0.024\n",
      "[648] loss: 0.024\n",
      "[649] loss: 0.024\n",
      "[650] loss: 0.024\n",
      "[651] loss: 0.024\n",
      "[652] loss: 0.024\n",
      "[653] loss: 0.024\n",
      "[654] loss: 0.024\n",
      "[655] loss: 0.024\n",
      "[656] loss: 0.024\n",
      "[657] loss: 0.024\n",
      "[658] loss: 0.024\n",
      "[659] loss: 0.024\n",
      "[660] loss: 0.024\n",
      "[661] loss: 0.024\n",
      "[662] loss: 0.024\n",
      "[663] loss: 0.024\n",
      "[664] loss: 0.024\n",
      "[665] loss: 0.024\n",
      "[666] loss: 0.024\n",
      "[667] loss: 0.024\n",
      "[668] loss: 0.024\n",
      "[669] loss: 0.024\n",
      "[670] loss: 0.024\n",
      "[671] loss: 0.024\n",
      "[672] loss: 0.024\n",
      "[673] loss: 0.024\n",
      "[674] loss: 0.024\n",
      "[675] loss: 0.024\n",
      "[676] loss: 0.024\n",
      "[677] loss: 0.024\n",
      "[678] loss: 0.024\n",
      "[679] loss: 0.024\n",
      "[680] loss: 0.024\n",
      "[681] loss: 0.024\n",
      "[682] loss: 0.024\n",
      "[683] loss: 0.024\n",
      "[684] loss: 0.024\n",
      "[685] loss: 0.024\n",
      "[686] loss: 0.024\n",
      "[687] loss: 0.024\n",
      "[688] loss: 0.024\n",
      "[689] loss: 0.024\n",
      "[690] loss: 0.024\n",
      "[691] loss: 0.024\n",
      "[692] loss: 0.024\n",
      "[693] loss: 0.024\n",
      "[694] loss: 0.024\n",
      "[695] loss: 0.024\n",
      "[696] loss: 0.024\n",
      "[697] loss: 0.024\n",
      "[698] loss: 0.024\n",
      "[699] loss: 0.024\n",
      "[700] loss: 0.024\n",
      "[701] loss: 0.024\n",
      "[702] loss: 0.024\n",
      "[703] loss: 0.024\n",
      "[704] loss: 0.024\n",
      "[705] loss: 0.024\n",
      "[706] loss: 0.024\n",
      "[707] loss: 0.024\n",
      "[708] loss: 0.024\n",
      "[709] loss: 0.024\n",
      "[710] loss: 0.024\n",
      "[711] loss: 0.024\n",
      "[712] loss: 0.024\n",
      "[713] loss: 0.024\n",
      "[714] loss: 0.024\n",
      "[715] loss: 0.024\n",
      "[716] loss: 0.024\n",
      "[717] loss: 0.024\n",
      "[718] loss: 0.024\n",
      "[719] loss: 0.024\n",
      "[720] loss: 0.024\n",
      "[721] loss: 0.024\n",
      "[722] loss: 0.024\n",
      "[723] loss: 0.024\n",
      "[724] loss: 0.024\n",
      "[725] loss: 0.024\n",
      "[726] loss: 0.024\n",
      "[727] loss: 0.024\n",
      "[728] loss: 0.024\n",
      "[729] loss: 0.024\n",
      "[730] loss: 0.024\n",
      "[731] loss: 0.024\n",
      "[732] loss: 0.024\n",
      "[733] loss: 0.024\n",
      "[734] loss: 0.024\n",
      "[735] loss: 0.024\n",
      "[736] loss: 0.024\n",
      "[737] loss: 0.024\n",
      "[738] loss: 0.024\n",
      "[739] loss: 0.024\n",
      "[740] loss: 0.024\n",
      "[741] loss: 0.024\n",
      "[742] loss: 0.024\n",
      "[743] loss: 0.024\n",
      "[744] loss: 0.024\n",
      "[745] loss: 0.024\n",
      "[746] loss: 0.024\n",
      "[747] loss: 0.024\n",
      "[748] loss: 0.024\n",
      "[749] loss: 0.024\n",
      "[750] loss: 0.024\n",
      "[751] loss: 0.024\n",
      "[752] loss: 0.024\n",
      "[753] loss: 0.024\n",
      "[754] loss: 0.024\n",
      "[755] loss: 0.024\n",
      "[756] loss: 0.024\n",
      "[757] loss: 0.024\n",
      "[758] loss: 0.024\n",
      "[759] loss: 0.024\n",
      "[760] loss: 0.024\n",
      "[761] loss: 0.024\n",
      "[762] loss: 0.024\n",
      "[763] loss: 0.024\n",
      "[764] loss: 0.024\n",
      "[765] loss: 0.024\n",
      "[766] loss: 0.024\n",
      "[767] loss: 0.024\n",
      "[768] loss: 0.024\n",
      "[769] loss: 0.024\n",
      "[770] loss: 0.024\n",
      "[771] loss: 0.024\n",
      "[772] loss: 0.024\n",
      "[773] loss: 0.024\n",
      "[774] loss: 0.024\n",
      "[775] loss: 0.024\n",
      "[776] loss: 0.024\n",
      "[777] loss: 0.023\n",
      "[778] loss: 0.023\n",
      "[779] loss: 0.023\n",
      "[780] loss: 0.023\n",
      "[781] loss: 0.023\n",
      "[782] loss: 0.023\n",
      "[783] loss: 0.023\n",
      "[784] loss: 0.023\n",
      "[785] loss: 0.023\n",
      "[786] loss: 0.023\n",
      "[787] loss: 0.023\n",
      "[788] loss: 0.023\n",
      "[789] loss: 0.023\n",
      "[790] loss: 0.023\n",
      "[791] loss: 0.023\n",
      "[792] loss: 0.023\n",
      "[793] loss: 0.023\n",
      "[794] loss: 0.023\n",
      "[795] loss: 0.023\n",
      "[796] loss: 0.023\n",
      "[797] loss: 0.023\n",
      "[798] loss: 0.023\n",
      "[799] loss: 0.023\n",
      "[800] loss: 0.023\n",
      "[801] loss: 0.023\n",
      "[802] loss: 0.023\n",
      "[803] loss: 0.023\n",
      "[804] loss: 0.023\n",
      "[805] loss: 0.023\n",
      "[806] loss: 0.023\n",
      "[807] loss: 0.023\n",
      "[808] loss: 0.023\n",
      "[809] loss: 0.023\n",
      "[810] loss: 0.023\n",
      "[811] loss: 0.023\n",
      "[812] loss: 0.023\n",
      "[813] loss: 0.023\n",
      "[814] loss: 0.023\n",
      "[815] loss: 0.023\n",
      "[816] loss: 0.023\n",
      "[817] loss: 0.023\n",
      "[818] loss: 0.023\n",
      "[819] loss: 0.023\n",
      "[820] loss: 0.023\n",
      "[821] loss: 0.023\n",
      "[822] loss: 0.023\n",
      "[823] loss: 0.023\n",
      "[824] loss: 0.023\n",
      "[825] loss: 0.023\n",
      "[826] loss: 0.023\n",
      "[827] loss: 0.023\n",
      "[828] loss: 0.023\n",
      "[829] loss: 0.023\n",
      "[830] loss: 0.023\n",
      "[831] loss: 0.023\n",
      "[832] loss: 0.023\n",
      "[833] loss: 0.023\n",
      "[834] loss: 0.023\n",
      "[835] loss: 0.023\n",
      "[836] loss: 0.023\n",
      "[837] loss: 0.023\n",
      "[838] loss: 0.023\n",
      "[839] loss: 0.023\n",
      "[840] loss: 0.023\n",
      "[841] loss: 0.023\n",
      "[842] loss: 0.023\n",
      "[843] loss: 0.023\n",
      "[844] loss: 0.023\n",
      "[845] loss: 0.023\n",
      "[846] loss: 0.023\n",
      "[847] loss: 0.023\n",
      "[848] loss: 0.023\n",
      "[849] loss: 0.023\n",
      "[850] loss: 0.023\n",
      "[851] loss: 0.023\n",
      "[852] loss: 0.023\n",
      "[853] loss: 0.023\n",
      "[854] loss: 0.023\n",
      "[855] loss: 0.023\n",
      "[856] loss: 0.023\n",
      "[857] loss: 0.023\n",
      "[858] loss: 0.023\n",
      "[859] loss: 0.023\n",
      "[860] loss: 0.023\n",
      "[861] loss: 0.023\n",
      "[862] loss: 0.023\n",
      "[863] loss: 0.023\n",
      "[864] loss: 0.023\n",
      "[865] loss: 0.023\n",
      "[866] loss: 0.023\n",
      "[867] loss: 0.023\n",
      "[868] loss: 0.023\n",
      "[869] loss: 0.023\n",
      "[870] loss: 0.023\n",
      "[871] loss: 0.023\n",
      "[872] loss: 0.023\n",
      "[873] loss: 0.023\n",
      "[874] loss: 0.023\n",
      "[875] loss: 0.023\n",
      "[876] loss: 0.023\n",
      "[877] loss: 0.023\n",
      "[878] loss: 0.023\n",
      "[879] loss: 0.023\n",
      "[880] loss: 0.023\n",
      "[881] loss: 0.023\n",
      "[882] loss: 0.023\n",
      "[883] loss: 0.023\n",
      "[884] loss: 0.023\n",
      "[885] loss: 0.023\n",
      "[886] loss: 0.023\n",
      "[887] loss: 0.023\n",
      "[888] loss: 0.023\n",
      "[889] loss: 0.023\n",
      "[890] loss: 0.023\n",
      "[891] loss: 0.023\n",
      "[892] loss: 0.023\n",
      "[893] loss: 0.023\n",
      "[894] loss: 0.023\n",
      "[895] loss: 0.023\n",
      "[896] loss: 0.023\n",
      "[897] loss: 0.023\n",
      "[898] loss: 0.023\n",
      "[899] loss: 0.023\n",
      "[900] loss: 0.023\n",
      "[901] loss: 0.023\n",
      "[902] loss: 0.023\n",
      "[903] loss: 0.023\n",
      "[904] loss: 0.023\n",
      "[905] loss: 0.023\n",
      "[906] loss: 0.023\n",
      "[907] loss: 0.023\n",
      "[908] loss: 0.023\n",
      "[909] loss: 0.023\n",
      "[910] loss: 0.023\n",
      "[911] loss: 0.023\n",
      "[912] loss: 0.022\n",
      "[913] loss: 0.022\n",
      "[914] loss: 0.022\n",
      "[915] loss: 0.022\n",
      "[916] loss: 0.022\n",
      "[917] loss: 0.022\n",
      "[918] loss: 0.022\n",
      "[919] loss: 0.022\n",
      "[920] loss: 0.022\n",
      "[921] loss: 0.022\n",
      "[922] loss: 0.022\n",
      "[923] loss: 0.022\n",
      "[924] loss: 0.022\n",
      "[925] loss: 0.022\n",
      "[926] loss: 0.022\n",
      "[927] loss: 0.022\n",
      "[928] loss: 0.022\n",
      "[929] loss: 0.022\n",
      "[930] loss: 0.022\n",
      "[931] loss: 0.022\n",
      "[932] loss: 0.022\n",
      "[933] loss: 0.022\n",
      "[934] loss: 0.022\n",
      "[935] loss: 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[936] loss: 0.022\n",
      "[937] loss: 0.022\n",
      "[938] loss: 0.022\n",
      "[939] loss: 0.022\n",
      "[940] loss: 0.022\n",
      "[941] loss: 0.022\n",
      "[942] loss: 0.022\n",
      "[943] loss: 0.022\n",
      "[944] loss: 0.022\n",
      "[945] loss: 0.022\n",
      "[946] loss: 0.022\n",
      "[947] loss: 0.022\n",
      "[948] loss: 0.022\n",
      "[949] loss: 0.022\n",
      "[950] loss: 0.022\n",
      "[951] loss: 0.022\n",
      "[952] loss: 0.022\n",
      "[953] loss: 0.022\n",
      "[954] loss: 0.022\n",
      "[955] loss: 0.022\n",
      "[956] loss: 0.022\n",
      "[957] loss: 0.022\n",
      "[958] loss: 0.022\n",
      "[959] loss: 0.022\n",
      "[960] loss: 0.022\n",
      "[961] loss: 0.022\n",
      "[962] loss: 0.022\n",
      "[963] loss: 0.022\n",
      "[964] loss: 0.022\n",
      "[965] loss: 0.022\n",
      "[966] loss: 0.022\n",
      "[967] loss: 0.022\n",
      "[968] loss: 0.022\n",
      "[969] loss: 0.022\n",
      "[970] loss: 0.022\n",
      "[971] loss: 0.022\n",
      "[972] loss: 0.022\n",
      "[973] loss: 0.022\n",
      "[974] loss: 0.022\n",
      "[975] loss: 0.022\n",
      "[976] loss: 0.022\n",
      "[977] loss: 0.022\n",
      "[978] loss: 0.022\n",
      "[979] loss: 0.022\n",
      "[980] loss: 0.022\n",
      "[981] loss: 0.022\n",
      "[982] loss: 0.022\n",
      "[983] loss: 0.022\n",
      "[984] loss: 0.022\n",
      "[985] loss: 0.022\n",
      "[986] loss: 0.022\n",
      "[987] loss: 0.022\n",
      "[988] loss: 0.022\n",
      "[989] loss: 0.022\n",
      "[990] loss: 0.022\n",
      "[991] loss: 0.022\n",
      "[992] loss: 0.022\n",
      "[993] loss: 0.022\n",
      "[994] loss: 0.022\n",
      "[995] loss: 0.022\n",
      "[996] loss: 0.022\n",
      "[997] loss: 0.022\n",
      "[998] loss: 0.022\n",
      "[999] loss: 0.022\n",
      "[1000] loss: 0.022\n",
      "[1001] loss: 0.022\n",
      "[1002] loss: 0.022\n",
      "[1003] loss: 0.022\n",
      "[1004] loss: 0.022\n",
      "[1005] loss: 0.022\n",
      "[1006] loss: 0.022\n",
      "[1007] loss: 0.022\n",
      "[1008] loss: 0.022\n",
      "[1009] loss: 0.022\n",
      "[1010] loss: 0.022\n",
      "[1011] loss: 0.022\n",
      "[1012] loss: 0.022\n",
      "[1013] loss: 0.022\n",
      "[1014] loss: 0.022\n",
      "[1015] loss: 0.022\n",
      "[1016] loss: 0.022\n",
      "[1017] loss: 0.022\n",
      "[1018] loss: 0.022\n",
      "[1019] loss: 0.022\n",
      "[1020] loss: 0.022\n",
      "[1021] loss: 0.022\n",
      "[1022] loss: 0.022\n",
      "[1023] loss: 0.022\n",
      "[1024] loss: 0.022\n",
      "[1025] loss: 0.022\n",
      "[1026] loss: 0.022\n",
      "[1027] loss: 0.022\n",
      "[1028] loss: 0.022\n",
      "[1029] loss: 0.022\n",
      "[1030] loss: 0.022\n",
      "[1031] loss: 0.022\n",
      "[1032] loss: 0.022\n",
      "[1033] loss: 0.022\n",
      "[1034] loss: 0.022\n",
      "[1035] loss: 0.022\n",
      "[1036] loss: 0.022\n",
      "[1037] loss: 0.022\n",
      "[1038] loss: 0.022\n",
      "[1039] loss: 0.022\n",
      "[1040] loss: 0.022\n",
      "[1041] loss: 0.022\n",
      "[1042] loss: 0.022\n",
      "[1043] loss: 0.022\n",
      "[1044] loss: 0.022\n",
      "[1045] loss: 0.022\n",
      "[1046] loss: 0.022\n",
      "[1047] loss: 0.021\n",
      "[1048] loss: 0.021\n",
      "[1049] loss: 0.021\n",
      "[1050] loss: 0.021\n",
      "[1051] loss: 0.021\n",
      "[1052] loss: 0.021\n",
      "[1053] loss: 0.021\n",
      "[1054] loss: 0.021\n",
      "[1055] loss: 0.021\n",
      "[1056] loss: 0.021\n",
      "[1057] loss: 0.021\n",
      "[1058] loss: 0.021\n",
      "[1059] loss: 0.021\n",
      "[1060] loss: 0.021\n",
      "[1061] loss: 0.021\n",
      "[1062] loss: 0.021\n",
      "[1063] loss: 0.021\n",
      "[1064] loss: 0.021\n",
      "[1065] loss: 0.021\n",
      "[1066] loss: 0.021\n",
      "[1067] loss: 0.021\n",
      "[1068] loss: 0.021\n",
      "[1069] loss: 0.021\n",
      "[1070] loss: 0.021\n",
      "[1071] loss: 0.021\n",
      "[1072] loss: 0.021\n",
      "[1073] loss: 0.021\n",
      "[1074] loss: 0.021\n",
      "[1075] loss: 0.021\n",
      "[1076] loss: 0.021\n",
      "[1077] loss: 0.021\n",
      "[1078] loss: 0.021\n",
      "[1079] loss: 0.021\n",
      "[1080] loss: 0.021\n",
      "[1081] loss: 0.021\n",
      "[1082] loss: 0.021\n",
      "[1083] loss: 0.021\n",
      "[1084] loss: 0.021\n",
      "[1085] loss: 0.021\n",
      "[1086] loss: 0.021\n",
      "[1087] loss: 0.021\n",
      "[1088] loss: 0.021\n",
      "[1089] loss: 0.021\n",
      "[1090] loss: 0.021\n",
      "[1091] loss: 0.021\n",
      "[1092] loss: 0.021\n",
      "[1093] loss: 0.021\n",
      "[1094] loss: 0.021\n",
      "[1095] loss: 0.021\n",
      "[1096] loss: 0.021\n",
      "[1097] loss: 0.021\n",
      "[1098] loss: 0.021\n",
      "[1099] loss: 0.021\n",
      "[1100] loss: 0.021\n",
      "[1101] loss: 0.021\n",
      "[1102] loss: 0.021\n",
      "[1103] loss: 0.021\n",
      "[1104] loss: 0.021\n",
      "[1105] loss: 0.021\n",
      "[1106] loss: 0.021\n",
      "[1107] loss: 0.021\n",
      "[1108] loss: 0.021\n",
      "[1109] loss: 0.021\n",
      "[1110] loss: 0.021\n",
      "[1111] loss: 0.021\n",
      "[1112] loss: 0.021\n",
      "[1113] loss: 0.021\n",
      "[1114] loss: 0.021\n",
      "[1115] loss: 0.021\n",
      "[1116] loss: 0.021\n",
      "[1117] loss: 0.021\n",
      "[1118] loss: 0.021\n",
      "[1119] loss: 0.021\n",
      "[1120] loss: 0.021\n",
      "[1121] loss: 0.021\n",
      "[1122] loss: 0.021\n",
      "[1123] loss: 0.021\n",
      "[1124] loss: 0.021\n",
      "[1125] loss: 0.021\n",
      "[1126] loss: 0.021\n",
      "[1127] loss: 0.021\n",
      "[1128] loss: 0.021\n",
      "[1129] loss: 0.021\n",
      "[1130] loss: 0.021\n",
      "[1131] loss: 0.021\n",
      "[1132] loss: 0.021\n",
      "[1133] loss: 0.021\n",
      "[1134] loss: 0.021\n",
      "[1135] loss: 0.021\n",
      "[1136] loss: 0.021\n",
      "[1137] loss: 0.021\n",
      "[1138] loss: 0.021\n",
      "[1139] loss: 0.021\n",
      "[1140] loss: 0.021\n",
      "[1141] loss: 0.021\n",
      "[1142] loss: 0.021\n",
      "[1143] loss: 0.021\n",
      "[1144] loss: 0.021\n",
      "[1145] loss: 0.021\n",
      "[1146] loss: 0.021\n",
      "[1147] loss: 0.021\n",
      "[1148] loss: 0.021\n",
      "[1149] loss: 0.021\n",
      "[1150] loss: 0.021\n",
      "[1151] loss: 0.021\n",
      "[1152] loss: 0.021\n",
      "[1153] loss: 0.021\n",
      "[1154] loss: 0.021\n",
      "[1155] loss: 0.021\n",
      "[1156] loss: 0.021\n",
      "[1157] loss: 0.021\n",
      "[1158] loss: 0.021\n",
      "[1159] loss: 0.021\n",
      "[1160] loss: 0.021\n",
      "[1161] loss: 0.021\n",
      "[1162] loss: 0.021\n",
      "[1163] loss: 0.021\n",
      "[1164] loss: 0.021\n",
      "[1165] loss: 0.021\n",
      "[1166] loss: 0.021\n",
      "[1167] loss: 0.021\n",
      "[1168] loss: 0.021\n",
      "[1169] loss: 0.021\n",
      "[1170] loss: 0.021\n",
      "[1171] loss: 0.021\n",
      "[1172] loss: 0.021\n",
      "[1173] loss: 0.021\n",
      "[1174] loss: 0.021\n",
      "[1175] loss: 0.021\n",
      "[1176] loss: 0.021\n",
      "[1177] loss: 0.021\n",
      "[1178] loss: 0.021\n",
      "[1179] loss: 0.021\n",
      "[1180] loss: 0.021\n",
      "[1181] loss: 0.021\n",
      "[1182] loss: 0.020\n",
      "[1183] loss: 0.020\n",
      "[1184] loss: 0.020\n",
      "[1185] loss: 0.020\n",
      "[1186] loss: 0.020\n",
      "[1187] loss: 0.020\n",
      "[1188] loss: 0.020\n",
      "[1189] loss: 0.020\n",
      "[1190] loss: 0.020\n",
      "[1191] loss: 0.020\n",
      "[1192] loss: 0.020\n",
      "[1193] loss: 0.020\n",
      "[1194] loss: 0.020\n",
      "[1195] loss: 0.020\n",
      "[1196] loss: 0.020\n",
      "[1197] loss: 0.020\n",
      "[1198] loss: 0.020\n",
      "[1199] loss: 0.020\n",
      "[1200] loss: 0.020\n",
      "[1201] loss: 0.020\n",
      "[1202] loss: 0.020\n",
      "[1203] loss: 0.020\n",
      "[1204] loss: 0.020\n",
      "[1205] loss: 0.020\n",
      "[1206] loss: 0.020\n",
      "[1207] loss: 0.020\n",
      "[1208] loss: 0.020\n",
      "[1209] loss: 0.020\n",
      "[1210] loss: 0.020\n",
      "[1211] loss: 0.020\n",
      "[1212] loss: 0.020\n",
      "[1213] loss: 0.020\n",
      "[1214] loss: 0.020\n",
      "[1215] loss: 0.020\n",
      "[1216] loss: 0.020\n",
      "[1217] loss: 0.020\n",
      "[1218] loss: 0.020\n",
      "[1219] loss: 0.020\n",
      "[1220] loss: 0.020\n",
      "[1221] loss: 0.020\n",
      "[1222] loss: 0.020\n",
      "[1223] loss: 0.020\n",
      "[1224] loss: 0.020\n",
      "[1225] loss: 0.020\n",
      "[1226] loss: 0.020\n",
      "[1227] loss: 0.020\n",
      "[1228] loss: 0.020\n",
      "[1229] loss: 0.020\n",
      "[1230] loss: 0.020\n",
      "[1231] loss: 0.020\n",
      "[1232] loss: 0.020\n",
      "[1233] loss: 0.020\n",
      "[1234] loss: 0.020\n",
      "[1235] loss: 0.020\n",
      "[1236] loss: 0.020\n",
      "[1237] loss: 0.020\n",
      "[1238] loss: 0.020\n",
      "[1239] loss: 0.020\n",
      "[1240] loss: 0.020\n",
      "[1241] loss: 0.020\n",
      "[1242] loss: 0.020\n",
      "[1243] loss: 0.020\n",
      "[1244] loss: 0.020\n",
      "[1245] loss: 0.020\n",
      "[1246] loss: 0.020\n",
      "[1247] loss: 0.020\n",
      "[1248] loss: 0.020\n",
      "[1249] loss: 0.020\n",
      "[1250] loss: 0.020\n",
      "[1251] loss: 0.020\n",
      "[1252] loss: 0.020\n",
      "[1253] loss: 0.020\n",
      "[1254] loss: 0.020\n",
      "[1255] loss: 0.020\n",
      "[1256] loss: 0.020\n",
      "[1257] loss: 0.020\n",
      "[1258] loss: 0.020\n",
      "[1259] loss: 0.020\n",
      "[1260] loss: 0.020\n",
      "[1261] loss: 0.020\n",
      "[1262] loss: 0.020\n",
      "[1263] loss: 0.020\n",
      "[1264] loss: 0.020\n",
      "[1265] loss: 0.020\n",
      "[1266] loss: 0.020\n",
      "[1267] loss: 0.020\n",
      "[1268] loss: 0.020\n",
      "[1269] loss: 0.020\n",
      "[1270] loss: 0.020\n",
      "[1271] loss: 0.020\n",
      "[1272] loss: 0.020\n",
      "[1273] loss: 0.020\n",
      "[1274] loss: 0.020\n",
      "[1275] loss: 0.020\n",
      "[1276] loss: 0.020\n",
      "[1277] loss: 0.020\n",
      "[1278] loss: 0.020\n",
      "[1279] loss: 0.020\n",
      "[1280] loss: 0.020\n",
      "[1281] loss: 0.020\n",
      "[1282] loss: 0.020\n",
      "[1283] loss: 0.020\n",
      "[1284] loss: 0.020\n",
      "[1285] loss: 0.020\n",
      "[1286] loss: 0.020\n",
      "[1287] loss: 0.020\n",
      "[1288] loss: 0.020\n",
      "[1289] loss: 0.020\n",
      "[1290] loss: 0.020\n",
      "[1291] loss: 0.020\n",
      "[1292] loss: 0.020\n",
      "[1293] loss: 0.020\n",
      "[1294] loss: 0.020\n",
      "[1295] loss: 0.020\n",
      "[1296] loss: 0.020\n",
      "[1297] loss: 0.020\n",
      "[1298] loss: 0.020\n",
      "[1299] loss: 0.020\n",
      "[1300] loss: 0.020\n",
      "[1301] loss: 0.020\n",
      "[1302] loss: 0.020\n",
      "[1303] loss: 0.020\n",
      "[1304] loss: 0.020\n",
      "[1305] loss: 0.020\n",
      "[1306] loss: 0.020\n",
      "[1307] loss: 0.020\n",
      "[1308] loss: 0.020\n",
      "[1309] loss: 0.020\n",
      "[1310] loss: 0.020\n",
      "[1311] loss: 0.020\n",
      "[1312] loss: 0.020\n",
      "[1313] loss: 0.020\n",
      "[1314] loss: 0.020\n",
      "[1315] loss: 0.020\n",
      "[1316] loss: 0.020\n",
      "[1317] loss: 0.020\n",
      "[1318] loss: 0.019\n",
      "[1319] loss: 0.019\n",
      "[1320] loss: 0.019\n",
      "[1321] loss: 0.019\n",
      "[1322] loss: 0.019\n",
      "[1323] loss: 0.019\n",
      "[1324] loss: 0.019\n",
      "[1325] loss: 0.019\n",
      "[1326] loss: 0.019\n",
      "[1327] loss: 0.019\n",
      "[1328] loss: 0.019\n",
      "[1329] loss: 0.019\n",
      "[1330] loss: 0.019\n",
      "[1331] loss: 0.019\n",
      "[1332] loss: 0.019\n",
      "[1333] loss: 0.019\n",
      "[1334] loss: 0.019\n",
      "[1335] loss: 0.019\n",
      "[1336] loss: 0.019\n",
      "[1337] loss: 0.019\n",
      "[1338] loss: 0.019\n",
      "[1339] loss: 0.019\n",
      "[1340] loss: 0.019\n",
      "[1341] loss: 0.019\n",
      "[1342] loss: 0.019\n",
      "[1343] loss: 0.019\n",
      "[1344] loss: 0.019\n",
      "[1345] loss: 0.019\n",
      "[1346] loss: 0.019\n",
      "[1347] loss: 0.019\n",
      "[1348] loss: 0.019\n",
      "[1349] loss: 0.019\n",
      "[1350] loss: 0.019\n",
      "[1351] loss: 0.019\n",
      "[1352] loss: 0.019\n",
      "[1353] loss: 0.019\n",
      "[1354] loss: 0.019\n",
      "[1355] loss: 0.019\n",
      "[1356] loss: 0.019\n",
      "[1357] loss: 0.019\n",
      "[1358] loss: 0.019\n",
      "[1359] loss: 0.019\n",
      "[1360] loss: 0.019\n",
      "[1361] loss: 0.019\n",
      "[1362] loss: 0.019\n",
      "[1363] loss: 0.019\n",
      "[1364] loss: 0.019\n",
      "[1365] loss: 0.019\n",
      "[1366] loss: 0.019\n",
      "[1367] loss: 0.019\n",
      "[1368] loss: 0.019\n",
      "[1369] loss: 0.019\n",
      "[1370] loss: 0.019\n",
      "[1371] loss: 0.019\n",
      "[1372] loss: 0.019\n",
      "[1373] loss: 0.019\n",
      "[1374] loss: 0.019\n",
      "[1375] loss: 0.019\n",
      "[1376] loss: 0.019\n",
      "[1377] loss: 0.019\n",
      "[1378] loss: 0.019\n",
      "[1379] loss: 0.019\n",
      "[1380] loss: 0.019\n",
      "[1381] loss: 0.019\n",
      "[1382] loss: 0.019\n",
      "[1383] loss: 0.019\n",
      "[1384] loss: 0.019\n",
      "[1385] loss: 0.019\n",
      "[1386] loss: 0.019\n",
      "[1387] loss: 0.019\n",
      "[1388] loss: 0.019\n",
      "[1389] loss: 0.019\n",
      "[1390] loss: 0.019\n",
      "[1391] loss: 0.019\n",
      "[1392] loss: 0.019\n",
      "[1393] loss: 0.019\n",
      "[1394] loss: 0.019\n",
      "[1395] loss: 0.019\n",
      "[1396] loss: 0.019\n",
      "[1397] loss: 0.019\n",
      "[1398] loss: 0.019\n",
      "[1399] loss: 0.019\n",
      "[1400] loss: 0.019\n",
      "[1401] loss: 0.019\n",
      "[1402] loss: 0.019\n",
      "[1403] loss: 0.019\n",
      "[1404] loss: 0.019\n",
      "[1405] loss: 0.019\n",
      "[1406] loss: 0.019\n",
      "[1407] loss: 0.019\n",
      "[1408] loss: 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1409] loss: 0.019\n",
      "[1410] loss: 0.019\n",
      "[1411] loss: 0.019\n",
      "[1412] loss: 0.019\n",
      "[1413] loss: 0.019\n",
      "[1414] loss: 0.019\n",
      "[1415] loss: 0.019\n",
      "[1416] loss: 0.019\n",
      "[1417] loss: 0.019\n",
      "[1418] loss: 0.019\n",
      "[1419] loss: 0.019\n",
      "[1420] loss: 0.019\n",
      "[1421] loss: 0.019\n",
      "[1422] loss: 0.019\n",
      "[1423] loss: 0.019\n",
      "[1424] loss: 0.019\n",
      "[1425] loss: 0.019\n",
      "[1426] loss: 0.019\n",
      "[1427] loss: 0.019\n",
      "[1428] loss: 0.019\n",
      "[1429] loss: 0.019\n",
      "[1430] loss: 0.019\n",
      "[1431] loss: 0.019\n",
      "[1432] loss: 0.019\n",
      "[1433] loss: 0.019\n",
      "[1434] loss: 0.019\n",
      "[1435] loss: 0.019\n",
      "[1436] loss: 0.019\n",
      "[1437] loss: 0.019\n",
      "[1438] loss: 0.019\n",
      "[1439] loss: 0.019\n",
      "[1440] loss: 0.019\n",
      "[1441] loss: 0.019\n",
      "[1442] loss: 0.019\n",
      "[1443] loss: 0.019\n",
      "[1444] loss: 0.019\n",
      "[1445] loss: 0.019\n",
      "[1446] loss: 0.019\n",
      "[1447] loss: 0.019\n",
      "[1448] loss: 0.019\n",
      "[1449] loss: 0.019\n",
      "[1450] loss: 0.019\n",
      "[1451] loss: 0.019\n",
      "[1452] loss: 0.019\n",
      "[1453] loss: 0.019\n",
      "[1454] loss: 0.019\n",
      "[1455] loss: 0.019\n",
      "[1456] loss: 0.018\n",
      "[1457] loss: 0.018\n",
      "[1458] loss: 0.018\n",
      "[1459] loss: 0.018\n",
      "[1460] loss: 0.018\n",
      "[1461] loss: 0.018\n",
      "[1462] loss: 0.018\n",
      "[1463] loss: 0.018\n",
      "[1464] loss: 0.018\n",
      "[1465] loss: 0.018\n",
      "[1466] loss: 0.018\n",
      "[1467] loss: 0.018\n",
      "[1468] loss: 0.018\n",
      "[1469] loss: 0.018\n",
      "[1470] loss: 0.018\n",
      "[1471] loss: 0.018\n",
      "[1472] loss: 0.018\n",
      "[1473] loss: 0.018\n",
      "[1474] loss: 0.018\n",
      "[1475] loss: 0.018\n",
      "[1476] loss: 0.018\n",
      "[1477] loss: 0.018\n",
      "[1478] loss: 0.018\n",
      "[1479] loss: 0.018\n",
      "[1480] loss: 0.018\n",
      "[1481] loss: 0.018\n",
      "[1482] loss: 0.018\n",
      "[1483] loss: 0.018\n",
      "[1484] loss: 0.018\n",
      "[1485] loss: 0.018\n",
      "[1486] loss: 0.018\n",
      "[1487] loss: 0.018\n",
      "[1488] loss: 0.018\n",
      "[1489] loss: 0.018\n",
      "[1490] loss: 0.018\n",
      "[1491] loss: 0.018\n",
      "[1492] loss: 0.018\n",
      "[1493] loss: 0.018\n",
      "[1494] loss: 0.018\n",
      "[1495] loss: 0.018\n",
      "[1496] loss: 0.018\n",
      "[1497] loss: 0.018\n",
      "[1498] loss: 0.018\n",
      "[1499] loss: 0.018\n",
      "[1500] loss: 0.018\n",
      "[1501] loss: 0.018\n",
      "[1502] loss: 0.018\n",
      "[1503] loss: 0.018\n",
      "[1504] loss: 0.018\n",
      "[1505] loss: 0.018\n",
      "[1506] loss: 0.018\n",
      "[1507] loss: 0.018\n",
      "[1508] loss: 0.018\n",
      "[1509] loss: 0.018\n",
      "[1510] loss: 0.018\n",
      "[1511] loss: 0.018\n",
      "[1512] loss: 0.018\n",
      "[1513] loss: 0.018\n",
      "[1514] loss: 0.018\n",
      "[1515] loss: 0.018\n",
      "[1516] loss: 0.018\n",
      "[1517] loss: 0.018\n",
      "[1518] loss: 0.018\n",
      "[1519] loss: 0.018\n",
      "[1520] loss: 0.018\n",
      "[1521] loss: 0.018\n",
      "[1522] loss: 0.018\n",
      "[1523] loss: 0.018\n",
      "[1524] loss: 0.018\n",
      "[1525] loss: 0.018\n",
      "[1526] loss: 0.018\n",
      "[1527] loss: 0.018\n",
      "[1528] loss: 0.018\n",
      "[1529] loss: 0.018\n",
      "[1530] loss: 0.018\n",
      "[1531] loss: 0.018\n",
      "[1532] loss: 0.018\n",
      "[1533] loss: 0.018\n",
      "[1534] loss: 0.018\n",
      "[1535] loss: 0.018\n",
      "[1536] loss: 0.018\n",
      "[1537] loss: 0.018\n",
      "[1538] loss: 0.018\n",
      "[1539] loss: 0.018\n",
      "[1540] loss: 0.018\n",
      "[1541] loss: 0.018\n",
      "[1542] loss: 0.018\n",
      "[1543] loss: 0.018\n",
      "[1544] loss: 0.018\n",
      "[1545] loss: 0.018\n",
      "[1546] loss: 0.018\n",
      "[1547] loss: 0.018\n",
      "[1548] loss: 0.018\n",
      "[1549] loss: 0.018\n",
      "[1550] loss: 0.018\n",
      "[1551] loss: 0.018\n",
      "[1552] loss: 0.018\n",
      "[1553] loss: 0.018\n",
      "[1554] loss: 0.018\n",
      "[1555] loss: 0.018\n",
      "[1556] loss: 0.018\n",
      "[1557] loss: 0.018\n",
      "[1558] loss: 0.018\n",
      "[1559] loss: 0.018\n",
      "[1560] loss: 0.018\n",
      "[1561] loss: 0.018\n",
      "[1562] loss: 0.018\n",
      "[1563] loss: 0.018\n",
      "[1564] loss: 0.018\n",
      "[1565] loss: 0.018\n",
      "[1566] loss: 0.018\n",
      "[1567] loss: 0.018\n",
      "[1568] loss: 0.018\n",
      "[1569] loss: 0.018\n",
      "[1570] loss: 0.018\n",
      "[1571] loss: 0.018\n",
      "[1572] loss: 0.018\n",
      "[1573] loss: 0.018\n",
      "[1574] loss: 0.018\n",
      "[1575] loss: 0.018\n",
      "[1576] loss: 0.018\n",
      "[1577] loss: 0.018\n",
      "[1578] loss: 0.018\n",
      "[1579] loss: 0.018\n",
      "[1580] loss: 0.018\n",
      "[1581] loss: 0.018\n",
      "[1582] loss: 0.018\n",
      "[1583] loss: 0.018\n",
      "[1584] loss: 0.018\n",
      "[1585] loss: 0.018\n",
      "[1586] loss: 0.018\n",
      "[1587] loss: 0.018\n",
      "[1588] loss: 0.018\n",
      "[1589] loss: 0.018\n",
      "[1590] loss: 0.018\n",
      "[1591] loss: 0.018\n",
      "[1592] loss: 0.018\n",
      "[1593] loss: 0.018\n",
      "[1594] loss: 0.018\n",
      "[1595] loss: 0.018\n",
      "[1596] loss: 0.017\n",
      "[1597] loss: 0.017\n",
      "[1598] loss: 0.017\n",
      "[1599] loss: 0.017\n",
      "[1600] loss: 0.017\n",
      "[1601] loss: 0.017\n",
      "[1602] loss: 0.017\n",
      "[1603] loss: 0.017\n",
      "[1604] loss: 0.017\n",
      "[1605] loss: 0.017\n",
      "[1606] loss: 0.017\n",
      "[1607] loss: 0.017\n",
      "[1608] loss: 0.017\n",
      "[1609] loss: 0.017\n",
      "[1610] loss: 0.017\n",
      "[1611] loss: 0.017\n",
      "[1612] loss: 0.017\n",
      "[1613] loss: 0.017\n",
      "[1614] loss: 0.017\n",
      "[1615] loss: 0.017\n",
      "[1616] loss: 0.017\n",
      "[1617] loss: 0.017\n",
      "[1618] loss: 0.017\n",
      "[1619] loss: 0.017\n",
      "[1620] loss: 0.017\n",
      "[1621] loss: 0.017\n",
      "[1622] loss: 0.017\n",
      "[1623] loss: 0.017\n",
      "[1624] loss: 0.017\n",
      "[1625] loss: 0.017\n",
      "[1626] loss: 0.017\n",
      "[1627] loss: 0.017\n",
      "[1628] loss: 0.017\n",
      "[1629] loss: 0.017\n",
      "[1630] loss: 0.017\n",
      "[1631] loss: 0.017\n",
      "[1632] loss: 0.017\n",
      "[1633] loss: 0.017\n",
      "[1634] loss: 0.017\n",
      "[1635] loss: 0.017\n",
      "[1636] loss: 0.017\n",
      "[1637] loss: 0.017\n",
      "[1638] loss: 0.017\n",
      "[1639] loss: 0.017\n",
      "[1640] loss: 0.017\n",
      "[1641] loss: 0.017\n",
      "[1642] loss: 0.017\n",
      "[1643] loss: 0.017\n",
      "[1644] loss: 0.017\n",
      "[1645] loss: 0.017\n",
      "[1646] loss: 0.017\n",
      "[1647] loss: 0.017\n",
      "[1648] loss: 0.017\n",
      "[1649] loss: 0.017\n",
      "[1650] loss: 0.017\n",
      "[1651] loss: 0.017\n",
      "[1652] loss: 0.017\n",
      "[1653] loss: 0.017\n",
      "[1654] loss: 0.017\n",
      "[1655] loss: 0.017\n",
      "[1656] loss: 0.017\n",
      "[1657] loss: 0.017\n",
      "[1658] loss: 0.017\n",
      "[1659] loss: 0.017\n",
      "[1660] loss: 0.017\n",
      "[1661] loss: 0.017\n",
      "[1662] loss: 0.017\n",
      "[1663] loss: 0.017\n",
      "[1664] loss: 0.017\n",
      "[1665] loss: 0.017\n",
      "[1666] loss: 0.017\n",
      "[1667] loss: 0.017\n",
      "[1668] loss: 0.017\n",
      "[1669] loss: 0.017\n",
      "[1670] loss: 0.017\n",
      "[1671] loss: 0.017\n",
      "[1672] loss: 0.017\n",
      "[1673] loss: 0.017\n",
      "[1674] loss: 0.017\n",
      "[1675] loss: 0.017\n",
      "[1676] loss: 0.017\n",
      "[1677] loss: 0.017\n",
      "[1678] loss: 0.017\n",
      "[1679] loss: 0.017\n",
      "[1680] loss: 0.017\n",
      "[1681] loss: 0.017\n",
      "[1682] loss: 0.017\n",
      "[1683] loss: 0.017\n",
      "[1684] loss: 0.017\n",
      "[1685] loss: 0.017\n",
      "[1686] loss: 0.017\n",
      "[1687] loss: 0.017\n",
      "[1688] loss: 0.017\n",
      "[1689] loss: 0.017\n",
      "[1690] loss: 0.017\n",
      "[1691] loss: 0.017\n",
      "[1692] loss: 0.017\n",
      "[1693] loss: 0.017\n",
      "[1694] loss: 0.017\n",
      "[1695] loss: 0.017\n",
      "[1696] loss: 0.017\n",
      "[1697] loss: 0.017\n",
      "[1698] loss: 0.017\n",
      "[1699] loss: 0.017\n",
      "[1700] loss: 0.017\n",
      "[1701] loss: 0.017\n",
      "[1702] loss: 0.017\n",
      "[1703] loss: 0.017\n",
      "[1704] loss: 0.017\n",
      "[1705] loss: 0.017\n",
      "[1706] loss: 0.017\n",
      "[1707] loss: 0.017\n",
      "[1708] loss: 0.017\n",
      "[1709] loss: 0.017\n",
      "[1710] loss: 0.017\n",
      "[1711] loss: 0.017\n",
      "[1712] loss: 0.017\n",
      "[1713] loss: 0.017\n",
      "[1714] loss: 0.017\n",
      "[1715] loss: 0.017\n",
      "[1716] loss: 0.017\n",
      "[1717] loss: 0.017\n",
      "[1718] loss: 0.017\n",
      "[1719] loss: 0.017\n",
      "[1720] loss: 0.017\n",
      "[1721] loss: 0.017\n",
      "[1722] loss: 0.017\n",
      "[1723] loss: 0.017\n",
      "[1724] loss: 0.017\n",
      "[1725] loss: 0.017\n",
      "[1726] loss: 0.017\n",
      "[1727] loss: 0.017\n",
      "[1728] loss: 0.017\n",
      "[1729] loss: 0.017\n",
      "[1730] loss: 0.017\n",
      "[1731] loss: 0.017\n",
      "[1732] loss: 0.017\n",
      "[1733] loss: 0.017\n",
      "[1734] loss: 0.017\n",
      "[1735] loss: 0.017\n",
      "[1736] loss: 0.017\n",
      "[1737] loss: 0.017\n",
      "[1738] loss: 0.017\n",
      "[1739] loss: 0.016\n",
      "[1740] loss: 0.016\n",
      "[1741] loss: 0.016\n",
      "[1742] loss: 0.016\n",
      "[1743] loss: 0.016\n",
      "[1744] loss: 0.016\n",
      "[1745] loss: 0.016\n",
      "[1746] loss: 0.016\n",
      "[1747] loss: 0.016\n",
      "[1748] loss: 0.016\n",
      "[1749] loss: 0.016\n",
      "[1750] loss: 0.016\n",
      "[1751] loss: 0.016\n",
      "[1752] loss: 0.016\n",
      "[1753] loss: 0.016\n",
      "[1754] loss: 0.016\n",
      "[1755] loss: 0.016\n",
      "[1756] loss: 0.016\n",
      "[1757] loss: 0.016\n",
      "[1758] loss: 0.016\n",
      "[1759] loss: 0.016\n",
      "[1760] loss: 0.016\n",
      "[1761] loss: 0.016\n",
      "[1762] loss: 0.016\n",
      "[1763] loss: 0.016\n",
      "[1764] loss: 0.016\n",
      "[1765] loss: 0.016\n",
      "[1766] loss: 0.016\n",
      "[1767] loss: 0.016\n",
      "[1768] loss: 0.016\n",
      "[1769] loss: 0.016\n",
      "[1770] loss: 0.016\n",
      "[1771] loss: 0.016\n",
      "[1772] loss: 0.016\n",
      "[1773] loss: 0.016\n",
      "[1774] loss: 0.016\n",
      "[1775] loss: 0.016\n",
      "[1776] loss: 0.016\n",
      "[1777] loss: 0.016\n",
      "[1778] loss: 0.016\n",
      "[1779] loss: 0.016\n",
      "[1780] loss: 0.016\n",
      "[1781] loss: 0.016\n",
      "[1782] loss: 0.016\n",
      "[1783] loss: 0.016\n",
      "[1784] loss: 0.016\n",
      "[1785] loss: 0.016\n",
      "[1786] loss: 0.016\n",
      "[1787] loss: 0.016\n",
      "[1788] loss: 0.016\n",
      "[1789] loss: 0.016\n",
      "[1790] loss: 0.016\n",
      "[1791] loss: 0.016\n",
      "[1792] loss: 0.016\n",
      "[1793] loss: 0.016\n",
      "[1794] loss: 0.016\n",
      "[1795] loss: 0.016\n",
      "[1796] loss: 0.016\n",
      "[1797] loss: 0.016\n",
      "[1798] loss: 0.016\n",
      "[1799] loss: 0.016\n",
      "[1800] loss: 0.016\n",
      "[1801] loss: 0.016\n",
      "[1802] loss: 0.016\n",
      "[1803] loss: 0.016\n",
      "[1804] loss: 0.016\n",
      "[1805] loss: 0.016\n",
      "[1806] loss: 0.016\n",
      "[1807] loss: 0.016\n",
      "[1808] loss: 0.016\n",
      "[1809] loss: 0.016\n",
      "[1810] loss: 0.016\n",
      "[1811] loss: 0.016\n",
      "[1812] loss: 0.016\n",
      "[1813] loss: 0.016\n",
      "[1814] loss: 0.016\n",
      "[1815] loss: 0.016\n",
      "[1816] loss: 0.016\n",
      "[1817] loss: 0.016\n",
      "[1818] loss: 0.016\n",
      "[1819] loss: 0.016\n",
      "[1820] loss: 0.016\n",
      "[1821] loss: 0.016\n",
      "[1822] loss: 0.016\n",
      "[1823] loss: 0.016\n",
      "[1824] loss: 0.016\n",
      "[1825] loss: 0.016\n",
      "[1826] loss: 0.016\n",
      "[1827] loss: 0.016\n",
      "[1828] loss: 0.016\n",
      "[1829] loss: 0.016\n",
      "[1830] loss: 0.016\n",
      "[1831] loss: 0.016\n",
      "[1832] loss: 0.016\n",
      "[1833] loss: 0.016\n",
      "[1834] loss: 0.016\n",
      "[1835] loss: 0.016\n",
      "[1836] loss: 0.016\n",
      "[1837] loss: 0.016\n",
      "[1838] loss: 0.016\n",
      "[1839] loss: 0.016\n",
      "[1840] loss: 0.016\n",
      "[1841] loss: 0.016\n",
      "[1842] loss: 0.016\n",
      "[1843] loss: 0.016\n",
      "[1844] loss: 0.016\n",
      "[1845] loss: 0.016\n",
      "[1846] loss: 0.016\n",
      "[1847] loss: 0.016\n",
      "[1848] loss: 0.016\n",
      "[1849] loss: 0.016\n",
      "[1850] loss: 0.016\n",
      "[1851] loss: 0.016\n",
      "[1852] loss: 0.016\n",
      "[1853] loss: 0.016\n",
      "[1854] loss: 0.016\n",
      "[1855] loss: 0.016\n",
      "[1856] loss: 0.016\n",
      "[1857] loss: 0.016\n",
      "[1858] loss: 0.016\n",
      "[1859] loss: 0.016\n",
      "[1860] loss: 0.016\n",
      "[1861] loss: 0.016\n",
      "[1862] loss: 0.016\n",
      "[1863] loss: 0.016\n",
      "[1864] loss: 0.016\n",
      "[1865] loss: 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1866] loss: 0.016\n",
      "[1867] loss: 0.016\n",
      "[1868] loss: 0.016\n",
      "[1869] loss: 0.016\n",
      "[1870] loss: 0.016\n",
      "[1871] loss: 0.016\n",
      "[1872] loss: 0.016\n",
      "[1873] loss: 0.016\n",
      "[1874] loss: 0.016\n",
      "[1875] loss: 0.016\n",
      "[1876] loss: 0.016\n",
      "[1877] loss: 0.016\n",
      "[1878] loss: 0.016\n",
      "[1879] loss: 0.016\n",
      "[1880] loss: 0.016\n",
      "[1881] loss: 0.016\n",
      "[1882] loss: 0.016\n",
      "[1883] loss: 0.016\n",
      "[1884] loss: 0.016\n",
      "[1885] loss: 0.016\n",
      "[1886] loss: 0.016\n",
      "[1887] loss: 0.016\n",
      "[1888] loss: 0.015\n",
      "[1889] loss: 0.015\n",
      "[1890] loss: 0.015\n",
      "[1891] loss: 0.015\n",
      "[1892] loss: 0.015\n",
      "[1893] loss: 0.015\n",
      "[1894] loss: 0.015\n",
      "[1895] loss: 0.015\n",
      "[1896] loss: 0.015\n",
      "[1897] loss: 0.015\n",
      "[1898] loss: 0.015\n",
      "[1899] loss: 0.015\n",
      "[1900] loss: 0.015\n",
      "[1901] loss: 0.015\n",
      "[1902] loss: 0.015\n",
      "[1903] loss: 0.015\n",
      "[1904] loss: 0.015\n",
      "[1905] loss: 0.015\n",
      "[1906] loss: 0.015\n",
      "[1907] loss: 0.015\n",
      "[1908] loss: 0.015\n",
      "[1909] loss: 0.015\n",
      "[1910] loss: 0.015\n",
      "[1911] loss: 0.015\n",
      "[1912] loss: 0.015\n",
      "[1913] loss: 0.015\n",
      "[1914] loss: 0.015\n",
      "[1915] loss: 0.015\n",
      "[1916] loss: 0.015\n",
      "[1917] loss: 0.015\n",
      "[1918] loss: 0.015\n",
      "[1919] loss: 0.015\n",
      "[1920] loss: 0.015\n",
      "[1921] loss: 0.015\n",
      "[1922] loss: 0.015\n",
      "[1923] loss: 0.015\n",
      "[1924] loss: 0.015\n",
      "[1925] loss: 0.015\n",
      "[1926] loss: 0.015\n",
      "[1927] loss: 0.015\n",
      "[1928] loss: 0.015\n",
      "[1929] loss: 0.015\n",
      "[1930] loss: 0.015\n",
      "[1931] loss: 0.015\n",
      "[1932] loss: 0.015\n",
      "[1933] loss: 0.015\n",
      "[1934] loss: 0.015\n",
      "[1935] loss: 0.015\n",
      "[1936] loss: 0.015\n",
      "[1937] loss: 0.015\n",
      "[1938] loss: 0.015\n",
      "[1939] loss: 0.015\n",
      "[1940] loss: 0.015\n",
      "[1941] loss: 0.015\n",
      "[1942] loss: 0.015\n",
      "[1943] loss: 0.015\n",
      "[1944] loss: 0.015\n",
      "[1945] loss: 0.015\n",
      "[1946] loss: 0.015\n",
      "[1947] loss: 0.015\n",
      "[1948] loss: 0.015\n",
      "[1949] loss: 0.015\n",
      "[1950] loss: 0.015\n",
      "[1951] loss: 0.015\n",
      "[1952] loss: 0.015\n",
      "[1953] loss: 0.015\n",
      "[1954] loss: 0.015\n",
      "[1955] loss: 0.015\n",
      "[1956] loss: 0.015\n",
      "[1957] loss: 0.015\n",
      "[1958] loss: 0.015\n",
      "[1959] loss: 0.015\n",
      "[1960] loss: 0.015\n",
      "[1961] loss: 0.015\n",
      "[1962] loss: 0.015\n",
      "[1963] loss: 0.015\n",
      "[1964] loss: 0.015\n",
      "[1965] loss: 0.015\n",
      "[1966] loss: 0.015\n",
      "[1967] loss: 0.015\n",
      "[1968] loss: 0.015\n",
      "[1969] loss: 0.015\n",
      "[1970] loss: 0.015\n",
      "[1971] loss: 0.015\n",
      "[1972] loss: 0.015\n",
      "[1973] loss: 0.015\n",
      "[1974] loss: 0.015\n",
      "[1975] loss: 0.015\n",
      "[1976] loss: 0.015\n",
      "[1977] loss: 0.015\n",
      "[1978] loss: 0.015\n",
      "[1979] loss: 0.015\n",
      "[1980] loss: 0.015\n",
      "[1981] loss: 0.015\n",
      "[1982] loss: 0.015\n",
      "[1983] loss: 0.015\n",
      "[1984] loss: 0.015\n",
      "[1985] loss: 0.015\n",
      "[1986] loss: 0.015\n",
      "[1987] loss: 0.015\n",
      "[1988] loss: 0.015\n",
      "[1989] loss: 0.015\n",
      "[1990] loss: 0.015\n",
      "[1991] loss: 0.015\n",
      "[1992] loss: 0.015\n",
      "[1993] loss: 0.015\n",
      "[1994] loss: 0.015\n",
      "[1995] loss: 0.015\n",
      "[1996] loss: 0.015\n",
      "[1997] loss: 0.015\n",
      "[1998] loss: 0.015\n",
      "[1999] loss: 0.015\n",
      "[2000] loss: 0.015\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0ca1d34a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "pt_data      = torch.FloatTensor(norm_oau_data)#continuous_norm_oau_data\n",
    "pt_data      = torch.autograd.Variable(pt_data)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss0   = 0\n",
    "epsilon = 0.00001\n",
    "\n",
    "def low_parameter_to_zero(para,epsilon):\n",
    "    for i in para:\n",
    "        para = i.data\n",
    "        mask = (torch.abs(para)<epsilon)\n",
    "        mask = mask.type(torch.FloatTensor)\n",
    "        i.data.addcmul_(-1.0,i.data,mask)\n",
    "    \n",
    "for epoch in range(100):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out  = net.forward(pt_data)\n",
    "    loss = criterion(out,pt_data)\n",
    "    print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1, loss.data[0]))\n",
    "    if abs(loss.data[0]-loss0) < epsilon:\n",
    "        break\n",
    "    loss0 = loss.data[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()        # Does the update\n",
    "loss_recorder = []\n",
    "for epoch in range(2000):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out  = net.forward(pt_data)\n",
    "    loss = criterion(out,pt_data)\n",
    "    loss_recorder.append(loss.data[0])\n",
    "    print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1, loss.data[0]))\n",
    "    #if abs(loss.data[0]-loss0) < epsilon:\n",
    "        #break\n",
    "    loss0 = loss.data[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()        # Does the update\n",
    "    #low_parameter_to_zero(net.parameters(),0.1)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_recorder)\n",
    "plt.title('error curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### encode all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_oau_data      = net.encode_data(pt_data)\n",
    "encoded_oau_data      = encoded_oau_data.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "Density-based spatial clustering of applications with noise is a cluster algorithm proposed by Martin Ester .etc. It is able to find cluster in any shape and does not need to specify the number of clusters in advance.\n",
    "\n",
    "We require that for every point p in a cluster C there is a point q in C so that p is inside of the Eps-\n",
    "neighborhood of q and NEps(q) contains at least MinPts points.\n",
    "\n",
    "We use the scikit-learn tool kits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to determine epsilon ?\n",
    "k-distance plot for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmdwlqYNM7pLUQf8fIJm5Ehyf4TMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0c5e691d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k = 5\n",
    "\n",
    "neigh = NearestNeighbors(k+1)\n",
    "nbrs  = neigh.fit(norm_oau_data)\n",
    "distances, _ = nbrs.kneighbors(norm_oau_data)\n",
    "distances    = np.sum(distances,axis=1)/k\n",
    "distances.sort()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.title('%d-NN distance' % k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-distance plot for encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuEtSB/1/DOUeBKk+CdoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0c6aa7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k = 5\n",
    "\n",
    "neigh = NearestNeighbors(k+1)\n",
    "nbrs  = neigh.fit(encoded_oau_data)\n",
    "distances, _ = nbrs.kneighbors(encoded_oau_data)\n",
    "distances    = np.sum(distances,axis=1)/k\n",
    "distances.sort()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.title('%d-NN distance' % k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCN for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Estimated number of clusters: 6\n",
      "Silhouette Coefficient: 0.812\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/y3nu1kAivuiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0ca778c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.02, min_samples=10).fit(norm_oau_data)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(norm_oau_data, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x   = np.arange(len(labels))\n",
    "plt.scatter(x,labels)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCAN for encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Estimated number of clusters: 5\n",
      "Silhouette Coefficient: 0.977\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0d978b630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.02, min_samples=10).fit(encoded_oau_data)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(encoded_oau_data, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x   = np.arange(len(labels))\n",
    "plt.scatter(x,labels)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...,  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(labels != -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### convert all data to discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def z_test(data,length,epsilon=None):\n",
    "    assert(length<len(data))\n",
    "    epsilon = 0.95 if epsilon == None else epsilon\n",
    "    thresh_hold = st.norm.ppf(epsilon)\n",
    "    result = np.zeros([len(data)-length,len(data[0])])\n",
    "    for i in range(length,len(data)):\n",
    "        mean = np.mean(data[i-length:i,:],0)\n",
    "        var  = np.var(data[i-length:i,:],0)\n",
    "        norm = (data[i,:]-mean)/(var**0.5)\n",
    "        z    = abs(norm) >= thresh_hold\n",
    "        result[i-length,:] = [int(j) for j in z]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_data(data,label,length,epsilon=None):\n",
    "    mask = (label!=-1)\n",
    "    data = data[mask]\n",
    "    label= label[mask]\n",
    "    result = z_test(data,length,epsilon)\n",
    "    label  = label[length:]\n",
    "    headings = ['feature'+str(i) for i in range(len(result[0]))]\n",
    "    headings.append('mode')\n",
    "    labeled_data = np.hstack((result,[[i] for i in label]))\n",
    "    return headings, labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature0', 'feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'mode']\n",
      "10177\n",
      "[ 1000.  1047.  1064.   950.   963.   975.  7223.]\n"
     ]
    }
   ],
   "source": [
    "z_len = 100\n",
    "\n",
    "headings, labeled_data = discrete_data(encoded_oau_data,labels,z_len,0.95)\n",
    "print(headings)\n",
    "print(len(labeled_data))\n",
    "print(sum(labeled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAN classifier\n",
    "TAN classifier is the Tree Augmented Naive Baysian, which is created by adding a spaned tree into naive baysian network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class node:\n",
    "    '''\n",
    "    The class to store the information of a node in a graph\n",
    "    '''\n",
    "    def __init__(self,name,domain):\n",
    "        self.name          = name       #a string\n",
    "        self.domain        = domain     #a list for string\n",
    "        self.father        = []         #a list\n",
    "        self.children      = []         #a list\n",
    "        self.pro_table     = {}         #init as an empty dict\n",
    "                                        #for example\n",
    "                                        #{(on,on):[0.1,0.9],(on,off):[0.2,0.8],(off,on):[0.3,0.7],(off,off):[0.4,0.6]}\n",
    "        \n",
    "    def add_father(self,fa):            #fa is a node\n",
    "        self.father.append(fa)\n",
    "        \n",
    "    def add_child(self,ch):             #ch is a node\n",
    "        self.children.append(ch)\n",
    "        \n",
    "    def add_pro(self,instance,table):   #instance is a string list, table is a float list\n",
    "        self.pro_table[instance] = table\n",
    "        \n",
    "    def print_self(self):\n",
    "        #structring information\n",
    "        father_info   = ''.join([i.name+' ' for i in self.father])\n",
    "        children_info = ''.join([i.name+' ' for i in self.children])\n",
    "        \n",
    "        print(father_info + '==>' + self.name + '==>' + children_info)\n",
    "        \n",
    "        #conditional probability table\n",
    "        for i in self.pro_table:\n",
    "            print(i,':',self.pro_table[i])\n",
    "        \n",
    "            \n",
    "        \n",
    "class graph:\n",
    "    '''\n",
    "    The class to store a graph\n",
    "    The code will not check the validation of data, so please preprocess the data to ensure\n",
    "    1) there are heandings: name1,name2,...\n",
    "    2) no empty data\n",
    "    3) all data is discretized\n",
    "    4) last colum is the label\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.nodes         = []         #a list\n",
    "        \n",
    "    def get_node(self,the_node):\n",
    "        '''\n",
    "        functional function\n",
    "        get the node by name\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "        output:\n",
    "            type         description\n",
    "            ------------------------------\n",
    "            class node,  the found node\n",
    "        '''\n",
    "        names              = [i.name for i in self.nodes]\n",
    "        #print('find node ',the_node,' in ',names)\n",
    "        node_index         = names.index(the_node)\n",
    "        return self.nodes[node_index]\n",
    "    \n",
    "    def get_node_index(self,the_node):\n",
    "        '''\n",
    "        functional function\n",
    "        get the node by name\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "        output:\n",
    "            type         description\n",
    "            ------------------------------\n",
    "            int,         index\n",
    "        '''\n",
    "        names              = [i.name for i in self.nodes]\n",
    "        node_index         = names.index(the_node)\n",
    "        return node_index\n",
    "        \n",
    "    def add_node(self,name,domain):\n",
    "        '''\n",
    "        functional function\n",
    "        add a node by name and domain\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            name,     string, node name\n",
    "            domain,   list,   domain of the node\n",
    "        '''\n",
    "        self.nodes.append(node(name,domain))\n",
    "        \n",
    "    def add_connection(self,from_node1,to_node2):#both from_node1 and to_node2 are strings\n",
    "        '''\n",
    "        functional function\n",
    "        add a connection betwen to nodes\n",
    "        input:\n",
    "            variable     type         description\n",
    "            -------------------------------------------\n",
    "            from_node1,  class node,  the first node\n",
    "            to_node2,    class node,  the second node\n",
    "        '''\n",
    "        #print('from node 1 =',from_node1)\n",
    "        #print('to node 2 =',to_node2)\n",
    "        from_node1         = self.get_node(from_node1)\n",
    "        to_node2           = self.get_node(to_node2)\n",
    "        \n",
    "        from_node1.add_child(to_node2)\n",
    "        to_node2.add_father(from_node1)\n",
    "        \n",
    "    def add_pro(self,the_node,instance,table):#the_node is a string\n",
    "        '''\n",
    "        functional function\n",
    "        add a conditioanal probability P(a|b) into a node\n",
    "        input:\n",
    "            variable  type    description\n",
    "            ------------------------------\n",
    "            the_node, string, node name\n",
    "            instance, list,   b part in P(a|b)\n",
    "                              where each entry in it is a value in domain\n",
    "            table,    list,   a part in P(a|b)\n",
    "                              where each entry in it is a probability responding to a value\n",
    "        '''\n",
    "        the_node           = self.get_node(the_node)\n",
    "        the_node.add_pro(instance,table)\n",
    "        \n",
    "    def get_nodes_from_data(self,headings,data):#headings = ['data1','data2',...], data = numpy 2d vector\n",
    "        '''\n",
    "        interface/functional function\n",
    "        get the basic information about node from data and insert them into the class\n",
    "        input:\n",
    "            variable  type            description\n",
    "            ------------------------------------------------\n",
    "            headings, numpy arrary,   the names of variables\n",
    "            data,     numpy 2darrary, all the data where row means different instance\n",
    "                                      and col means each sensor\n",
    "        '''\n",
    "        for i,name in enumerate(headings):\n",
    "            domain = list(set(data[:,i]))\n",
    "            self.add_node(name,domain)\n",
    "            \n",
    "    def mutual_information(self,data1,data2):\n",
    "        '''\n",
    "        functional function\n",
    "        get the mutual information between two cols\n",
    "        input:\n",
    "            variable  type            description\n",
    "            ------------------------------------------------\n",
    "            data1,    numpy arrary,   data of the first col\n",
    "            data2,    numpy arrary,   data of the second col\n",
    "        output:\n",
    "            type    description\n",
    "            -----------------------------\n",
    "            float,  mutual information entropy with laplace smooth\n",
    "        '''\n",
    "        assert(len(data1)==len(data2))\n",
    "        \n",
    "        mi       = 0.0\n",
    "        length   = len(data1)\n",
    "        \n",
    "        domain1 = list(set(data1))\n",
    "        #print('domain1=',domain1)\n",
    "        domain2 = list(set(data2))\n",
    "        #print('domain2=',domain2)\n",
    "        len1    = len(domain1)\n",
    "        len2    = len(domain2)\n",
    "        \n",
    "        for x in domain1:\n",
    "            for y in domain2:\n",
    "                Px     = float(sum(data1==x) + 1)/(length+len1)    #laplace smooth\n",
    "                Py     = float(sum(data2==y) + 1)/(length+len2)    #laplace smooth\n",
    "                \n",
    "                indexxy= [1 if (i==x)and(j==y) else 0 for i,j in zip(data1,data2)]\n",
    "                Pxy    = float(sum(indexxy) + 1)/(length+len1*len2) #laplace smooth\n",
    "                \n",
    "                mi     = mi + Pxy*math.log(Pxy/(Px*Py))\n",
    "        return mi\n",
    "    \n",
    "    def get_max_span_tree_prime(self,mi_dict):\n",
    "        '''\n",
    "        functional function\n",
    "        judge if some edges compose a span tree\n",
    "        input:\n",
    "            variable  type    description\n",
    "            -----------------------------\n",
    "            edges,    dict,   all the edges\n",
    "        output:\n",
    "            type    description\n",
    "            -----------------------------\n",
    "            set,    all the edges that compose a max span tree\n",
    "        '''\n",
    "        #prepare structure to qurey\n",
    "        #test\n",
    "        #print(mi_dict)\n",
    "        \n",
    "        Vnew  = set()\n",
    "        V     = set()\n",
    "        Enew  = set()\n",
    "        edges  = defaultdict(list)\n",
    "        initial_node = max(mi_dict.items(),key=lambda d: d[1])[0][0]\n",
    "        Vnew.add(initial_node)\n",
    "        #print('Vnew=',Vnew)\n",
    "        for i in mi_dict:\n",
    "            edges[i[0]].append(i[1])\n",
    "            edges[i[1]].append(i[0])\n",
    "            V.add(i[0])\n",
    "            V.add(i[1])\n",
    "        #print('V=',V)\n",
    "        while Vnew != V:\n",
    "            #print('Vnew=',Vnew)\n",
    "            new_edges = {}\n",
    "            for u in Vnew:\n",
    "                v = [i for i in edges[u] if i not in Vnew]\n",
    "                for vi in v:\n",
    "                    edge = (u,vi) if (u,vi) in mi_dict else (vi,u)\n",
    "                    new_edges[edge] = mi_dict[edge]\n",
    "            new_edges = sorted(new_edges.items(),key=lambda d:d[1],reverse = True)\n",
    "            best_edge = new_edges[0]\n",
    "            Enew.add(best_edge[0])\n",
    "            Vnew.add(best_edge[0][0])\n",
    "            Vnew.add(best_edge[0][1])\n",
    "        return Enew\n",
    "        \n",
    "        \n",
    "    def get_basic_tan_structure(self,headings,data):#headings = ['data1','data2',...], data = numpy 2d vector\n",
    "        '''\n",
    "        interface function\n",
    "        get the basic structure of TAN and connect the nodes in the class.\n",
    "        we assumpe that the last col is the label colum and is ignored when handle features\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "        '''\n",
    "        #computing mutual information entropy between features\n",
    "        mi_dict = {}\n",
    "        for i in range(len(headings)-1):\n",
    "            for j in range(i+1,len(headings)-1):\n",
    "                data1   = data[:,i]\n",
    "                data2   = data[:,j]\n",
    "                mi      = self.mutual_information(data1,data2)\n",
    "                mi_dict[(headings[i],headings[j])] = mi\n",
    "        \n",
    "        #set maximal span tree, mst\n",
    "        mst             = self.get_max_span_tree_prime(mi_dict)\n",
    "        Visited         = set()\n",
    "        edges           = defaultdict(list)\n",
    "        for i in mst:\n",
    "            edges[i[0]].append(i[1])\n",
    "            edges[i[1]].append(i[0])\n",
    "        initial_node    = self.nodes[0].name\n",
    "        start           = [initial_node]\n",
    "        Visited.add(initial_node)\n",
    "        while len(start)!=0:\n",
    "            new_start = []\n",
    "            for i in start:\n",
    "                ends = edges[i]\n",
    "                for j in ends:\n",
    "                    if j not in Visited:\n",
    "                        Visited.add(j)\n",
    "                        new_start.append(j)\n",
    "                        self.add_connection(i,j)\n",
    "            start = new_start\n",
    "        \n",
    "        #set the connection between label and features\n",
    "        for i in range(len(self.nodes)-1):\n",
    "            #print('class node =',self.nodes[-1].name)\n",
    "            self.add_connection(self.nodes[-1].name,self.nodes[i].name)\n",
    "            \n",
    "    def get_cpt(self,headings,data,node_name):\n",
    "        '''\n",
    "        interface/functional function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        x, y==>x, y,z==>x\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "            node_name,string,         the node name\n",
    "        '''\n",
    "        the_index     = self.get_node_index(node_name)\n",
    "        the_node      = self.nodes[the_index]\n",
    "        father_nodes  = the_node.father\n",
    "        \n",
    "        domain_x = the_node.domain\n",
    "        data_x   = data[:,the_index]\n",
    "        len_x    = len(domain_x)\n",
    "        \n",
    "        length   = len(data)\n",
    "        \n",
    "        if len(father_nodes) == 0:\n",
    "            table = []\n",
    "            for x in domain_x:\n",
    "                Px = float(sum(data_x == x)+1)/(length + len_x)          #laplace smooth\n",
    "                table.append(Px)\n",
    "            self.add_pro(node_name,(),table)\n",
    "        elif len(father_nodes)==1:#y-->x\n",
    "            domain_y = father_nodes[0].domain\n",
    "            data_y   = data[:,self.get_node_index(father_nodes[0].name)]\n",
    "            len_y    = len(domain_y)\n",
    "            \n",
    "            for y in domain_y:\n",
    "                table    = []\n",
    "                Py     = float(sum(data_y == y) + 1)/(length+len_y)       #laplace smooth\n",
    "                for x in domain_x:\n",
    "                    indexxy= [1 if (i==x)and(j==y) else 0 for i,j in zip(data_x,data_y)]\n",
    "                    Pxy    = float(sum(indexxy) + 1)/(length+len_x*len_y) #laplace smooth\n",
    "                    table.append(float(Pxy)/Py)\n",
    "                self.add_pro(node_name,(y,),table)\n",
    "                    \n",
    "        elif len(father_nodes)==2:\n",
    "            domain_y = father_nodes[0].domain\n",
    "            data_y   = data[:,self.get_node_index(father_nodes[0].name)]\n",
    "            len_y    = len(domain_y)\n",
    "            domain_z = father_nodes[1].domain\n",
    "            data_z   = data[:,self.get_node_index(father_nodes[1].name)]\n",
    "            len_z    = len(domain_z)\n",
    "            \n",
    "            for y in domain_y:\n",
    "                for z in domain_z:\n",
    "                    table = []\n",
    "                    indexyz= [1 if (i==y)and(j==z) else 0 for i,j in zip(data_y,data_z)]\n",
    "                    Pyz    = float(sum(indexyz) + 1)/(length+len_y*len_z) #laplace smooth\n",
    "                    for x in domain_x:\n",
    "                        indexxyz= [1 if (i==y)and(j==z)and(k==x) else 0 for i,j,k in zip(data_y,data_z,data_x)]\n",
    "                        Pxyz    = float(sum(indexxyz) + 1)/(length+len_y*len_z*len_x) #laplace smooth\n",
    "                        table.append(float(Pxyz)/Pyz)\n",
    "                    self.add_pro(node_name,(y,z),table)\n",
    "        \n",
    "    def train_tan(self,headings,data):\n",
    "        '''\n",
    "        interface function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            headings, numpy arrary,   names of variables/nodes\n",
    "            data,     numpy arrary,   all the monitor data\n",
    "        '''\n",
    "        for i in self.nodes:\n",
    "            self.get_cpt(headings,data,i.name)\n",
    "    \n",
    "    def print_tan(self):\n",
    "        for i in self.nodes:\n",
    "            i.print_self()\n",
    "        \n",
    "    def predict(self,features):\n",
    "        '''\n",
    "        interface function\n",
    "        calculate the condition probability table based on the basic structure\n",
    "        input:\n",
    "            variable  type            description\n",
    "            --------------------------------------------------\n",
    "            features, numpy arrary,   names of variables/nodes\n",
    "        '''\n",
    "        weight   = []\n",
    "        features = np.ndarray.tolist(features)\n",
    "        #print(self.nodes[-1].domain)\n",
    "        for i in self.nodes[-1].domain:\n",
    "            #print('i=',i)\n",
    "            Pi = 1\n",
    "            new_features = features + [i]\n",
    "            #print('new_features=',new_features)\n",
    "            for node in self.nodes:\n",
    "                #father\n",
    "                instance = []\n",
    "                father = node.father\n",
    "                for f in father:\n",
    "                    index = self.get_node_index(f.name)\n",
    "                    instance.append(new_features[index])\n",
    "                #print('instance=',instance)\n",
    "                table = node.pro_table[tuple(instance)]\n",
    "                #print('table = ',table)\n",
    "                #this node\n",
    "                index = self.get_node_index(node.name)           #index of this node in features\n",
    "                value = new_features[index]                          #value of this ndde in features\n",
    "                index = node.domain.index(value)                 #index value in domain\n",
    "                Pi = Pi*table[index]\n",
    "            weight.append(Pi)\n",
    "        norm = sum(weight)\n",
    "        weight = [i/norm for i in weight]\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train TAN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode ==>feature0==>feature5 feature2 \n",
      "feature5 mode ==>feature1==>feature4 \n",
      "feature0 mode ==>feature2==>\n",
      "feature5 mode ==>feature3==>\n",
      "feature1 mode ==>feature4==>\n",
      "feature0 mode ==>feature5==>feature3 feature1 \n",
      "==>mode==>feature0 feature1 feature2 feature3 feature4 feature5 \n",
      "mode ==>feature0==>feature5 feature2 \n",
      "(0.0,) : [0.9239443592618989, 0.07572525396277413]\n",
      "(1.0,) : [0.5133252568932625, 0.4873639795331434]\n",
      "(2.0,) : [0.9643976147217984, 0.03544595948700091]\n",
      "(3.0,) : [0.8031770183286823, 0.2141805382209819]\n",
      "(4.0,) : [0.9828506920585058, 0.033316972612152745]\n",
      "feature5 mode ==>feature1==>feature4 \n",
      "(0.0, 0.0) : [0.949710753764329, 0.04948218735335169]\n",
      "(0.0, 1.0) : [0.9921767213295343, 0.009123464104179626]\n",
      "(0.0, 2.0) : [0.9787920922817073, 0.020570061484028877]\n",
      "(0.0, 3.0) : [0.944527356530887, 0.07265595050237593]\n",
      "(0.0, 4.0) : [0.9773015081161978, 0.04343562258294213]\n",
      "(1.0, 0.0) : [0.48589618889429603, 0.5152175106379173]\n",
      "(1.0, 1.0) : [0.004873264972720337, 0.9965826869213088]\n",
      "(1.0, 2.0) : [0.420639713434808, 0.5915245970176987]\n",
      "(1.0, 3.0) : [0.999019319407669, 0.4995096597038345]\n",
      "(1.0, 4.0) : [0.9990193194076691, 0.06660128796051126]\n",
      "feature0 mode ==>feature2==>\n",
      "(0.0, 0.0) : [0.95513116385807, 0.04406162652015071]\n",
      "(0.0, 1.0) : [0.9806465503151142, 0.020669365229124187]\n",
      "(0.0, 2.0) : [0.986202497348694, 0.01316322265516346]\n",
      "(0.0, 3.0) : [0.9546184607673279, 0.06660128796051125]\n",
      "(0.0, 4.0) : [0.9820867885702509, 0.03386506167483624]\n",
      "(1.0, 0.0) : [0.37039911206852133, 0.630736773693825]\n",
      "(1.0, 1.0) : [0.07740585525676855, 0.9240323971276743]\n",
      "(1.0, 2.0) : [0.3015907379343906, 0.706853292033728]\n",
      "(1.0, 3.0) : [0.16650321990127817, 0.9157677094570298]\n",
      "(1.0, 4.0) : [0.4995096597038345, 0.999019319407669]\n",
      "feature5 mode ==>feature3==>\n",
      "(0.0, 0.0) : [0.9564820004547876, 0.04271094066289304]\n",
      "(0.0, 1.0) : [0.996738453381624, 0.004561732052089813]\n",
      "(0.0, 2.0) : [0.9746780799849015, 0.02468407378083465]\n",
      "(0.0, 3.0) : [0.4359357030142555, 0.5812476040190074]\n",
      "(0.0, 4.0) : [0.49950965970383443, 0.5212274709953055]\n",
      "(1.0, 0.0) : [0.6618241193560238, 0.33928958017618943]\n",
      "(1.0, 1.0) : [0.029239589836322015, 0.9722163620577071]\n",
      "(1.0, 2.0) : [0.841279426869616, 0.17088488358289075]\n",
      "(1.0, 3.0) : [0.4995096597038345, 0.999019319407669]\n",
      "(1.0, 4.0) : [0.5328103036840901, 0.5328103036840901]\n",
      "feature1 mode ==>feature4==>\n",
      "(0.0, 0.0) : [0.9639721585794749, 0.03522239663233493]\n",
      "(0.0, 1.0) : [0.9944366619791936, 0.006873986142713318]\n",
      "(0.0, 2.0) : [0.9813651318505686, 0.018000348097435476]\n",
      "(0.0, 3.0) : [0.9236216349240711, 0.09424710560449706]\n",
      "(0.0, 4.0) : [0.8804916035457422, 0.13546024669934495]\n",
      "(1.0, 0.0) : [0.5390934440577232, 0.46181081746203567]\n",
      "(1.0, 1.0) : [0.021823237559876262, 0.9796208860211122]\n",
      "(1.0, 2.0) : [0.5379334796810525, 0.4706917947209209]\n",
      "(1.0, 3.0) : [0.24975482985191724, 0.999019319407669]\n",
      "(1.0, 4.0) : [0.4995096597038345, 0.999019319407669]\n",
      "feature0 mode ==>feature5==>feature3 feature1 \n",
      "(0.0, 0.0) : [0.9671006608261424, 0.03209212955207826]\n",
      "(0.0, 1.0) : [0.9967227232710997, 0.004593192273138708]\n",
      "(0.0, 2.0) : [0.9927841086762758, 0.00658161132758173]\n",
      "(0.0, 3.0) : [0.9768188900874983, 0.044400858640340844]\n",
      "(0.0, 4.0) : [0.7619638876838153, 0.25398796256127176]\n",
      "(1.0, 0.0) : [0.38098194384190764, 0.6201539419204386]\n",
      "(1.0, 1.0) : [0.012094664883870083, 0.9893435875005728]\n",
      "(1.0, 2.0) : [0.46181081746203556, 0.546633212506083]\n",
      "(1.0, 3.0) : [0.9990193194076689, 0.08325160995063909]\n",
      "(1.0, 4.0) : [0.999019319407669, 0.4995096597038345]\n",
      "==>mode==>feature0 feature1 feature2 feature3 feature4 feature5 \n",
      "() : [0.611864073855824, 0.08318601453545472, 0.2935572579061088, 0.005499901787468081, 0.0058927519151443725]\n"
     ]
    }
   ],
   "source": [
    "#%xmode Plain\n",
    "\n",
    "tan = graph()\n",
    "tan.get_nodes_from_data(headings,labeled_data)\n",
    "tan.get_basic_tan_structure(headings,labeled_data)\n",
    "tan.print_tan()\n",
    "tan.train_tan(headings,labeled_data)\n",
    "tan.print_tan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7029588572311506, 0.032127814088876896, 0.2292895391011142, 0.029551140805178993, 0.006072648773679429]\n"
     ]
    }
   ],
   "source": [
    "w = tan.predict(np.array([1,0,1,0,0,0]))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### test all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  2.  2.  2.]\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "input_data   = labeled_data[:,:-1]\n",
    "output_label = labeled_data[:,-1]\n",
    "print(output_label)\n",
    "predict_labels = []\n",
    "for i in input_data:\n",
    "    w = tan.predict(i)\n",
    "    index = w.index(max(w))\n",
    "    predict_labels.append(index)\n",
    "\n",
    "\n",
    "predict_labels = np.array(predict_labels)\n",
    "print(predict_labels)\n",
    "\n",
    "error_rate = sum([i==j for i,j in zip(predict_labels,output_label)])/float(len(output_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.641937702663\n"
     ]
    }
   ],
   "source": [
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model based Diagnosis\n",
    "r1 = Qe-delta_e*k1*Ne\n",
    "\n",
    "r2 = Qo-delta_o*k2*No\n",
    "\n",
    "r3 = Pe-delta_e*k3*Ne^2\n",
    "\n",
    "r4 = Po-delta_o*k4*No^2\n",
    "\n",
    "\n",
    "Qe exhuast fan airflow\n",
    "\n",
    "Ne exhuast fan rotational speed\n",
    "\n",
    "Qo outdoor fan airflow\n",
    "\n",
    "No outdoor fan rotational speed\n",
    "\n",
    "delta_e = 1 when exhuast fan is on, 0 when exhuast fan is off\n",
    "\n",
    "delta_o = 1 when outdoor fan is on, 0 when outdoor fan is off\n",
    "\n",
    "k1 = 86.2834\n",
    "\n",
    "k2 = 91.84286\n",
    "\n",
    "k3 = 0.0001472726\n",
    "\n",
    "k4 = 0.0001595658 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.88892610e+02   6.26171574e+02   5.45776651e-02   9.52501291e-02]\n",
      " [  4.60998659e+02   6.07997301e+02   4.92166792e-02   9.12588874e-02]\n",
      " [  4.43953845e+02   5.75093801e+02   4.79557015e-02   9.39267574e-02]\n",
      " ..., \n",
      " [  4.38398930e+02   4.84148684e+02   4.51344318e-02   6.44472616e-02]\n",
      " [  4.63937464e+02   5.58659727e+02   4.94103240e-02   6.87279366e-02]\n",
      " [  4.50130019e+02   5.74590413e+02   4.65992455e-02   8.37728537e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Qe = oau_data[:,5]\n",
    "Qo = oau_data[:,4]\n",
    "Ne = oau_data[:,3]\n",
    "No = oau_data[:,2]\n",
    "Pe = oau_data[:,1]\n",
    "Po = oau_data[:,0]\n",
    "De = [1 if i==0 else 0 for i in oau_data[:,13]]\n",
    "Do = [1 if i==0 else 0 for i in oau_data[:,12]]\n",
    "k1 = 86.2834\n",
    "k2 = 91.84286\n",
    "k3 = 0.0001472726\n",
    "k4 = 0.0001595658\n",
    "\n",
    "\n",
    "def residual_ariflow_balance(Q,delta,k,N):\n",
    "    return Q - delta*k*N\n",
    "\n",
    "def residual_pressure_balance(P,delta,k,N):\n",
    "    return P - delta*k*(N**2)\n",
    "\n",
    "residuals = [[residual_ariflow_balance(qe,de,k1,ne),\\\n",
    "              residual_ariflow_balance(qo,do,k2,no),\\\n",
    "              residual_pressure_balance(pe,de,k3,ne),\\\n",
    "              residual_pressure_balance(po,do,k4,no)] \\\n",
    "              for qe,qo,pe,po,ne,no,de,do in zip(Qe,Qo,Pe,Po,Ne,No,De,Do)]\n",
    "residuals = np.array(residuals)\n",
    "print(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\App\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\App\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  if sys.path[0] == '':\n",
      "D:\\App\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dis_residuals= [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "dis_residuals = z_test(residuals,100,0.95)\n",
    "print('dis_residuals=',dis_residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['residual0', 'residual1', 'residual2', 'residual3', 'mode']\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "mode ==>residual0==>residual2 \n",
      "residual3 mode ==>residual1==>\n",
      "residual0 mode ==>residual2==>residual3 \n",
      "residual2 mode ==>residual3==>residual1 \n",
      "==>mode==>residual0 residual1 residual2 residual3 \n",
      "mode ==>residual0==>residual2 \n",
      "(0.0,) : [0.9236234895417176, 0.07604612368295538]\n",
      "(1.0,) : [0.6974143145377428, 0.3032749218886631]\n",
      "(2.0,) : [0.9550345310837226, 0.04480904312507662]\n",
      "(3.0,) : [0.9459640438093369, 0.0713935127403273]\n",
      "(4.0,) : [0.9162167468342004, 0.09995091783645822]\n",
      "residual3 mode ==>residual1==>\n",
      "(0.0, 0.0) : [0.9848070390135973, 0.014385600886682254]\n",
      "(0.0, 1.0) : [0.9889620779371219, 0.011733448382304836]\n",
      "(0.0, 2.0) : [0.9860176610122826, 0.013353054568234761]\n",
      "(0.0, 3.0) : [0.9801698982867696, 0.03769884224179883]\n",
      "(0.0, 4.0) : [0.9620186038740516, 0.05550107330042604]\n",
      "(1.0, 0.0) : [0.18611280682755285, 0.8150457402448006]\n",
      "(1.0, 1.0) : [0.13875268325106513, 0.8642309985352057]\n",
      "(1.0, 2.0) : [0.3058222406350007, 0.6999931285645572]\n",
      "(1.0, 3.0) : [0.24975482985191724, 0.999019319407669]\n",
      "(1.0, 4.0) : [0.7135852281483349, 0.4281511368890009]\n",
      "residual0 mode ==>residual2==>residual3 \n",
      "(0.0, 0.0) : [0.9757661339290119, 0.023426716713572226]\n",
      "(0.0, 1.0) : [0.9635211710023204, 0.037188536424650954]\n",
      "(0.0, 2.0) : [0.9762825351774523, 0.023086580910681425]\n",
      "(0.0, 3.0) : [0.9424710560449705, 0.07539768448359765]\n",
      "(0.0, 4.0) : [0.962691344156481, 0.05449196287678194]\n",
      "(1.0, 0.0) : [0.2318399264448177, 0.7692870286578042]\n",
      "(1.0, 1.0) : [0.01943617352933208, 0.9834703805842032]\n",
      "(1.0, 2.0) : [0.33549156248765, 0.6709831249753]\n",
      "(1.0, 3.0) : [0.7492644895557516, 0.4995096597038345]\n",
      "(1.0, 4.0) : [0.33300643980255634, 0.8325160995063907]\n",
      "residual2 mode ==>residual3==>residual1 \n",
      "(0.0, 0.0) : [0.9582359068568325, 0.04095770063866054]\n",
      "(0.0, 1.0) : [0.9659507356642095, 0.03480903551943097]\n",
      "(0.0, 2.0) : [0.9602566650391176, 0.03911504213553837]\n",
      "(0.0, 3.0) : [0.9605954994304507, 0.05763572996582705]\n",
      "(0.0, 4.0) : [0.9435182461072429, 0.07400143106723474]\n",
      "(1.0, 0.0) : [0.5345454073784521, 0.46647595475348075]\n",
      "(1.0, 1.0) : [0.15313434823037259, 0.8495310270875434]\n",
      "(1.0, 2.0) : [0.7669890258678232, 0.23847557947150808]\n",
      "(1.0, 3.0) : [0.7992154555261353, 0.39960772776306763]\n",
      "(1.0, 4.0) : [0.570868182518668, 0.570868182518668]\n",
      "==>mode==>residual0 residual1 residual2 residual3 \n",
      "() : [0.611864073855824, 0.08318601453545472, 0.2935572579061088, 0.005499901787468081, 0.0058927519151443725]\n",
      "[ 0.  0.  0. ...,  2.  2.  2.]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "error rate =  0.613147292915\n"
     ]
    }
   ],
   "source": [
    "model_headings = ['residual'+str(i) for i in range(len(dis_residuals[0]))]\n",
    "model_headings.append('mode')\n",
    "print(model_headings)\n",
    "labels = labeled_data[:,-1]\n",
    "model_labeled_data  = [i+[j] for i,j in zip(dis_residuals.tolist(),labels.tolist())]\n",
    "model_labeled_data  = np.array(model_labeled_data)\n",
    "print(model_labeled_data)\n",
    "\n",
    "tan = graph()\n",
    "tan.get_nodes_from_data(model_headings,model_labeled_data)\n",
    "tan.get_basic_tan_structure(model_headings,model_labeled_data)\n",
    "tan.print_tan()\n",
    "tan.train_tan(model_headings,model_labeled_data)\n",
    "tan.print_tan()\n",
    "\n",
    "input_data   = model_labeled_data[:,:-1]\n",
    "output_label = model_labeled_data[:,-1]\n",
    "print(output_label)\n",
    "predict_labels = []\n",
    "for i in input_data:\n",
    "    w = tan.predict(i)\n",
    "    index = w.index(max(w))\n",
    "    predict_labels.append(index)\n",
    "\n",
    "\n",
    "predict_labels = np.array(predict_labels)\n",
    "print(predict_labels)\n",
    "\n",
    "error_rate = sum([i==j for i,j in zip(predict_labels,output_label)])/float(len(output_label))\n",
    "print('error rate = ',error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine encoded data and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature0', 'feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'residual0', 'residual1', 'residual2', 'residual3', 'mode']\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  2.]\n",
      " [ 0.  0.  0. ...,  0.  0.  2.]\n",
      " [ 0.  0.  0. ...,  0.  0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "hybrid_headings = headings[:]\n",
    "hybrid_headings[-1:-1] = ['residual'+str(i) for i in range(len(dis_residuals[0]))]\n",
    "print(hybrid_headings)\n",
    "hybrid_labeled_data = np.zeros([len(labeled_data),len(labeled_data[0])+len(dis_residuals[0])])\n",
    "for i in range(len(hybrid_labeled_data)):\n",
    "    tmp_data = np.hstack((labeled_data[i,:-1],dis_residuals[i,:]))\n",
    "    hybrid_labeled_data[i,:] = np.hstack((tmp_data,labeled_data[i,-1]))\n",
    "\n",
    "print(hybrid_labeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid TAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode ==>feature0==>feature5 feature2 \n",
      "feature5 mode ==>feature1==>feature4 \n",
      "feature0 mode ==>feature2==>\n",
      "feature5 mode ==>feature3==>\n",
      "feature1 mode ==>feature4==>\n",
      "feature0 mode ==>feature5==>residual2 feature1 feature3 \n",
      "residual2 mode ==>residual0==>\n",
      "residual3 mode ==>residual1==>\n",
      "feature5 mode ==>residual2==>residual3 residual0 \n",
      "residual2 mode ==>residual3==>residual1 \n",
      "==>mode==>feature0 feature1 feature2 feature3 feature4 feature5 residual0 residual1 residual2 residual3 \n",
      "mode ==>feature0==>feature5 feature2 \n",
      "(0.0,) : [0.9239443592618989, 0.07572525396277413]\n",
      "(1.0,) : [0.5133252568932625, 0.4873639795331434]\n",
      "(2.0,) : [0.9643976147217984, 0.03544595948700091]\n",
      "(3.0,) : [0.8031770183286823, 0.2141805382209819]\n",
      "(4.0,) : [0.9828506920585058, 0.033316972612152745]\n",
      "feature5 mode ==>feature1==>feature4 \n",
      "(0.0, 0.0) : [0.949710753764329, 0.04948218735335169]\n",
      "(0.0, 1.0) : [0.9921767213295343, 0.009123464104179626]\n",
      "(0.0, 2.0) : [0.9787920922817073, 0.020570061484028877]\n",
      "(0.0, 3.0) : [0.944527356530887, 0.07265595050237593]\n",
      "(0.0, 4.0) : [0.9773015081161978, 0.04343562258294213]\n",
      "(1.0, 0.0) : [0.48589618889429603, 0.5152175106379173]\n",
      "(1.0, 1.0) : [0.004873264972720337, 0.9965826869213088]\n",
      "(1.0, 2.0) : [0.420639713434808, 0.5915245970176987]\n",
      "(1.0, 3.0) : [0.999019319407669, 0.4995096597038345]\n",
      "(1.0, 4.0) : [0.9990193194076691, 0.06660128796051126]\n",
      "feature0 mode ==>feature2==>\n",
      "(0.0, 0.0) : [0.95513116385807, 0.04406162652015071]\n",
      "(0.0, 1.0) : [0.9806465503151142, 0.020669365229124187]\n",
      "(0.0, 2.0) : [0.986202497348694, 0.01316322265516346]\n",
      "(0.0, 3.0) : [0.9546184607673279, 0.06660128796051125]\n",
      "(0.0, 4.0) : [0.9820867885702509, 0.03386506167483624]\n",
      "(1.0, 0.0) : [0.37039911206852133, 0.630736773693825]\n",
      "(1.0, 1.0) : [0.07740585525676855, 0.9240323971276743]\n",
      "(1.0, 2.0) : [0.3015907379343906, 0.706853292033728]\n",
      "(1.0, 3.0) : [0.16650321990127817, 0.9157677094570298]\n",
      "(1.0, 4.0) : [0.4995096597038345, 0.999019319407669]\n",
      "feature5 mode ==>feature3==>\n",
      "(0.0, 0.0) : [0.9564820004547876, 0.04271094066289304]\n",
      "(0.0, 1.0) : [0.996738453381624, 0.004561732052089813]\n",
      "(0.0, 2.0) : [0.9746780799849015, 0.02468407378083465]\n",
      "(0.0, 3.0) : [0.4359357030142555, 0.5812476040190074]\n",
      "(0.0, 4.0) : [0.49950965970383443, 0.5212274709953055]\n",
      "(1.0, 0.0) : [0.6618241193560238, 0.33928958017618943]\n",
      "(1.0, 1.0) : [0.029239589836322015, 0.9722163620577071]\n",
      "(1.0, 2.0) : [0.841279426869616, 0.17088488358289075]\n",
      "(1.0, 3.0) : [0.4995096597038345, 0.999019319407669]\n",
      "(1.0, 4.0) : [0.5328103036840901, 0.5328103036840901]\n",
      "feature1 mode ==>feature4==>\n",
      "(0.0, 0.0) : [0.9639721585794749, 0.03522239663233493]\n",
      "(0.0, 1.0) : [0.9944366619791936, 0.006873986142713318]\n",
      "(0.0, 2.0) : [0.9813651318505686, 0.018000348097435476]\n",
      "(0.0, 3.0) : [0.9236216349240711, 0.09424710560449706]\n",
      "(0.0, 4.0) : [0.8804916035457422, 0.13546024669934495]\n",
      "(1.0, 0.0) : [0.5390934440577232, 0.46181081746203567]\n",
      "(1.0, 1.0) : [0.021823237559876262, 0.9796208860211122]\n",
      "(1.0, 2.0) : [0.5379334796810525, 0.4706917947209209]\n",
      "(1.0, 3.0) : [0.24975482985191724, 0.999019319407669]\n",
      "(1.0, 4.0) : [0.4995096597038345, 0.999019319407669]\n",
      "feature0 mode ==>feature5==>residual2 feature1 feature3 \n",
      "(0.0, 0.0) : [0.9671006608261424, 0.03209212955207826]\n",
      "(0.0, 1.0) : [0.9967227232710997, 0.004593192273138708]\n",
      "(0.0, 2.0) : [0.9927841086762758, 0.00658161132758173]\n",
      "(0.0, 3.0) : [0.9768188900874983, 0.044400858640340844]\n",
      "(0.0, 4.0) : [0.7619638876838153, 0.25398796256127176]\n",
      "(1.0, 0.0) : [0.38098194384190764, 0.6201539419204386]\n",
      "(1.0, 1.0) : [0.012094664883870083, 0.9893435875005728]\n",
      "(1.0, 2.0) : [0.46181081746203556, 0.546633212506083]\n",
      "(1.0, 3.0) : [0.9990193194076689, 0.08325160995063909]\n",
      "(1.0, 4.0) : [0.999019319407669, 0.4995096597038345]\n",
      "residual2 mode ==>residual0==>\n",
      "(0.0, 0.0) : [0.9800219178348434, 0.019171689660649614]\n",
      "(0.0, 1.0) : [0.9920575123037828, 0.008702258879857743]\n",
      "(0.0, 2.0) : [0.9835142576602484, 0.015857449514407443]\n",
      "(0.0, 3.0) : [0.9605954994304507, 0.05763572996582705]\n",
      "(0.0, 4.0) : [0.9805189616408603, 0.03700071553361737]\n",
      "(1.0, 0.0) : [0.2702757677756219, 0.730745594356311]\n",
      "(1.0, 1.0) : [0.08021323002543326, 0.9224521452924827]\n",
      "(1.0, 2.0) : [0.4253888714897171, 0.5800757338496142]\n",
      "(1.0, 3.0) : [0.7992154555261353, 0.39960772776306763]\n",
      "(1.0, 4.0) : [0.4281511368890009, 0.7135852281483349]\n",
      "residual3 mode ==>residual1==>\n",
      "(0.0, 0.0) : [0.9848070390135973, 0.014385600886682254]\n",
      "(0.0, 1.0) : [0.9889620779371219, 0.011733448382304836]\n",
      "(0.0, 2.0) : [0.9860176610122826, 0.013353054568234761]\n",
      "(0.0, 3.0) : [0.9801698982867696, 0.03769884224179883]\n",
      "(0.0, 4.0) : [0.9620186038740516, 0.05550107330042604]\n",
      "(1.0, 0.0) : [0.18611280682755285, 0.8150457402448006]\n",
      "(1.0, 1.0) : [0.13875268325106513, 0.8642309985352057]\n",
      "(1.0, 2.0) : [0.3058222406350007, 0.6999931285645572]\n",
      "(1.0, 3.0) : [0.24975482985191724, 0.999019319407669]\n",
      "(1.0, 4.0) : [0.7135852281483349, 0.4281511368890009]\n",
      "feature5 mode ==>residual2==>residual3 residual0 \n",
      "(0.0, 0.0) : [0.9321749610531411, 0.06701798006453948]\n",
      "(0.0, 1.0) : [0.7983031091157171, 0.20299707631799663]\n",
      "(0.0, 2.0) : [0.9465656626233955, 0.05279649114234078]\n",
      "(0.0, 3.0) : [0.926363368905293, 0.09081993812796989]\n",
      "(0.0, 4.0) : [0.8904302629503135, 0.13030686774882638]\n",
      "(1.0, 0.0) : [0.762354365334154, 0.23875933419805925]\n",
      "(1.0, 1.0) : [0.5482423094310379, 0.45321364246299123]\n",
      "(1.0, 2.0) : [0.9858743283628312, 0.0262899820896755]\n",
      "(1.0, 3.0) : [0.999019319407669, 0.4995096597038345]\n",
      "(1.0, 4.0) : [0.9324180314471578, 0.13320257592102253]\n",
      "residual2 mode ==>residual3==>residual1 \n",
      "(0.0, 0.0) : [0.9582359068568325, 0.04095770063866054]\n",
      "(0.0, 1.0) : [0.9659507356642095, 0.03480903551943097]\n",
      "(0.0, 2.0) : [0.9602566650391176, 0.03911504213553837]\n",
      "(0.0, 3.0) : [0.9605954994304507, 0.05763572996582705]\n",
      "(0.0, 4.0) : [0.9435182461072429, 0.07400143106723474]\n",
      "(1.0, 0.0) : [0.5345454073784521, 0.46647595475348075]\n",
      "(1.0, 1.0) : [0.15313434823037259, 0.8495310270875434]\n",
      "(1.0, 2.0) : [0.7669890258678232, 0.23847557947150808]\n",
      "(1.0, 3.0) : [0.7992154555261353, 0.39960772776306763]\n",
      "(1.0, 4.0) : [0.570868182518668, 0.570868182518668]\n",
      "==>mode==>feature0 feature1 feature2 feature3 feature4 feature5 residual0 residual1 residual2 residual3 \n",
      "() : [0.611864073855824, 0.08318601453545472, 0.2935572579061088, 0.005499901787468081, 0.0058927519151443725]\n"
     ]
    }
   ],
   "source": [
    "#%xmode Plain\n",
    "\n",
    "tan = graph()\n",
    "tan.get_nodes_from_data(hybrid_headings,hybrid_labeled_data)\n",
    "tan.get_basic_tan_structure(hybrid_headings,hybrid_labeled_data)\n",
    "tan.print_tan()\n",
    "tan.train_tan(hybrid_headings,hybrid_labeled_data)\n",
    "tan.print_tan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  2.  2.  2.]\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "input_data   = hybrid_labeled_data[:,:-1]\n",
    "output_label = hybrid_labeled_data[:,-1]\n",
    "print(output_label)\n",
    "predict_labels = []\n",
    "for i in input_data:\n",
    "    w = tan.predict(i)\n",
    "    index = w.index(max(w))\n",
    "    predict_labels.append(index)\n",
    "\n",
    "\n",
    "predict_labels = np.array(predict_labels)\n",
    "print(predict_labels)\n",
    "\n",
    "error_rate = sum([i==j for i,j in zip(predict_labels,output_label)])/float(len(output_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640660312469\n"
     ]
    }
   ],
   "source": [
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
